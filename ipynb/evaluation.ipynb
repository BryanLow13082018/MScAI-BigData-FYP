{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os\n",
    "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'  # Suppress Git warnings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Enable CUDA launch blocking for debugging\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"  # Enable CUDA device-side assertions\n",
    "os.environ['MLFLOW_FLATTEN_PARAMS'] = 'true' # Flatten parameters parameters for logging\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../py')  # Add the parent directory to the Python path\n",
    "import gc\n",
    "import torch\n",
    "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import mlflow\n",
    "import optuna\n",
    "import json\n",
    "from accelerate import Accelerator\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from dataset_loader import DatasetLoader\n",
    "from utils import get_model, set_seed, load_config, get_device, CustomDataset\n",
    "from models.llama2_decoder import Llama2Decoder  # Import Llama2Decoder model\n",
    "from models.ernie_m import ErnieM  # Import ErnieM model\n",
    "from evaluators.africomet_evaluator import AfriCOMETEvaluator\n",
    "from trainers.encoder_decoder_trainer import EncoderDecoderTrainer\n",
    "from trainers.combined_encoder_decoder_trainer import CombinedEncoderDecoderTrainer\n",
    "from hyperparameter_analysis import (plot_hyperparameter_importance, plot_optimization_history,\n",
    "                                     plot_parallel_coordinate, analyze_hyperparameter_sensitivity,\n",
    "                                     plot_sensitivity_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(config):\n",
    "    \"\"\"\n",
    "    Set up logging configuration based on the provided config.\n",
    "    \n",
    "    This function initializes the logging system with the specified log level,\n",
    "    format, and output file from the configuration.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing logging configuration.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger object.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, config['logging']['log_level']),\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        filename=config['logging']['log_file']\n",
    "    )\n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clear unused memory to prevent out-of-memory errors.\n",
    "    \n",
    "    This function uses Python's garbage collector and PyTorch's CUDA memory \n",
    "    cache clearing (if available) to free up memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            logging.info(\"CUDA cache cleared successfully\")\n",
    "        except RuntimeError as e:\n",
    "            logging.warning(f\"Failed to clear CUDA cache: {str(e)}\")\n",
    "    logging.info(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_loader):\n",
    "    \"\"\"\n",
    "    Load datasets using the provided DatasetLoader object.\n",
    "    \n",
    "    This function attempts to load all datasets specified in the configuration\n",
    "    using the DatasetLoader. It includes error handling for common issues.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DatasetLoader): An instance of the DatasetLoader class.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary of loaded datasets, or None if loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Loading and preparing datasets...\")\n",
    "        return data_loader.load_datasets()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading datasets: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_combined(trial, encoder, decoder, tokenizer, train_dataset, eval_dataset, config, evaluator):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization of the combined Afro-XLMR and LLaMA model.\n",
    "\n",
    "    This function is called by Optuna for each trial to evaluate the performance of the model \n",
    "    with different hyperparameter settings.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The Optuna trial object used for hyperparameter suggestions.\n",
    "        encoder (torch.nn.Module): The Afro-XLMR encoder model to be trained and evaluated.\n",
    "        decoder (torch.nn.Module): The LLaMA decoder model to be trained and evaluated.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for processing text.\n",
    "        train_dataset (Dataset): The dataset used for training the model.\n",
    "        eval_dataset (Dataset): The dataset used for evaluating the model.\n",
    "        config (dict): The configuration dictionary containing hyperparameter ranges and settings.\n",
    "        evaluator (Evaluator): The evaluator object used to compute evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation metric to be minimized (lower is better).\n",
    "    \"\"\"\n",
    "    # Hyperparameter suggestions\n",
    "    encoder_lr = trial.suggest_float('encoder_lr', 1e-6, 1e-4, log=True)\n",
    "    decoder_lr = trial.suggest_float('decoder_lr', 1e-6, 1e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 1, 3)\n",
    "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [1, 2])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n",
    "    warmup_steps = trial.suggest_int('warmup_steps', 50, 200)\n",
    "    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4, 8])\n",
    "\n",
    "    # Update config with trial-suggested hyperparameters\n",
    "    trial_config = config.copy()\n",
    "    trial_config.update({\n",
    "        \"encoder_lr\": encoder_lr,\n",
    "        \"decoder_lr\": decoder_lr,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "        \"per_device_eval_batch_size\": per_device_train_batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"fp16\": True,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 200,\n",
    "        \"save_steps\": 200,\n",
    "        \"logging_steps\": 50,\n",
    "        \"max_grad_norm\": 1.0\n",
    "    })\n",
    "\n",
    "    logging.info(f\"Trial {trial.number}: Starting with hyperparameters: {trial_config}\")\n",
    "\n",
    "    try:\n",
    "        # Initialize the trainer\n",
    "        trainer = CombinedEncoderDecoderTrainer(encoder, decoder, tokenizer, trial_config)\n",
    "\n",
    "        # Log some information about the datasets\n",
    "        logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        logging.info(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "        # Validate datasets\n",
    "        logging.info(\"Validating train dataset:\")\n",
    "        trainer.validate_dataset(train_dataset)\n",
    "        logging.info(\"Validating eval dataset:\")\n",
    "        trainer.validate_dataset(eval_dataset)\n",
    "\n",
    "        # Train the combined model\n",
    "        train_result = trainer.train(train_dataset, eval_dataset)\n",
    "\n",
    "        # Evaluate the model\n",
    "        eval_results = evaluator.evaluate(encoder, decoder, tokenizer, eval_dataset)\n",
    "\n",
    "        # Ensure evaluation results are valid\n",
    "        if 'average_score' not in eval_results:\n",
    "            logging.error(\"Evaluation results do not contain average_score.\")\n",
    "            return float('inf')  # Return a high metric to minimize\n",
    "\n",
    "        # Calculate optimization metric (lower is better)\n",
    "        optimization_metric = 1 - eval_results.get('average_score', 0)\n",
    "        return optimization_metric\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA error: device-side assert triggered\" in str(e):\n",
    "            logging.error(f\"CUDA assert error: {str(e)}\")\n",
    "            logging.error(f\"Input shapes - encoder: {encoder.config.hidden_size}, decoder: {decoder.config.hidden_size}\")\n",
    "            return float('inf')  # Return a high value to deprioritize this trial\n",
    "        elif \"You can't move a model that has some modules offloaded to cpu or disk\" in str(e):\n",
    "            logging.error(f\"Model movement error: {str(e)}\")\n",
    "            return float('inf')  # Return a high value to deprioritize this trial\n",
    "        else:\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during objective_combined: {str(e)}\")\n",
    "        logging.error(f\"Error details: {type(e).__name__}, {str(e)}\")\n",
    "        logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return float('inf')  # Return a high metric to minimize\n",
    "\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()  # Clear CUDA cache after each trial\n",
    "        logging.info(f\"Trial {trial.number}: CUDA cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_combined(trial, encoder, decoder, tokenizer, train_dataset, eval_dataset, config, evaluator):\n",
    "#     \"\"\"\n",
    "#     Objective function for hyperparameter optimization of the combined Afro-XLMR and LLaMA model.\n",
    "    \n",
    "#     Args:\n",
    "#         trial (optuna.trial.Trial): Optuna trial object for hyperparameter suggestions.\n",
    "#         encoder (torch.nn.Module): The Afro-XLMR encoder model.\n",
    "#         decoder (torch.nn.Module): The LLaMA decoder model.\n",
    "#         tokenizer: The tokenizer for the models.\n",
    "#         train_dataset: Dataset for training.\n",
    "#         eval_dataset: Dataset for evaluation.\n",
    "#         config (dict): Configuration dictionary.\n",
    "#         evaluator: The AfriCOMET evaluator object.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The evaluation metric to be minimized.\n",
    "#     \"\"\"\n",
    "#     torch.cuda.empty_cache()  # Clear CUDA cache before each trial\n",
    "    \n",
    "#     try:\n",
    "#         # Hyperparameter suggestions\n",
    "#         encoder_lr = trial.suggest_float('encoder_lr', 1e-6, 1e-4, log=True)\n",
    "#         decoder_lr = trial.suggest_float('decoder_lr', 1e-6, 1e-4, log=True)\n",
    "#         num_train_epochs = trial.suggest_int('num_train_epochs', 1, 2)\n",
    "#         per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [1, 2])\n",
    "#         weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n",
    "#         warmup_steps = trial.suggest_int('warmup_steps', 50, 200)\n",
    "#         gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4, 8])\n",
    "\n",
    "#         # Update config with trial-suggested hyperparameters\n",
    "#         config.update({\n",
    "#             \"encoder_lr\": encoder_lr,\n",
    "#             \"decoder_lr\": decoder_lr,\n",
    "#             \"num_train_epochs\": num_train_epochs,\n",
    "#             \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "#             \"per_device_eval_batch_size\": per_device_train_batch_size,\n",
    "#             \"weight_decay\": weight_decay,\n",
    "#             \"warmup_steps\": warmup_steps,\n",
    "#             \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "#             \"fp16\": True,  # Enable mixed precision training\n",
    "#             \"evaluation_strategy\": \"steps\",\n",
    "#             \"eval_steps\": 200,\n",
    "#             \"save_steps\": 200,\n",
    "#             \"logging_steps\": 50,\n",
    "#             \"max_grad_norm\": 1.0,\n",
    "#         })\n",
    "\n",
    "#         logging.info(f\"Trial {trial.number}: Starting with hyperparameters: {config}\")\n",
    "\n",
    "#         try:\n",
    "#             # Initialize trainer\n",
    "#             logging.info(f\"Trial {trial.number}: Initializing trainer with encoder hidden size: {encoder.config.hidden_size}, decoder hidden size: {decoder.config.hidden_size}\")\n",
    "#             trainer = CombinedEncoderDecoderTrainer(encoder, decoder, tokenizer, config)\n",
    "#             logging.info(f\"Trial {trial.number}: Trainer initialized successfully\")\n",
    "\n",
    "#             # Validate datasets\n",
    "#             logging.info(f\"Trial {trial.number}: Validating training dataset\")\n",
    "#             trainer.validate_dataset(train_dataset)\n",
    "#             logging.info(f\"Trial {trial.number}: Validating evaluation dataset\")\n",
    "#             trainer.validate_dataset(eval_dataset)\n",
    "\n",
    "#             # Train the combined model\n",
    "#             logging.info(f\"Trial {trial.number}: Starting training\")\n",
    "#             train_result = trainer.train(train_dataset, eval_dataset)\n",
    "#             logging.info(f\"Trial {trial.number}: Training completed\")\n",
    "            \n",
    "#             # Evaluate using AfriCOMET\n",
    "#             logging.info(f\"Trial {trial.number}: Starting evaluation\")\n",
    "#             eval_results = evaluator.evaluate(encoder, decoder, tokenizer, eval_dataset)\n",
    "#             logging.info(f\"Trial {trial.number}: Evaluation completed\")\n",
    "            \n",
    "#             # Calculate optimization metric (lower is better)\n",
    "#             optimization_metric = 1 - eval_results.get('average_score', 0)\n",
    "#             logging.info(f\"Trial {trial.number}: Optimization metric: {optimization_metric}\")\n",
    "            \n",
    "#             return optimization_metric\n",
    "\n",
    "#         except RuntimeError as e:\n",
    "#             if \"CUDA error: device-side assert triggered\" in str(e):\n",
    "#                 logging.error(f\"Trial {trial.number}: CUDA assert error: {str(e)}\")\n",
    "#                 logging.error(f\"Trial {trial.number}: Input shapes - encoder: {encoder.config.hidden_size}, decoder: {decoder.config.hidden_size}\")\n",
    "#                 return float('inf')  # Return a high value to deprioritize this trial\n",
    "#             else:\n",
    "#                 raise\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Trial {trial.number}: Error in objective_combined: {str(e)}\")\n",
    "#         raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "#     finally:\n",
    "#         torch.cuda.empty_cache()  # Clear CUDA cache after each trial\n",
    "#         logging.info(f\"Trial {trial.number}: CUDA cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_ernie(trial, model, tokenizer, train_dataset, eval_dataset, config, evaluator):\n",
    "    \"\"\"\n",
    "    Optimized objective function for hyperparameter optimization of the ERNIE-M model.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna trial object for hyperparameter suggestions.\n",
    "        model (ErnieM): The ERNIE-M model to be optimized.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        train_dataset: Dataset for training.\n",
    "        eval_dataset: Dataset for evaluation.\n",
    "        config (dict): Configuration dictionary containing hyperparameter ranges and other settings.\n",
    "        evaluator: The AfriCOMET evaluator object for computing metrics.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation metric to be minimized (or float('inf') if an error occurs).\n",
    "    \"\"\"\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "    \n",
    "    patience = 3\n",
    "    best_metric = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        try:\n",
    "            # Suggest hyperparameters\n",
    "            lr_min = min(float(config['hyperparameters']['learning_rate_min']), float(config['hyperparameters']['learning_rate_max']))\n",
    "            lr_max = max(float(config['hyperparameters']['learning_rate_min']), float(config['hyperparameters']['learning_rate_max']))\n",
    "            learning_rate = trial.suggest_float('learning_rate', lr_min, lr_max, log=True)\n",
    "            num_train_epochs = trial.suggest_int('num_train_epochs', 1, 2)\n",
    "            per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8, 16])\n",
    "            per_device_eval_batch_size = per_device_train_batch_size\n",
    "            weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n",
    "            warmup_steps = trial.suggest_int('warmup_steps', 50, 200)\n",
    "            gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n",
    "\n",
    "            # Log the hyperparameters to MLflow\n",
    "            mlflow.log_params({\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"num_train_epochs\": num_train_epochs,\n",
    "                \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "                \"per_device_eval_batch_size\": per_device_eval_batch_size,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"warmup_steps\": warmup_steps,\n",
    "                \"gradient_accumulation_steps\": gradient_accumulation_steps\n",
    "            })\n",
    "\n",
    "            # Update the config with the trial-suggested hyperparameters\n",
    "            config.update({\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"num_train_epochs\": num_train_epochs,\n",
    "                \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "                \"per_device_eval_batch_size\": per_device_eval_batch_size,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"warmup_steps\": warmup_steps,\n",
    "                \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "                \"fp16\": True,\n",
    "                \"evaluation_strategy\": \"steps\",\n",
    "                \"eval_steps\": 200,\n",
    "                \"save_steps\": 200,\n",
    "                \"logging_steps\": 50,\n",
    "                \"max_grad_norm\": 1.0,\n",
    "            })\n",
    "\n",
    "            # Initialize the trainer\n",
    "            trainer = EncoderDecoderTrainer(model=model, tokenizer=tokenizer, config=config)\n",
    "\n",
    "            # Log dataset sizes and sample batch\n",
    "            logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "            logging.info(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "            sample_batch = next(iter(torch.utils.data.DataLoader(train_dataset, batch_size=per_device_train_batch_size)))\n",
    "            logging.info(f\"Sample batch keys: {sample_batch.keys()}\")\n",
    "            logging.info(f\"Sample input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "            logging.info(f\"Sample labels shape: {sample_batch['labels'].shape}\")\n",
    "\n",
    "            # Train the model\n",
    "            train_loss, eval_loss = trainer.train(train_dataset, eval_dataset)\n",
    "\n",
    "            # Log the train and eval losses\n",
    "            mlflow.log_metric(\"final_train_loss\", train_loss)\n",
    "            mlflow.log_metric(\"final_eval_loss\", eval_loss)\n",
    "            \n",
    "            logging.info(f\"Final training loss: {train_loss:.4f}\")\n",
    "            logging.info(f\"Final evaluation loss: {eval_loss:.4f}\")\n",
    "\n",
    "            # Begin AfriCOMET evaluation\n",
    "            logging.info(\"Starting AfriCOMET evaluation...\")\n",
    "            eval_results = evaluator.evaluate(model, tokenizer, eval_dataset)\n",
    "\n",
    "            # Log the evaluation results, handling potential NaN values\n",
    "            for key, value in eval_results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    for subkey, subvalue in value.items():\n",
    "                        if isinstance(subvalue, (int, float)) and not torch.isnan(torch.tensor(subvalue)):\n",
    "                            mlflow.log_metric(f\"{key}_{subkey}\", subvalue)\n",
    "                elif isinstance(value, (int, float)) and not torch.isnan(torch.tensor(value)):\n",
    "                    mlflow.log_metric(key, value)\n",
    "\n",
    "            # Calculate the optimization metric\n",
    "            africomet_score = eval_results.get('average_score', 0)\n",
    "            optimization_metric = (1 - africomet_score) + eval_loss  # Lower is better\n",
    "            logging.info(f\"Optimization metric (lower is better): {optimization_metric}\")\n",
    "\n",
    "            return optimization_metric\n",
    "\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"ValueError in objective_ernie: {str(ve)}\")\n",
    "            logging.error(\"This might be due to a mismatch in input or label shapes.\")\n",
    "            mlflow.log_metric(\"failed_due_to_error\", 1)\n",
    "            return float('inf')\n",
    "        \n",
    "        except RuntimeError as re:\n",
    "            logging.error(f\"RuntimeError in objective_ernie: {str(re)}\")\n",
    "            logging.error(\"This might be due to CUDA out of memory. Try reducing batch size or model size.\")\n",
    "            mlflow.log_metric(\"failed_due_to_error\", 1)\n",
    "            return float('inf')\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred in objective_ernie: {str(e)}\")\n",
    "            logging.error(f\"Error type: {type(e).__name__}\")\n",
    "            logging.error(f\"Error args: {e.args}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            mlflow.log_metric(\"failed_due_to_error\", 1)\n",
    "            return float('inf')\n",
    "        \n",
    "        finally:\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combined_optimization(encoder, decoder, tokenizer, datasets, config, evaluator):\n",
    "    \"\"\"\n",
    "    Run hyperparameter optimization for the combined Afro-XLMR and LLaMA model.\n",
    "    \n",
    "    Args:\n",
    "        encoder: The Afro-XLMR encoder model.\n",
    "        decoder: The LLaMA decoder model.\n",
    "        tokenizer: The tokenizer for the models.\n",
    "        datasets: Dictionary containing 'train' and 'eval' datasets.\n",
    "        config: Configuration dictionary.\n",
    "        evaluator: AfriCOMET evaluator object.\n",
    "    \n",
    "    Returns:\n",
    "        optuna.Study: The completed Optuna study object.\n",
    "    \"\"\"\n",
    "    mlflow.end_run()  # End any existing runs\n",
    "    \n",
    "    logging.info(\"Starting hyperparameter optimization for combined Afro-XLMR and LLaMA\")\n",
    "\n",
    "    # Log GPU memory information\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    available_memory = total_memory - torch.cuda.memory_allocated()\n",
    "    logging.info(f\"Total GPU memory: {total_memory / 1e9:.2f} GB\")\n",
    "    logging.info(f\"Available GPU memory before optimization: {available_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    try:\n",
    "        with mlflow.start_run(run_name=\"optimization_combined\"):\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(\n",
    "                lambda trial: objective_combined(\n",
    "                    trial, encoder, decoder, tokenizer,\n",
    "                    datasets['train'], datasets['eval'],\n",
    "                    config, evaluator\n",
    "                ),\n",
    "                n_trials=config['hyperparameters']['n_trials'],\n",
    "                timeout=3600,\n",
    "                catch=(Exception,),\n",
    "                n_jobs=1\n",
    "            )\n",
    "\n",
    "            if study.best_trial:\n",
    "                best_params = study.best_params\n",
    "                for param, value in best_params.items():\n",
    "                    mlflow.log_param(f\"best_{param}\", value)\n",
    "                mlflow.log_metric(\"best_score\", study.best_value)\n",
    "            else:\n",
    "                logging.warning(\"No completed trials found.\")\n",
    "\n",
    "    except optuna.exceptions.OptunaError as e:\n",
    "        logging.error(f\"Optuna error during hyperparameter optimization: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during hyperparameter optimization: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        logging.info(f\"GPU memory after optimization: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_combined_optimization(encoder, decoder, tokenizer, datasets, config, evaluator):\n",
    "#     \"\"\"\n",
    "#     Run hyperparameter optimization for the combined Afro-XLMR and LLaMA model.\n",
    "    \n",
    "#     Args:\n",
    "#         encoder: The Afro-XLMR encoder model.\n",
    "#         decoder: The LLaMA decoder model.\n",
    "#         tokenizer: The tokenizer for the models.\n",
    "#         datasets: Dictionary containing 'train' and 'eval' datasets.\n",
    "#         config: Configuration dictionary.\n",
    "#         evaluator: AfriCOMET evaluator object.\n",
    "    \n",
    "#     Returns:\n",
    "#         optuna.Study: The completed Optuna study object.\n",
    "#     \"\"\"\n",
    "#     mlflow.end_run()  # End any existing runs\n",
    "    \n",
    "#     logging.info(\"Starting hyperparameter optimization for combined Afro-XLMR and LLaMA\")\n",
    "\n",
    "#     # Log GPU memory information\n",
    "#     total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "#     available_memory = total_memory - torch.cuda.memory_allocated()\n",
    "#     logging.info(f\"Total GPU memory: {total_memory/1e9:.2f} GB\")\n",
    "#     logging.info(f\"Available GPU memory before optimization: {available_memory/1e9:.2f} GB\")\n",
    "\n",
    "#     try:\n",
    "#         with mlflow.start_run(run_name=\"optimization_combined\"):\n",
    "#             study = optuna.create_study(direction='minimize')\n",
    "#             study.optimize(\n",
    "#                 lambda trial: objective_combined(\n",
    "#                     trial, encoder, decoder, tokenizer,\n",
    "#                     datasets['train'], \n",
    "#                     datasets['eval'],  \n",
    "#                     config, evaluator\n",
    "#                 ),\n",
    "#                 n_trials=config['hyperparameters']['n_trials'],\n",
    "#                 timeout=3600,\n",
    "#                 catch=(Exception,),\n",
    "#                 n_jobs=1  # Run trials sequentially to avoid GPU conflicts\n",
    "#             )\n",
    "\n",
    "#             best_params = study.best_params\n",
    "#             for param, value in best_params.items():\n",
    "#                 mlflow.log_param(f\"best_{param}\", value)\n",
    "#             mlflow.log_metric(\"best_score\", study.best_value)\n",
    "\n",
    "#         logging.info(\"Completed hyperparameter optimization for combined model\")\n",
    "#         logging.info(f\"Best trial: {study.best_trial}\")\n",
    "#         print_best_trial_info(study.best_trial)\n",
    "\n",
    "#     except optuna.exceptions.TrialPruned:\n",
    "#         logging.info(\"Trial was pruned due to a CUDA error.\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error during hyperparameter optimization: {str(e)}\")\n",
    "#         logging.exception(\"Exception details:\")\n",
    "#         return None\n",
    "\n",
    "#     finally:\n",
    "#         try:\n",
    "#             torch.cuda.empty_cache()\n",
    "#             logging.info(f\"GPU memory after optimization: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "#         except RuntimeError as e:\n",
    "#             logging.warning(f\"Failed to empty CUDA cache: {str(e)}\")\n",
    "\n",
    "#     return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ernie_optimization(model, tokenizer, datasets, config, evaluator):\n",
    "    \"\"\"\n",
    "    Run hyperparameter optimization for ERNIE-M model.\n",
    "    \n",
    "    Args:\n",
    "        model: The ERNIE-M model.\n",
    "        tokenizer: The tokenizer for ERNIE-M.\n",
    "        datasets: Dictionary containing 'train' and 'eval' datasets.\n",
    "        config: Configuration dictionary.\n",
    "        evaluator: Evaluator object for ERNIE-M.\n",
    "    \n",
    "    Returns:\n",
    "        optuna.Study: The completed Optuna study object.\n",
    "    \"\"\"\n",
    "    # End any existing MLflow runs\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    logging.info(\"Starting hyperparameter optimization for ERNIE-M\")\n",
    "    \n",
    "    # Log GPU memory information\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    available_memory = total_memory - torch.cuda.memory_allocated()\n",
    "    logging.info(f\"Total GPU memory: {total_memory/1e9:.2f} GB\")\n",
    "    logging.info(f\"Available GPU memory before optimization: {available_memory/1e9:.2f} GB\")\n",
    "    \n",
    "    try:\n",
    "        # Start a new MLflow run for this optimization\n",
    "        with mlflow.start_run(run_name=\"optimization_ernie_m\"):\n",
    "            # Create an Optuna study object\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            \n",
    "            # Log dataset sizes and model information\n",
    "            logging.info(f\"Train dataset size: {len(datasets['train'])}\")\n",
    "            logging.info(f\"Eval dataset size: {len(datasets['eval'])}\")\n",
    "            logging.info(f\"Model type: {type(model).__name__}\")\n",
    "            logging.info(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
    "            \n",
    "            # Run the optimization\n",
    "            study.optimize(\n",
    "                lambda trial: objective_ernie(\n",
    "                    trial, \n",
    "                    model, \n",
    "                    tokenizer, \n",
    "                    datasets['train'],\n",
    "                    datasets['eval'], \n",
    "                    config, \n",
    "                    evaluator\n",
    "                ),\n",
    "                n_trials=config['hyperparameters']['n_trials'],\n",
    "                timeout=3600,\n",
    "                catch=(Exception,),\n",
    "                n_jobs=1,  # Run trials sequentially to avoid GPU conflicts\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "            \n",
    "            # Log the best parameters and score\n",
    "            best_params = study.best_params\n",
    "            for param, value in best_params.items():\n",
    "                mlflow.log_param(f\"best_{param}\", value)\n",
    "            mlflow.log_metric(\"best_score\", study.best_value)\n",
    "            \n",
    "            # Save the optimization results\n",
    "            save_results(\"ernie_m\", study, config)\n",
    "        \n",
    "        logging.info(\"Completed hyperparameter optimization for ERNIE-M\")\n",
    "        logging.info(f\"Best trial: {study.best_trial}\")\n",
    "        print_best_trial_info(study.best_trial)\n",
    "    \n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        logging.info(\"Trial was pruned.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during hyperparameter optimization for ERNIE-M: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        clear_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            model.cpu()\n",
    "            torch.cuda.empty_cache()\n",
    "        logging.info(f\"GPU memory after optimization: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        logging.info(\"Memory cleared after ERNIE-M optimization\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_name, study, config):\n",
    "    \"\"\"Save the optimization results to disk.\"\"\"\n",
    "    output_dir = os.path.join(config['model']['output_dir'], 'optimization_results')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    result = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'best_trial': study.best_trial.number\n",
    "    }\n",
    "    with open(os.path.join(output_dir, f\"{model_name}_optimization_results.json\"), 'w') as f:\n",
    "        json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_trial_info(trial):\n",
    "    \"\"\"Print detailed information about the best trial.\"\"\"\n",
    "    print(\"\\nBest Trial Information:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    print(f\"  Trial number: {trial.number}\")\n",
    "    print(f\"  DateTime start: {trial.datetime_start}\")\n",
    "    print(f\"  DateTime complete: {trial.datetime_complete}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model, dataset, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance.\n",
    "    \n",
    "    This function evaluates the classification performance of the model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataset (Dataset): The dataset to evaluate on.\n",
    "        evaluator (ClassificationEvaluator): The evaluator to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation results.\n",
    "    \"\"\"\n",
    "    clear_memory()\n",
    "    logging.info(f\"Evaluating classification performance for {dataset.name}\")\n",
    "    results = evaluator.evaluate(dataset)\n",
    "    logging.info(f\"Classification Report:\\n{results['classification_report']}\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    results['confusion_matrix_plot'].savefig(f\"{model}/{dataset.name}_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    clear_memory()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(model, tokenizer, eval_dataset, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluate translation performance using the existing dataset structure.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The translation model to evaluate.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        eval_dataset (Dataset): The evaluation dataset.\n",
    "        evaluator (AfriCOMETEvaluator): The evaluator to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation results.\n",
    "    \"\"\"\n",
    "    results = {'translations': {}}\n",
    "    target_languages = ['swh', 'kin', 'lug']  # ISO codes for Swahili, Kinyarwanda, and Luganda\n",
    "    english_code = 'eng'\n",
    "    \n",
    "    # Convert dataset to DataFrame if it's not already\n",
    "    if not isinstance(eval_dataset, pd.DataFrame):\n",
    "        if hasattr(eval_dataset, 'data'):\n",
    "            eval_dataset = eval_dataset.data\n",
    "        else:\n",
    "            eval_dataset = pd.DataFrame(eval_dataset)\n",
    "    \n",
    "    # Log dataset information\n",
    "    logging.info(f\"Evaluation dataset shape: {eval_dataset.shape}\")\n",
    "    logging.info(f\"Languages in dataset: {eval_dataset['language'].unique()}\")\n",
    "    logging.info(f\"Splits in dataset: {eval_dataset['split'].unique()}\")\n",
    "    \n",
    "    for lang in target_languages:\n",
    "        try:\n",
    "            eng_texts = eval_dataset[eval_dataset['language'] == english_code]['text'].tolist()\n",
    "            lang_texts = eval_dataset[eval_dataset['language'] == lang]['text'].tolist()\n",
    "            \n",
    "            if not eng_texts or not lang_texts:\n",
    "                logging.warning(f\"No data found for {english_code} to {lang} translation. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            logging.info(f\"Translating {len(eng_texts)} sentences from {english_code} to {lang}\")\n",
    "            \n",
    "            inputs = tokenizer(eng_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(model.device)\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(**inputs, max_length=128)\n",
    "            translations = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "            \n",
    "            scores = evaluator.evaluate(eng_texts, translations, lang_texts)\n",
    "            results['translations'][f'{english_code}_to_{lang}'] = scores\n",
    "            \n",
    "            logging.info(f\"Translation scores for {english_code} to {lang}: {scores}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during translation evaluation for language {lang}: {str(e)}\")\n",
    "            results['translations'][f'{english_code}_to_{lang}'] = {'error': str(e)}\n",
    "    \n",
    "    # Calculate average score\n",
    "    valid_scores = [score['average_score'] for score in results['translations'].values() \n",
    "                    if isinstance(score, dict) and 'average_score' in score]\n",
    "    \n",
    "    if valid_scores:\n",
    "        results['average_score'] = np.mean(valid_scores)\n",
    "        logging.info(f\"Overall average translation score: {results['average_score']}\")\n",
    "    else:\n",
    "        results['average_score'] = np.nan\n",
    "        logging.warning(\"No valid scores for translations\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(model, prompt_texts, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluate text generation performance.\n",
    "    \n",
    "    This function evaluates the text generation performance of the model using\n",
    "    the provided evaluator.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The generation model to evaluate.\n",
    "        prompt_texts (list): A list of prompt texts for generation.\n",
    "        evaluator (GenerationEvaluator): The evaluator to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation results.\n",
    "    \"\"\"\n",
    "    clear_memory()\n",
    "    logging.info(\"Evaluating text generation performance\")\n",
    "    generated_texts = model.generate(prompt_texts)\n",
    "    generation_results = evaluator.evaluate(prompt_texts, generated_texts)\n",
    "    logging.info(f\"Average perplexity: {generation_results['average_perplexity']}\")\n",
    "    clear_memory()\n",
    "    return generation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zero_shot(model, tokenizer, dataset, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluate zero-shot performance.\n",
    "    \n",
    "    This function evaluates the zero-shot performance of the model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        dataset (Dataset): The dataset to evaluate on.\n",
    "        evaluator: The evaluator to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation results.\n",
    "    \"\"\"\n",
    "    clear_memory()\n",
    "    logging.info(\"Evaluating zero-shot performance\")\n",
    "    \n",
    "    inputs = tokenizer(dataset['source_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    results = evaluator.evaluate(dataset['source_text'].tolist(), generated_texts, dataset['target_text'].tolist())\n",
    "    \n",
    "    logging.info(f\"Zero-shot performance: {results}\")\n",
    "    clear_memory()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_code_switch(model, tokenizer, dataset, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluate code-switch performance.\n",
    "    \n",
    "    This function evaluates the performance of the model on code-switched text.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        dataset (Dataset): The dataset containing code-switched text.\n",
    "        evaluator: The evaluator to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation results.\n",
    "    \"\"\"\n",
    "    clear_memory()\n",
    "    logging.info(\"Evaluating code-switch performance\")\n",
    "    \n",
    "    inputs = tokenizer(dataset['code_switched_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    \n",
    "    predictions = torch.argmax(outputs, dim=-1).tolist()\n",
    "    \n",
    "    results = evaluator.evaluate(predictions, dataset['label'].tolist())\n",
    "    \n",
    "    logging.info(f\"Code-switch performance: {results}\")\n",
    "    clear_memory()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results, config):\n",
    "    \"\"\"\n",
    "    Summarize evaluation results.\n",
    "    \n",
    "    This function summarizes the results from various evaluation tasks and saves\n",
    "    them to files.\n",
    "\n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of evaluation results.\n",
    "    \"\"\"\n",
    "    logging.info(\"Summarizing results...\")\n",
    "\n",
    "    summary = {}\n",
    "    for model_name in config['model']['names']:\n",
    "        summary[model_name] = {\n",
    "            'classification': results['classification'][model_name]['classification_report']['accuracy'],\n",
    "            'translation': results['translation'][model_name]['average_score'] if results['translation'] else None,\n",
    "            'generation': results['generation'][model_name]['average_perplexity'] if results['generation'] else None,\n",
    "            'zero_shot': results['zero_shot'][model_name]['accuracy'] if results['zero_shot'] else None,\n",
    "            'code_switch': results['code_switch'][model_name]['accuracy'] if results['code_switch'] else None\n",
    "        }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    logging.info(\"Evaluation Results Summary:\")\n",
    "    logging.info(summary_df)\n",
    "\n",
    "    # Save results\n",
    "    summary_df.to_csv(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
    "    \n",
    "    with open(f\"{config['model']['output_dir']}/all_results.txt\", 'w') as f:\n",
    "        f.write(str(results))\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, config):\n",
    "    \"\"\"\n",
    "    Plot evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "    \"\"\"\n",
    "    for model_name in config['model']['names']:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        if model_name in results['classification']:\n",
    "            sns.heatmap(results['classification'][model_name]['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_confusion_matrix.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        if model_name in results['translation']:\n",
    "            plt.bar(results['translation'][model_name].keys(), results['translation'][model_name].values())\n",
    "            plt.title(f\"Translation Scores - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_translation_scores.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        if model_name in results['generation']:\n",
    "            plt.hist(results['generation'][model_name]['perplexities'], bins=20)\n",
    "            plt.title(f\"Perplexity Distribution - {model_name}\")\n",
    "            plt.xlabel(\"Perplexity\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_perplexity_distribution.png\")\n",
    "            plt.close()\n",
    "\n",
    "    # Plot overall performance comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    performance_data = {\n",
    "        model: {\n",
    "            'Classification': results['classification'].get(model, {}).get('accuracy', 0),\n",
    "            'Translation': results['translation'].get(model, {}).get('average_score', 0),\n",
    "            'Generation': 1 / results['generation'].get(model, {}).get('average_perplexity', 1),  # Inverse of perplexity\n",
    "            'Zero-shot': results['zero_shot'].get(model, {}).get('accuracy', 0),\n",
    "            'Code-switch': results['code_switch'].get(model, {}).get('accuracy', 0)\n",
    "        } for model in config['model']['names']\n",
    "    }\n",
    "    df = pd.DataFrame(performance_data).T\n",
    "    sns.heatmap(df, annot=True, cmap='YlGnBu')\n",
    "    plt.title(\"Model Performance Across Tasks\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_mlflow(results, config, best_params):\n",
    "    \"\"\"\n",
    "    Log results to MLflow.\n",
    "    \n",
    "    This function logs the evaluation results, model parameters, and artifacts to MLflow.\n",
    "\n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters for each model\n",
    "        for model_name, params in best_params.items():\n",
    "            for param, value in params.items():\n",
    "                mlflow.log_param(f\"{model_name}_{param}\", value)\n",
    "        \n",
    "        # Log model information\n",
    "        for model_name in config['model']['names']:\n",
    "            mlflow.log_param(f\"{model_name}_model\", model_name)\n",
    "\n",
    "        # Log metrics\n",
    "        for model_name, metrics in results['summary'].items():\n",
    "            for metric, value in metrics.items():\n",
    "                if value is not None:\n",
    "                    mlflow.log_metric(f\"{model_name}_{metric}\", value)\n",
    "\n",
    "        # Log artifacts\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/all_results.txt\")\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
    "        \n",
    "        # Log hyperparameter optimization plots\n",
    "        for model_name in config['model']['names']:\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_summary(results_summary, best_params):\n",
    "    \"\"\"\n",
    "    Print a summary of the evaluation results and best hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        results_summary (dict): A dictionary containing summarized results.\n",
    "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
    "    \"\"\"\n",
    "    print(\"\\n===== EVALUATION RESULTS SUMMARY =====\")\n",
    "\n",
    "    if 'classification' in results_summary:\n",
    "        print(\"\\nClassification Results:\")\n",
    "        for dataset, metrics in results_summary['classification'].items():\n",
    "            print(f\"\\n{dataset}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    if 'translation' in results_summary:\n",
    "        print(\"\\nTranslation Results:\")\n",
    "        print(f\"  FLORES-200 Average AfriCOMET Score (A to B): {results_summary['translation']['a_to_b']['average_score']:.4f}\")\n",
    "        print(f\"  FLORES-200 Average AfriCOMET Score (B to A): {results_summary['translation']['b_to_a']['average_score']:.4f}\")\n",
    "\n",
    "    if 'generation' in results_summary:\n",
    "        print(\"\\nGeneration Results:\")\n",
    "        print(f\"  FLORES-200 Average Perplexity: {results_summary['generation']['average_perplexity']:.4f}\")\n",
    "\n",
    "    if 'zero_shot' in results_summary:\n",
    "        print(\"\\nZero-shot Results:\")\n",
    "        print(f\"  Accuracy: {results_summary['zero_shot']['accuracy']:.4f}\")\n",
    "\n",
    "    if 'code_switch' in results_summary:\n",
    "        print(\"\\nCode-switch Results:\")\n",
    "        print(f\"  Accuracy: {results_summary['code_switch']['accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "    print(\"\\n======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../py/config.yaml')\n",
    "# Set the device dynamically based on availability\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'  # Update device setting\n",
    "auth_token = config.get(\"auth_token\")\n",
    "cache_dir = os.path.abspath(config['cache']['dir'])\n",
    "logger = setup_logging(config)\n",
    "set_seed(config['seed'])\n",
    "device = get_device(config['device'])\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the cache directory exists\n",
    "os.makedirs(cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare datasets\n",
    "data_loader = DatasetLoader(config)\n",
    "stratified_datasets = data_loader.prepare_stratified_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in stratified_datasets.items():\n",
    "    print(f\"{split} dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data integrity\n",
    "if not data_loader.verify_data_integrity(stratified_datasets):\n",
    "    logger.error(\"Data integrity check failed. Please review the datasets.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset information\n",
    "data_loader.print_dataset_info(stratified_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess datasets\n",
    "preprocessed_datasets = {\n",
    "    key: data_loader.preprocess_dataset(dataset)\n",
    "    for key, dataset in stratified_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Directory contents: {os.listdir()}\")\n",
    "print(f\"Parent directory contents: {os.listdir('..')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the models directory is in the Python path\n",
    "models_dir = os.path.abspath(os.path.join('..', 'py', 'models'))\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.append(models_dir)\n",
    "    print(f\"Added {models_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of the models directory\n",
    "print(f\"Models directory contents: {os.listdir(models_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models, tokenizers = {}, {}\n",
    "num_labels = len(preprocessed_datasets['train']['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models with the configured parameters and authentication token\n",
    "for model_name in config['model']['names']:\n",
    "    print(f\"Initializing model: {model_name}\")\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    print(f\"Auth token: {auth_token[:5]}...{auth_token[-5:] if auth_token else None}\")\n",
    "    try:\n",
    "        if model_name == \"meta-llama/Llama-2-7b-hf\":\n",
    "            llama_model = Llama2Decoder(model_name, auth_token=auth_token, cache_dir=cache_dir)\n",
    "            models[model_name] = llama_model.get_model()   # The model is already on the appropriate device\n",
    "            tokenizers[model_name] = llama_model.get_tokenizer()\n",
    "        elif model_name == \"ernie-m-large\":\n",
    "            ernie_model = ErnieM(num_labels)\n",
    "            models[model_name] = ernie_model.get_model().to('cuda') # Ensure Ernie M is on GPU\n",
    "            tokenizers[model_name] = ernie_model.get_tokenizer()\n",
    "        else:\n",
    "            model, tokenizer = get_model(model_name, num_labels=num_labels, auth_token=auth_token, cache_dir=cache_dir)\n",
    "            models[model_name] = model.to('cuda') # Ensure Afro XLMR is on GPU\n",
    "            tokenizers[model_name] = tokenizer\n",
    "\n",
    "        if models[model_name] is not None and tokenizers[model_name] is not None:\n",
    "            print(f\"Successfully initialized {model_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to initialize {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing {model_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available models:\", list(models.keys()))\n",
    "print(\"Available tokenizers:\", list(tokenizers.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom datasets for PyTorch\n",
    "datasets = {}\n",
    "\n",
    "model_type = 'encoder_decoder'  # New model type for the combined model\n",
    "\n",
    "# For the combined Afro-XLMR and LLaMA model\n",
    "combined_tokenizer = tokenizers['afro-xlmr-large']  # Assuming you're using the Afro-XLMR tokenizer for the combined model\n",
    "\n",
    "logging.info(f\"Creating datasets for combined Afro-XLMR and LLaMA model with model type: {model_type}\")\n",
    "\n",
    "# Use the key 'combined_afro_xlmr_llama'\n",
    "datasets['combined_afro_xlmr_llama'] = {\n",
    "    'train': CustomDataset(preprocessed_datasets['train'], combined_tokenizer, model_type=model_type),\n",
    "    'eval': CustomDataset(preprocessed_datasets['eval'], combined_tokenizer, model_type=model_type),\n",
    "    'benchmark': CustomDataset(preprocessed_datasets['benchmark'], combined_tokenizer, model_type=model_type)\n",
    "}\n",
    "\n",
    "# For ERNIE-M\n",
    "ernie_m_tokenizer = tokenizers['ernie-m-large']\n",
    "\n",
    "logging.info(f\"Creating datasets for ERNIE-M with model type: {model_type}\")\n",
    "\n",
    "datasets['ernie-m-large'] = {\n",
    "    'train': CustomDataset(preprocessed_datasets['train'], ernie_m_tokenizer, model_type=model_type),\n",
    "    'eval': CustomDataset(preprocessed_datasets['eval'], ernie_m_tokenizer, model_type=model_type),\n",
    "    'benchmark': CustomDataset(preprocessed_datasets['benchmark'], ernie_m_tokenizer, model_type=model_type)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available datasets:\", datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys in 'combined_afro_xlmr_llama':\", datasets['combined_afro_xlmr_llama'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset:\", datasets['combined_afro_xlmr_llama']['train'])\n",
    "print(\"Eval dataset:\", datasets['combined_afro_xlmr_llama']['eval'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluators\n",
    "evaluators = {\n",
    "    'combined_afro_xlmr_llama': AfriCOMETEvaluator(\n",
    "        model=None,  # AfriCOMETEvaluator doesn't use the model directly\n",
    "        tokenizer=tokenizers['afro-xlmr-large']\n",
    "    ),\n",
    "    'ernie-m-large': AfriCOMETEvaluator(\n",
    "        model=None,  # AfriCOMETEvaluator doesn't use the model directly\n",
    "        tokenizer=tokenizers['ernie-m-large']\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization for combined Afro-XLMR and LLaMA\n",
    "combined_study = run_combined_optimization(\n",
    "    models['afro-xlmr-large'],  # Encoder\n",
    "    models['meta-llama/Llama-2-7b-hf'],  # Decoder\n",
    "    tokenizers['afro-xlmr-large'],  \n",
    "    datasets['combined_afro_xlmr_llama'],  \n",
    "    config,\n",
    "    evaluators['combined_afro_xlmr_llama'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization for ERNIE-M\n",
    "ernie_study = run_ernie_optimization(\n",
    "    models['ernie-m-large'], \n",
    "    tokenizers['ernie-m-large'], \n",
    "    datasets['ernie-m-large'], \n",
    "    config, \n",
    "    evaluators['ernie-m-large']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter optimization\n",
    "try:\n",
    "    # Consolidate the results into the studies variable\n",
    "    studies = {\n",
    "        'combined_study': combined_study,\n",
    "        'ernie-m-large': ernie_study\n",
    "    }\n",
    "    # Extract best parameters\n",
    "    best_params = {model_name: study.best_params for model_name, study in studies.items()}\n",
    "\n",
    "    # Log best parameters\n",
    "    for model_name, params in best_params.items():\n",
    "        logger.info(f\"Best hyperparameters for {model_name}: {params}\")\n",
    "        config['training'][model_name] = params\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during or after hyperparameter optimization: {str(e)}\")\n",
    "    logger.exception(\"Exception details:\")\n",
    "    studies = {}  # Initialize an empty dictionary if optimization failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter analysis\n",
    "logger.info(\"Performing hyperparameter analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, study in studies.items():\n",
    "    # Plot hyperparameter importance\n",
    "    importance_fig = plot_hyperparameter_importance(study)\n",
    "    importance_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
    "\n",
    "    # Plot optimization history\n",
    "    history_fig = plot_optimization_history(study)\n",
    "    history_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
    "\n",
    "    # Plot parallel coordinate\n",
    "    parallel_fig = plot_parallel_coordinate(study)\n",
    "    parallel_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
    "\n",
    "    # Analyze and plot sensitivity\n",
    "    sensitivity = analyze_hyperparameter_sensitivity(study)\n",
    "    sensitivity_fig = plot_sensitivity_analysis(sensitivity)\n",
    "    sensitivity_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")\n",
    "\n",
    "    # Print sensitivity analysis results\n",
    "    print(f\"\\nHyperparameter Sensitivity Analysis for {model_name}:\")\n",
    "    for param, sens in sensitivity:\n",
    "        print(f\"{param}: {sens:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Hyperparameter analysis complete. Plots saved in output directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainers with the best hyperparameters from the studies\n",
    "trainers = {\n",
    "    # Using CombinedEncoderDecoderTrainer for the encoder-decoder setup\n",
    "    'combined_afro_xlmr_llama': CombinedEncoderDecoderTrainer(\n",
    "        encoder=models['afro-xlmr-large'],  # Afro-XLMR as encoder\n",
    "        decoder=models['meta-llama/Llama-2-7b-hf'],  # LLaMA as decoder\n",
    "        tokenizer=tokenizers['afro-xlmr-large'],  # Using afro-xlmr tokenizer for both (or adjust as needed)\n",
    "        config={**config, **studies['combined_afro_xlmr_llama'].best_params}  # Best hyperparameters from the tuning study\n",
    "    ),\n",
    "    'ernie-m-large': EncoderDecoderTrainer(\n",
    "        model=models['ernie-m-large'],  # Single model setup (if it's used as an encoder-decoder internally)\n",
    "        tokenizer=tokenizers['ernie-m-large'],\n",
    "        config={**config, **studies['ernie-m-large'].best_params}  # Best hyperparameters for ERNIE-M\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with the best hyperparameters\n",
    "for model_name, trainer in trainers.items():\n",
    "    logger.info(f\"Training model: {model_name}\")\n",
    "    train_dataset = datasets[model_name]['train']\n",
    "    eval_dataset = datasets[model_name]['eval']\n",
    "    \n",
    "    try:\n",
    "        trainer.train(train_dataset, eval_dataset)\n",
    "        logger.info(f\"Training completed for {model_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training of {model_name}: {str(e)}\")\n",
    "        continue  # Move to the next model if there's an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluations\n",
    "results = {\n",
    "    'classification': {},\n",
    "    'translation': {},\n",
    "    'generation': {},\n",
    "    'zero_shot': {},\n",
    "    'code_switch': {},\n",
    "    'hyperparameter_studies': studies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for all models using AfriCOMETEvaluator\n",
    "for model_name, model in models.items():\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    evaluator = evaluators[model_name]  # Assuming all evaluators are AfriCOMETEvaluator\n",
    "    \n",
    "    logger.info(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # Assuming all evaluators are AfriCOMETEvaluator\n",
    "        flores_data = data_loader.load_flores_200_benchmark()\n",
    "        if flores_data:\n",
    "            results['translation'][model_name] = evaluate_translation(model, tokenizer, flores_data, evaluator)\n",
    "        else:\n",
    "            logger.warning(f\"FLORES data not available for model: {model_name}\")\n",
    "\n",
    "        # Zero-shot and code-switch evaluations\n",
    "        zero_shot_data = datasets[model_name]['benchmark'][datasets[model_name]['benchmark']['split'] == 'zero_shot']\n",
    "        if not zero_shot_data.empty:\n",
    "            results['zero_shot'][model_name] = evaluate_zero_shot(model, tokenizer, zero_shot_data, evaluator)\n",
    "        else:\n",
    "            logger.info(f\"No zero-shot data for model: {model_name}\")\n",
    "\n",
    "        code_switch_data = datasets[model_name]['benchmark'][datasets[model_name]['benchmark']['split'] == 'code_switch']\n",
    "        if not code_switch_data.empty:\n",
    "            results['code_switch'][model_name] = evaluate_code_switch(model, tokenizer, code_switch_data, evaluator)\n",
    "        else:\n",
    "            logger.info(f\"No code-switch data for model: {model_name}\")\n",
    "\n",
    "        logger.info(f\"Completed evaluation for {model_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during evaluation of {model_name}: {str(e)}\")\n",
    "        continue  # Move to the next model if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize and log results\n",
    "results_summary = summarize_results(results, config)\n",
    "plot_results(results, config)\n",
    "log_results_to_mlflow(results, config, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Evaluation complete!\")\n",
    "print(\"Evaluation completed successfully. Results and visualizations have been saved and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results and visualizations\n",
    "display(Image(filename=f\"{config['model']['output_dir']}/overall_performance_heatmap.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameter optimization results\n",
    "for model_name in config['model']['names']:\n",
    "    print(f\"\\nHyperparameter Optimization Results for {model_name}:\")\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_summary(results_summary, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation notebook execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
