{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n"
      ],
      "metadata": {
        "id": "mzz3WkmCJJ2K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %ls"
      ],
      "metadata": {
        "id": "jhBG7XtYJSR1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shutil.rmtree('sample_data/', ignore_errors=True)"
      ],
      "metadata": {
        "id": "kuv0yNXIJSvA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"BryanLow13082018\"\n",
        "!git config --global user.email \"7alibandi7@gmail.com\"\n",
        "!git config --global user.password \"H2Odin77$'\""
      ],
      "metadata": {
        "id": "MreRytBe7luL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = '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'\n",
        "username = 'BryanLow13082018'\n",
        "repo = 'MScAI-BigData-FYP'"
      ],
      "metadata": {
        "id": "cXEAanJu78Yx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{token}@github.com/{username}/{repo}.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxTC_t2m87Y_",
        "outputId": "6b6b9ff8-2965-47d1-bff7-5c37d7632a19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MScAI-BigData-FYP'...\n",
            "remote: Enumerating objects: 5819, done.\u001b[K\n",
            "remote: Counting objects: 100% (1139/1139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (427/427), done.\u001b[K\n",
            "remote: Total 5819 (delta 289), reused 1006 (delta 191), pack-reused 4680 (from 1)\u001b[K\n",
            "Receiving objects: 100% (5819/5819), 52.76 MiB | 32.78 MiB/s, done.\n",
            "Resolving deltas: 100% (3774/3774), done.\n",
            "Updating files: 100% (3703/3703), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd {repo} && git checkout colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feW6tHwW-tIZ",
        "outputId": "c503d300-9e21-461e-d019-75c6dc7a3bea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'colab' set up to track remote branch 'colab' from 'origin'.\n",
            "Switched to a new branch 'colab'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git init"
      ],
      "metadata": {
        "id": "yTBL545wDMaN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global init.defaultBranch colab"
      ],
      "metadata": {
        "id": "0AtaRv0RDR80"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git pull https://{token}@github.com/{username}/{repo}.git"
      ],
      "metadata": {
        "id": "2FgnPnzbC6fO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {repo}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxoZBFuA9mYM",
        "outputId": "46c85916-a017-4fed-e0d8-8a0ca1aebe94"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MScAI-BigData-FYP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %ls"
      ],
      "metadata": {
        "id": "it1VeX6o9sPr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -b colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSBU5-yiI4Zi",
        "outputId": "1877f820-cb67-4682-bf11-f9a66521010f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: A branch named 'colab' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SEtDwFED923J",
        "outputId": "6b28edcc-ab10-478e-f4ba-fa1cb0d3195a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch colab\n",
            "Your branch is up to date with 'origin/colab'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git config --global core.editor \"nano\""
      ],
      "metadata": {
        "id": "mAcEM9niG7SH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git config pull.rebase false"
      ],
      "metadata": {
        "id": "R_oCGuDDMsaA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/MScAI-BigData-FYP\n",
        "# !git pull https://{token}@github.com/{username}/{repo}.git colab"
      ],
      "metadata": {
        "id": "fdRDRFFuEkiF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git remote -v"
      ],
      "metadata": {
        "id": "uDB-116X-Pfz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git commit -a -m \"Colab commit\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "f-MZCP-pEFeA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git push https://{token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "id": "rG4-0nvGAQZE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unbabel-comet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jhcstygkdDyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596f5b90-8d76-4b10-81d1-a10bb3910d0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unbabel-comet\n",
            "  Downloading unbabel_comet-2.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting entmax<2.0,>=1.1 (from unbabel-comet)\n",
            "  Downloading entmax-1.3-py3-none-any.whl.metadata (348 bytes)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.24.6)\n",
            "Collecting jsonargparse==3.13.1 (from unbabel-comet)\n",
            "  Downloading jsonargparse-3.13.1-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m900.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.1.4)\n",
            "Collecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pytorch-lightning<3.0.0,>=2.0.0 (from unbabel-comet)\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting sacrebleu<3.0.0,>=2.0.0 (from unbabel-comet)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.4.0+cpu)\n",
            "Collecting torchmetrics<0.11.0,>=0.10.2 (from unbabel-comet)\n",
            "  Downloading torchmetrics-0.10.3-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: transformers<5.0,>=4.17 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (4.44.2)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2024.1)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting portalocker (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2024.9.11)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting colorama (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lxml (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.19.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (71.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (24.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Downloading unbabel_comet-2.2.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-3.13.1-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading entmax-1.3-py3-none-any.whl (13 kB)\n",
            "Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.8/446.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabulate, protobuf, portalocker, multidict, lxml, lightning-utilities, jsonargparse, frozenlist, colorama, async-timeout, aiohappyeyeballs, yarl, sacrebleu, aiosignal, torchmetrics, entmax, aiohttp, pytorch-lightning, unbabel-comet\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.4 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 colorama-0.4.6 entmax-1.3 frozenlist-1.4.1 jsonargparse-3.13.1 lightning-utilities-0.11.7 lxml-5.3.0 multidict-6.1.0 portalocker-2.10.1 protobuf-4.25.4 pytorch-lightning-2.4.0 sacrebleu-2.4.3 tabulate-0.9.0 torchmetrics-0.10.3 unbabel-comet-2.2.2 yarl-1.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets peft"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IO7_CLQ-dGzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5a3532-c56e-4bcb-f413-80fa9dbb985f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cpu)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, peft, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.9.0\n",
            "    Uninstalling fsspec-2024.9.0:\n",
            "      Successfully uninstalled fsspec-2024.9.0\n",
            "Successfully installed datasets-3.0.0 dill-0.3.8 fsspec-2024.6.1 multiprocess-0.70.16 peft-0.12.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DNlSQFt3dK7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a54f47f-f760-4cf1-c96d-3a017ba9022e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna-integration lightning"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iPDeo4I4dM2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241a5308-8a7a-4a20-e15d-1c4693d05ee8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna-integration\n",
            "  Downloading optuna_integration-4.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting optuna (from optuna-integration)\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.7)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0+cpu)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.10.3)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Collecting alembic>=1.5.0 (from optuna->optuna-integration)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna->optuna-integration)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sqlalchemy>=1.3.0 (from optuna->optuna-integration)\n",
            "  Downloading SQLAlchemy-2.0.34-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->optuna-integration)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.3.0->optuna->optuna-integration)\n",
            "  Downloading greenlet-3.1.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.8)\n",
            "Downloading optuna_integration-4.0.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.34-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading greenlet-3.1.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (617 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.1/617.1 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna, optuna-integration, lightning\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 greenlet-3.1.0 lightning-2.4.0 optuna-4.0.0 optuna-integration-4.0.0 sqlalchemy-2.0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C9mt2qLydPKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e46f218-9156-4806-fdf3-a8579c7381b4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed[dev,1bit,cpu_adam,deepspeed_aio,inf]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l01-5NHsdSLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f9b60c-2d46-4296-c6fa-33755075007d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]\n",
            "  Downloading deepspeed-0.15.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: deepspeed 0.15.1 does not provide the extra 'cpu-adam'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: deepspeed 0.15.1 does not provide the extra 'deepspeed-aio'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting hjson (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (5.9.5)\n",
            "Collecting py-cpuinfo (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.4.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.66.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.34.2)\n",
            "Collecting clang-format==18.1.3 (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading clang_format-18.1.3-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting comet_ml>=3.41.0 (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading comet_ml-3.45.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting docutils<0.18 (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting future (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4 in /usr/lib/python3/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.6.4)\n",
            "Collecting mup (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading mup-1.0.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pre-commit>=2.20.0 (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pre_commit-3.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pytest>=7.2.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (7.4.4)\n",
            "Collecting pytest-forked (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pytest_forked-1.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pytest-randomly (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pytest_randomly-3.15.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting pytest-xdist (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting qtorch==0.3.0 (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading qtorch-0.3.0-py3-none-any.whl.metadata (455 bytes)\n",
            "Collecting recommonmark (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading recommonmark-0.7.1-py2.py3-none-any.whl.metadata (463 bytes)\n",
            "Collecting sphinx (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinx-8.0.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting sphinx-rtd-theme (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.15.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.19.0+cpu)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.44.2)\n",
            "Collecting wandb (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting deepspeed-kernels (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading deepspeed_kernels-0.0.1.dev1698255861-py3-none-manylinux1_x86_64.whl.metadata (680 bytes)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.0.3)\n",
            "Collecting lm-eval==0.3.0 (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading lm_eval-0.3.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.25.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.4.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.1.99)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.0.0)\n",
            "Collecting jsonlines (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting numexpr (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading numexpr-2.10.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting openai>=0.6.4 (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading openai-1.45.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pybind11>=2.6.2 (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pycountry (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pytablewriter (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu==1.5.0 (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.3.2)\n",
            "Collecting sqlitedict (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting zstandard (from lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu==1.5.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.10.1)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.23.0)\n",
            "Collecting python-box<7.0.0 (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting requests-toolbelt>=0.8.0 (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.32.3)\n",
            "Collecting semantic-version>=2.8.0 (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting sentry-sdk>=1.1.0 (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting simplejson (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.0.7)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.14.1)\n",
            "Collecting wurlitzer>=1.0.2 (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (13.8.1)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit>=2.20.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit>=2.20.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading identify-2.6.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit>=2.20.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit>=2.20.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit>=2.20.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading virtualenv-20.26.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.12.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=7.2.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.2.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.2.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.2.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.24.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.19.1)\n",
            "Collecting cmake>=3.24 (from deepspeed-kernels->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading cmake-3.30.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.1.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.13.1)\n",
            "Collecting py (from pytest-forked->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting execnet>=2.1 (from pytest-xdist->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting commonmark>=0.8.1 (from recommonmark->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinxcontrib-applehelp (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-devhelp (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-jsmath (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sphinxcontrib-qthelp (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.18.0)\n",
            "INFO: pip is looking at multiple versions of sphinx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinx (from deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinx-8.0.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading sphinx-8.0.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.4.6-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.4.5-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "INFO: pip is still looking at multiple versions of sphinx to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading sphinx-7.4.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.4.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.3.7-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading sphinx-7.3.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading sphinx-7.3.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading sphinx-7.3.4-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading sphinx-7.3.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading sphinx-7.3.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading sphinx-7.3.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading sphinx-7.3.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading sphinx-7.2.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading sphinx-7.2.5-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading sphinx-7.2.4-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading sphinx-7.2.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading sphinx-7.2.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading sphinx-7.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading sphinx-7.2.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading sphinx-7.1.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading sphinx-7.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading sphinx-7.1.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading sphinx-7.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-7.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-6.2.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading sphinx-6.2.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading sphinx-6.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-6.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-6.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading sphinx-6.0.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading sphinx-6.0.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading sphinx-5.3.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting snowballstemmer>=2.0 (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting babel>=2.9 (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting alabaster<0.8,>=0.7 (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting imagesize>=1.3 (from sphinx->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.66.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (10.4.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.3.2)\n",
            "Collecting setproctitle (from wandb->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.10.5)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=0.6.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=0.6.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.8.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.5.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=2.20.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2024.1)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.3.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from tqdm-multiprocess->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.4.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.0.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=0.6.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.6.4->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml>=3.41.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->mup->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.1.4)\n",
            "Collecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.3.0->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf])\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->deepspeed[1bit,cpu_adam,deepspeed_aio,dev,inf]) (3.2.2)\n",
            "Downloading clang_format-18.1.3-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.3.0-py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qtorch-0.3.0-py3-none-any.whl (21 kB)\n",
            "Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comet_ml-3.45.1-py3-none-any.whl (691 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.2/691.2 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-3.8.0-py2.py3-none-any.whl (204 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepspeed_kernels-0.0.1.dev1698255861-py3-none-manylinux1_x86_64.whl (44.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading pytest_forked-1.6.0-py3-none-any.whl (4.9 kB)\n",
            "Downloading pytest_randomly-3.15.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading recommonmark-0.7.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sphinx-5.3.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
            "Downloading babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading cmake-3.30.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.1/979.1 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading identify-2.6.1-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading openai-1.45.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.26.4-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading numexpr-2.10.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.0/405.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n",
            "Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed, mup, rouge-score, sqlitedict\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.15.1-py3-none-any.whl size=1483723 sha256=8687a7a7686f7d444b2010b64d857a778b7066201d2bf173680e74805f9fd0da\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/cb/14/9cbba50c73df044eb32a7ca29e34844c5f8959e12d22ae8b60\n",
            "  Building wheel for mup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mup: filename=mup-1.0.0-py3-none-any.whl size=23629 sha256=0fb1e637cf70aea8e13e2f1e3a7a4e4ecc27d4f12694dc322a1605dcdfb27688\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c8/88/3c23a3d10c50053b6552d2d30aee5b53ba89a47f742420036c\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=31ae3327e713e92b55fc5806c96305aaeb3b052701a341aa262c7f5b73c176e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=3b9e97358378acf3589f010d09ecd1795c8703936cea511c001d7ca34f0c46c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built deepspeed mup rouge-score sqlitedict\n",
            "Installing collected packages: sqlitedict, snowballstemmer, py-cpuinfo, ninja, hjson, everett, distlib, commonmark, clang-format, zstandard, wurlitzer, virtualenv, tqdm-multiprocess, tcolorpy, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, smmap, simplejson, setproctitle, sentry-sdk, semantic-version, sacrebleu, python-box, pycountry, pybind11, py, pathvalidate, numexpr, nodeenv, jsonlines, jiter, imagesize, identify, h11, future, execnet, dulwich, docutils, docker-pycreds, configobj, cmake, chardet, cfgv, babel, alabaster, sphinx, rouge-score, requests-toolbelt, pytest-xdist, pytest-randomly, pytest-forked, pre-commit, mbstrdecoder, httpcore, gitdb, deepspeed-kernels, typepy, sphinxcontrib-jquery, recommonmark, qtorch, httpx, gitpython, deepspeed, wandb, sphinx-rtd-theme, openai, mup, comet_ml, DataProperty, tabledata, pytablewriter, lm-eval\n",
            "  Attempting uninstall: sacrebleu\n",
            "    Found existing installation: sacrebleu 2.4.3\n",
            "    Uninstalling sacrebleu-2.4.3:\n",
            "      Successfully uninstalled sacrebleu-2.4.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unbabel-comet 2.2.2 requires sacrebleu<3.0.0,>=2.0.0, but you have sacrebleu 1.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.0.1 alabaster-0.7.16 babel-2.16.0 cfgv-3.4.0 chardet-5.2.0 clang-format-18.1.3 cmake-3.30.3 comet_ml-3.45.1 commonmark-0.9.1 configobj-5.0.8 deepspeed-0.15.1 deepspeed-kernels-0.0.1.dev1698255861 distlib-0.3.8 docker-pycreds-0.4.0 docutils-0.17.1 dulwich-0.22.1 everett-3.1.0 execnet-2.1.1 future-1.0.0 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 hjson-3.1.0 httpcore-1.0.5 httpx-0.27.2 identify-2.6.1 imagesize-1.4.1 jiter-0.5.0 jsonlines-4.0.0 lm-eval-0.3.0 mbstrdecoder-1.1.3 mup-1.0.0 ninja-1.11.1.1 nodeenv-1.9.1 numexpr-2.10.1 openai-1.45.0 pathvalidate-3.2.1 pre-commit-3.8.0 py-1.11.0 py-cpuinfo-9.0.0 pybind11-2.13.6 pycountry-24.6.1 pytablewriter-1.2.0 pytest-forked-1.6.0 pytest-randomly-3.15.0 pytest-xdist-3.6.1 python-box-6.1.0 qtorch-0.3.0 recommonmark-0.7.1 requests-toolbelt-1.0.0 rouge-score-0.1.2 sacrebleu-1.5.0 semantic-version-2.10.0 sentry-sdk-2.14.0 setproctitle-1.3.3 simplejson-3.19.3 smmap-5.0.1 snowballstemmer-2.2.0 sphinx-5.3.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.6 tqdm-multiprocess-0.0.11 typepy-1.3.2 virtualenv-20.26.4 wandb-0.18.0 wurlitzer-3.1.1 zstandard-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5zuQb0ojcwB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39cc98d-c44b-49d6-9118-ba0423f48c2b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.16.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==2.16.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.16.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting Flask<4 (from mlflow)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.2)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.1.4)\n",
            "Requirement already satisfied: pyarrow<18,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (17.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.3.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.34)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (3.0.0)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.1->mlflow)\n",
            "  Downloading databricks_sdk-0.32.1-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (3.1.43)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.16.1->mlflow) (4.6.4)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.16.1->mlflow)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.16.1->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (24.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (4.25.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.1->mlflow) (2.32.3)\n",
            "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.16.1->mlflow)\n",
            "  Downloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.4)\n",
            "Collecting itsdangerous>=2.1.2 (from Flask<4->mlflow)\n",
            "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting blinker>=1.6.2 (from Flask<4->mlflow)\n",
            "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow) (2.27.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.1->mlflow) (4.0.11)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.1->mlflow)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.16.1->mlflow)\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.16.1->mlflow) (3.20.1)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.1->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4->mlflow) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.1->mlflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.1->mlflow) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.1->mlflow) (2024.8.30)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.1->mlflow) (1.14.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.1->mlflow) (5.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.16.1-py3-none-any.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.16.1-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m20.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
            "Downloading databricks_sdk-0.32.1-py3-none-any.whl (551 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.0/552.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: aniso8601, sqlparse, itsdangerous, importlib-metadata, gunicorn, graphql-core, deprecated, blinker, opentelemetry-api, graphql-relay, Flask, docker, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "  Attempting uninstall: blinker\n",
            "    Found existing installation: blinker 1.4\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-distutils-installed-package\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Cannot uninstall blinker 1.4\n",
            "\u001b[31m╰─>\u001b[0m It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OXdZD2tOcYqk",
        "outputId": "adf1c8f3-68db-429f-a750-ff09d16239fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-42ff658fe814>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    303\u001b[0m             )\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries and modules\n",
        "import os\n",
        "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'  # Suppress Git warnings\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Enable CUDA launch blocking for debugging\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"  # Enable CUDA device-side assertions\n",
        "os.environ['MLFLOW_FLATTEN_PARAMS'] = 'true' # Flatten parameters parameters for logging\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/MScAI-BigData-FYP/py')  # Add the parent directory to the Python path\n",
        "import gc\n",
        "import torch\n",
        "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import logging\n",
        "import mlflow\n",
        "import optuna\n",
        "import json\n",
        "import traceback\n",
        "from accelerate import Accelerator\n",
        "from IPython.display import Image, display\n",
        "from transformers import AutoConfig\n",
        "\n",
        "from dataset_loader import DatasetLoader\n",
        "from utils import get_model, set_seed, load_config, get_device, CustomDataset\n",
        "from models.llama2_decoder import Llama2Decoder  # Import Llama2Decoder model\n",
        "from models.ernie_m import ErnieM  # Import ErnieM model\n",
        "from evaluators.africomet_evaluator import AfriCOMETEvaluator\n",
        "from trainers.encoder_decoder_trainer import EncoderDecoderTrainer\n",
        "from trainers.combined_encoder_decoder_trainer import CombinedEncoderDecoderTrainer\n",
        "from hyperparameter_analysis import (plot_hyperparameter_importance, plot_optimization_history,\n",
        "                                     plot_parallel_coordinate, analyze_hyperparameter_sensitivity,\n",
        "                                     plot_sensitivity_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qfzvzem5cYql"
      },
      "outputs": [],
      "source": [
        "def setup_logging(config):\n",
        "    \"\"\"\n",
        "    Set up logging configuration based on the provided config.\n",
        "\n",
        "    This function initializes the logging system with the specified log level,\n",
        "    format, and output file from the configuration.\n",
        "\n",
        "    Args:\n",
        "        config (dict): A dictionary containing logging configuration.\n",
        "\n",
        "    Returns:\n",
        "        logging.Logger: Configured logger object.\n",
        "    \"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=getattr(logging, config['logging']['log_level']),\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        filename=config['logging']['log_file']\n",
        "    )\n",
        "    return logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mKVSgFmHcYql"
      },
      "outputs": [],
      "source": [
        "def clear_memory():\n",
        "    \"\"\"\n",
        "    Clear unused memory to prevent out-of-memory errors.\n",
        "\n",
        "    This function uses Python's garbage collector and PyTorch's CUDA memory\n",
        "    cache clearing (if available) to free up memory.\n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "            logging.info(\"CUDA cache cleared successfully\")\n",
        "        except RuntimeError as e:\n",
        "            logging.warning(f\"Failed to clear CUDA cache: {str(e)}\")\n",
        "    logging.info(\"Memory cleared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Dr6fcW0bcYql"
      },
      "outputs": [],
      "source": [
        "def load_datasets(data_loader):\n",
        "    \"\"\"\n",
        "    Load datasets using the provided DatasetLoader object.\n",
        "\n",
        "    This function attempts to load all datasets specified in the configuration\n",
        "    using the DatasetLoader. It includes error handling for common issues.\n",
        "\n",
        "    Args:\n",
        "        data_loader (DatasetLoader): An instance of the DatasetLoader class.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: A dictionary of loaded datasets, or None if loading fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Loading and preparing datasets...\")\n",
        "        return data_loader.load_datasets()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading datasets: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_num_labels(dataset):\n",
        "    \"\"\"\n",
        "    Determine the number of labels from the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: The dataset to analyze.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of labels in the dataset.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If unable to determine the number of labels.\n",
        "    \"\"\"\n",
        "    # This is a placeholder implementation. Modify this based on your actual dataset structure.\n",
        "    if hasattr(dataset, 'features') and 'labels' in dataset.features:\n",
        "        return dataset.features['labels'].num_classes\n",
        "    elif 'labels' in dataset[0]:\n",
        "        return dataset[0]['labels'].shape[-1]\n",
        "    else:\n",
        "        raise ValueError(\"Unable to determine number of labels from the dataset\")"
      ],
      "metadata": {
        "id": "mZSI5YdNxGos"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mB5oJ2dvcYql"
      },
      "outputs": [],
      "source": [
        "def objective_combined(trial, encoder, decoder, tokenizer, train_dataset, eval_dataset, config, evaluator):\n",
        "    \"\"\"\n",
        "    Objective function for hyperparameter optimization of the combined Afro-XLMR and LLaMA model.\n",
        "\n",
        "    This function is called by Optuna for each trial to evaluate the performance of the model\n",
        "    with different hyperparameter settings.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The Optuna trial object used for hyperparameter suggestions.\n",
        "        encoder (torch.nn.Module): The Afro-XLMR encoder model to be trained and evaluated.\n",
        "        decoder (torch.nn.Module): The LLaMA decoder model to be trained and evaluated.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for processing text.\n",
        "        train_dataset (Dataset): The dataset used for training the model.\n",
        "        eval_dataset (Dataset): The dataset used for evaluating the model.\n",
        "        config (dict): The configuration dictionary containing hyperparameter ranges and settings.\n",
        "        evaluator (Evaluator): The evaluator object used to compute evaluation metrics.\n",
        "\n",
        "    Returns:\n",
        "        float: The evaluation metric to be minimized (lower is better).\n",
        "    \"\"\"\n",
        "    # Hyperparameter suggestions\n",
        "    encoder_lr = trial.suggest_float('encoder_lr', 1e-6, 1e-4, log=True)\n",
        "    decoder_lr = trial.suggest_float('decoder_lr', 1e-6, 1e-4, log=True)\n",
        "    num_train_epochs = trial.suggest_int('num_train_epochs', 1, 3)\n",
        "    per_device_train_batch_size = 1  # Fixed to 1 to reduce memory usage\n",
        "    weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n",
        "    warmup_steps = trial.suggest_int('warmup_steps', 50, 200)\n",
        "    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [32, 64, 128])  # Increased accumulation steps\n",
        "\n",
        "    # Determine the number of labels\n",
        "    try:\n",
        "        num_labels = determine_num_labels(train_dataset)\n",
        "        logging.info(f\"Determined number of labels: {num_labels}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error determining number of labels: {str(e)}\")\n",
        "        return float('inf')  # Return a high metric to minimize\n",
        "\n",
        "    # Update config with trial-suggested hyperparameters\n",
        "    trial_config = config.copy()\n",
        "    trial_config.update({\n",
        "        \"encoder_lr\": encoder_lr,\n",
        "        \"decoder_lr\": decoder_lr,\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "        \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "        \"per_device_eval_batch_size\": per_device_train_batch_size,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"warmup_steps\": warmup_steps,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"fp16\": True,\n",
        "        \"bf16\": False,  # Set to True if your GPU supports bfloat16\n",
        "        \"evaluation_strategy\": \"steps\",\n",
        "        \"eval_steps\": 200,\n",
        "        \"save_steps\": 200,\n",
        "        \"logging_steps\": 50,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"offload_optimizer\": True,\n",
        "        \"offload_scheduler\": True\n",
        "    })\n",
        "\n",
        "    logging.info(f\"Trial {trial.number}: Starting with hyperparameters: {trial_config}\")\n",
        "\n",
        "    try:\n",
        "        # Enable gradient checkpointing\n",
        "        encoder.gradient_checkpointing_enable()\n",
        "        decoder.gradient_checkpointing_enable()\n",
        "\n",
        "        # Offload encoder to CPU\n",
        "        encoder_config = AutoConfig.from_pretrained(encoder.config._name_or_path)\n",
        "        encoder_config.gradient_checkpointing = True\n",
        "        encoder.cpu()\n",
        "\n",
        "        # Keep decoder on GPU but enable efficient memory usage\n",
        "        decoder.config.use_cache = False\n",
        "\n",
        "        # Initialize the trainer with offloading and sequential processing\n",
        "        print(\"CombinedEncoderDecoderTrainer:\", CombinedEncoderDecoderTrainer)\n",
        "        print(\"CombinedEncoderDecoderTrainer.__init__:\", CombinedEncoderDecoderTrainer.__init__)\n",
        "        trainer = CombinedEncoderDecoderTrainer(encoder, decoder, tokenizer, trial_config, num_labels)\n",
        "\n",
        "        # Log some information about the datasets\n",
        "        logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
        "        logging.info(f\"Eval dataset size: {len(eval_dataset)}\")\n",
        "\n",
        "        # Train the model\n",
        "        train_results = trainer.train(train_dataset, eval_dataset)\n",
        "\n",
        "        # Evaluate the model\n",
        "        eval_results = evaluator.evaluate(trainer.encoder, trainer.decoder, trainer.combine_layer, eval_dataset)\n",
        "\n",
        "        # Log the results\n",
        "        logging.info(f\"Trial {trial.number}: Training completed. Results: {train_results}\")\n",
        "        logging.info(f\"Trial {trial.number}: Evaluation results: {eval_results}\")\n",
        "\n",
        "        # Return the metric to be minimized\n",
        "        return eval_results['loss']  # Assuming 'loss' is the metric to be minimized\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in trial {trial.number}: {str(e)}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return float('inf')  # Return a high metric to minimize in case of error\n",
        "\n",
        "    finally:\n",
        "        # Clean up\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bt8B5SiUcYqm"
      },
      "outputs": [],
      "source": [
        "def objective_ernie(trial, model, tokenizer, train_dataset, eval_dataset, config, evaluator):\n",
        "    \"\"\"\n",
        "    Optimized objective function for hyperparameter optimization of the ERNIE-M model.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): Optuna trial object for hyperparameter suggestions.\n",
        "        model (ErnieM): The ERNIE-M model to be optimized.\n",
        "        tokenizer: The tokenizer associated with the model.\n",
        "        train_dataset: Dataset for training.\n",
        "        eval_dataset: Dataset for evaluation.\n",
        "        config (dict): Configuration dictionary containing hyperparameter ranges and other settings.\n",
        "        evaluator: The AfriCOMET evaluator object for computing metrics.\n",
        "\n",
        "    Returns:\n",
        "        float: The evaluation metric to be minimized (or float('inf') if an error occurs).\n",
        "    \"\"\"\n",
        "    torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "    patience = 3\n",
        "    best_metric = float('inf')\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    with mlflow.start_run(nested=True):\n",
        "        try:\n",
        "            # Suggest hyperparameters\n",
        "            lr_min = min(float(config['hyperparameters']['learning_rate_min']), float(config['hyperparameters']['learning_rate_max']))\n",
        "            lr_max = max(float(config['hyperparameters']['learning_rate_min']), float(config['hyperparameters']['learning_rate_max']))\n",
        "            learning_rate = trial.suggest_float('learning_rate', lr_min, lr_max, log=True)\n",
        "            num_train_epochs = trial.suggest_int('num_train_epochs', 1, 2)\n",
        "            per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8, 16])\n",
        "            per_device_eval_batch_size = per_device_train_batch_size\n",
        "            weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n",
        "            warmup_steps = trial.suggest_int('warmup_steps', 50, 200)\n",
        "            gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n",
        "\n",
        "            # Log the hyperparameters to MLflow\n",
        "            mlflow.log_params({\n",
        "                \"learning_rate\": learning_rate,\n",
        "                \"num_train_epochs\": num_train_epochs,\n",
        "                \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "                \"per_device_eval_batch_size\": per_device_eval_batch_size,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"warmup_steps\": warmup_steps,\n",
        "                \"gradient_accumulation_steps\": gradient_accumulation_steps\n",
        "            })\n",
        "\n",
        "            # Update the config with the trial-suggested hyperparameters\n",
        "            config.update({\n",
        "                \"learning_rate\": learning_rate,\n",
        "                \"num_train_epochs\": num_train_epochs,\n",
        "                \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "                \"per_device_eval_batch_size\": per_device_eval_batch_size,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"warmup_steps\": warmup_steps,\n",
        "                \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "                \"fp16\": True,\n",
        "                \"evaluation_strategy\": \"steps\",\n",
        "                \"eval_steps\": 200,\n",
        "                \"save_steps\": 200,\n",
        "                \"logging_steps\": 50,\n",
        "                \"max_grad_norm\": 1.0,\n",
        "            })\n",
        "\n",
        "            # Initialize the trainer\n",
        "            trainer = EncoderDecoderTrainer(model=model, tokenizer=tokenizer, config=config)\n",
        "\n",
        "            # Log dataset sizes and sample batch\n",
        "            logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
        "            logging.info(f\"Eval dataset size: {len(eval_dataset)}\")\n",
        "            sample_batch = next(iter(torch.utils.data.DataLoader(train_dataset, batch_size=per_device_train_batch_size)))\n",
        "            logging.info(f\"Sample batch keys: {sample_batch.keys()}\")\n",
        "            logging.info(f\"Sample input_ids shape: {sample_batch['input_ids'].shape}\")\n",
        "            logging.info(f\"Sample labels shape: {sample_batch['labels'].shape}\")\n",
        "\n",
        "            # Train the model\n",
        "            train_loss, eval_loss = trainer.train(train_dataset, eval_dataset)\n",
        "\n",
        "            # Log the train and eval losses\n",
        "            mlflow.log_metric(\"final_train_loss\", train_loss)\n",
        "            mlflow.log_metric(\"final_eval_loss\", eval_loss)\n",
        "\n",
        "            logging.info(f\"Final training loss: {train_loss:.4f}\")\n",
        "            logging.info(f\"Final evaluation loss: {eval_loss:.4f}\")\n",
        "\n",
        "            # Begin AfriCOMET evaluation\n",
        "            logging.info(\"Starting AfriCOMET evaluation...\")\n",
        "            eval_results = evaluator.evaluate(model, tokenizer, eval_dataset)\n",
        "\n",
        "            # Log the evaluation results, handling potential NaN values\n",
        "            for key, value in eval_results.items():\n",
        "                if isinstance(value, dict):\n",
        "                    for subkey, subvalue in value.items():\n",
        "                        if isinstance(subvalue, (int, float)) and not torch.isnan(torch.tensor(subvalue)):\n",
        "                            mlflow.log_metric(f\"{key}_{subkey}\", subvalue)\n",
        "                elif isinstance(value, (int, float)) and not torch.isnan(torch.tensor(value)):\n",
        "                    mlflow.log_metric(key, value)\n",
        "\n",
        "            # Calculate the optimization metric\n",
        "            africomet_score = eval_results.get('average_score', 0)\n",
        "            optimization_metric = (1 - africomet_score) + eval_loss  # Lower is better\n",
        "            logging.info(f\"Optimization metric (lower is better): {optimization_metric}\")\n",
        "\n",
        "            return optimization_metric\n",
        "\n",
        "        except ValueError as ve:\n",
        "            logging.error(f\"ValueError in objective_ernie: {str(ve)}\")\n",
        "            logging.error(\"This might be due to a mismatch in input or label shapes.\")\n",
        "            mlflow.log_metric(\"failed_due_to_error\", 1)\n",
        "            return float('inf')\n",
        "\n",
        "        except RuntimeError as re:\n",
        "            logging.error(f\"RuntimeError in objective_ernie: {str(re)}\")\n",
        "            logging.error(\"This might be due to CUDA out of memory. Try reducing batch size or model size.\")\n",
        "            mlflow.log_metric(\"failed_due_to_error\", 1)\n",
        "            return float('inf')\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred in objective_ernie: {str(e)}\")\n",
        "            logging.error(f\"Error type: {type(e).__name__}\")\n",
        "            logging.error(f\"Error args: {e.args}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            mlflow.log_metric(\"failed_due_to_error\", 1)\n",
        "            return float('inf')\n",
        "\n",
        "        finally:\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BgUUCv6mcYqm"
      },
      "outputs": [],
      "source": [
        "def run_combined_optimization(encoder, decoder, tokenizer, datasets, config, evaluator):\n",
        "    \"\"\"\n",
        "    Run hyperparameter optimization for the combined Afro-XLMR and LLaMA model.\n",
        "\n",
        "    Args:\n",
        "        encoder: The Afro-XLMR encoder model.\n",
        "        decoder: The LLaMA decoder model.\n",
        "        tokenizer: The tokenizer for the models.\n",
        "        datasets: Dictionary containing 'train' and 'eval' datasets.\n",
        "        config: Configuration dictionary.\n",
        "        evaluator: AfriCOMET evaluator object.\n",
        "\n",
        "    Returns:\n",
        "        optuna.Study: The completed Optuna study object.\n",
        "    \"\"\"\n",
        "    mlflow.end_run()  # End any existing runs\n",
        "\n",
        "    logging.info(\"Starting hyperparameter optimization for combined Afro-XLMR and LLaMA\")\n",
        "\n",
        "    # Log GPU memory information\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    available_memory = total_memory - torch.cuda.memory_allocated()\n",
        "    logging.info(f\"Total GPU memory: {total_memory / 1e9:.2f} GB\")\n",
        "    logging.info(f\"Available GPU memory before optimization: {available_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    try:\n",
        "        with mlflow.start_run(run_name=\"optimization_combined\"):\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            study.optimize(\n",
        "                lambda trial: objective_combined(\n",
        "                    trial, encoder, decoder, tokenizer,\n",
        "                    datasets['train'], datasets['eval'],\n",
        "                    config, evaluator\n",
        "                ),\n",
        "                n_trials=config['hyperparameters']['n_trials'],\n",
        "                timeout=3600,\n",
        "                catch=(Exception,),\n",
        "                n_jobs=1\n",
        "            )\n",
        "\n",
        "            if study.best_trial:\n",
        "                best_params = study.best_params\n",
        "                for param, value in best_params.items():\n",
        "                    mlflow.log_param(f\"best_{param}\", value)\n",
        "                mlflow.log_metric(\"best_score\", study.best_value)\n",
        "            else:\n",
        "                logging.warning(\"No completed trials found.\")\n",
        "\n",
        "    except optuna.exceptions.OptunaError as e:\n",
        "        logging.error(f\"Optuna error during hyperparameter optimization: {str(e)}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during hyperparameter optimization: {str(e)}\")\n",
        "        logging.exception(\"Exception details:\")\n",
        "        return None\n",
        "\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        logging.info(f\"GPU memory after optimization: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "    return study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qDY5ZyTCcYqm"
      },
      "outputs": [],
      "source": [
        "def run_ernie_optimization(model, tokenizer, datasets, config, evaluator):\n",
        "    \"\"\"\n",
        "    Run hyperparameter optimization for ERNIE-M model.\n",
        "\n",
        "    Args:\n",
        "        model: The ERNIE-M model.\n",
        "        tokenizer: The tokenizer for ERNIE-M.\n",
        "        datasets: Dictionary containing 'train' and 'eval' datasets.\n",
        "        config: Configuration dictionary.\n",
        "        evaluator: Evaluator object for ERNIE-M.\n",
        "\n",
        "    Returns:\n",
        "        optuna.Study: The completed Optuna study object.\n",
        "    \"\"\"\n",
        "    # End any existing MLflow runs\n",
        "    mlflow.end_run()\n",
        "\n",
        "    logging.info(\"Starting hyperparameter optimization for ERNIE-M\")\n",
        "\n",
        "    # Log GPU memory information\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    available_memory = total_memory - torch.cuda.memory_allocated()\n",
        "    logging.info(f\"Total GPU memory: {total_memory/1e9:.2f} GB\")\n",
        "    logging.info(f\"Available GPU memory before optimization: {available_memory/1e9:.2f} GB\")\n",
        "\n",
        "    try:\n",
        "        # Start a new MLflow run for this optimization\n",
        "        with mlflow.start_run(run_name=\"optimization_ernie_m\"):\n",
        "            # Create an Optuna study object\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "\n",
        "            # Log dataset sizes and model information\n",
        "            logging.info(f\"Train dataset size: {len(datasets['train'])}\")\n",
        "            logging.info(f\"Eval dataset size: {len(datasets['eval'])}\")\n",
        "            logging.info(f\"Model type: {type(model).__name__}\")\n",
        "            logging.info(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
        "\n",
        "            # Run the optimization\n",
        "            study.optimize(\n",
        "                lambda trial: objective_ernie(\n",
        "                    trial,\n",
        "                    model,\n",
        "                    tokenizer,\n",
        "                    datasets['train'],\n",
        "                    datasets['eval'],\n",
        "                    config,\n",
        "                    evaluator\n",
        "                ),\n",
        "                n_trials=config['hyperparameters']['n_trials'],\n",
        "                timeout=3600,\n",
        "                catch=(Exception,),\n",
        "                n_jobs=1,  # Run trials sequentially to avoid GPU conflicts\n",
        "                show_progress_bar=True\n",
        "            )\n",
        "\n",
        "            # Log the best parameters and score\n",
        "            best_params = study.best_params\n",
        "            for param, value in best_params.items():\n",
        "                mlflow.log_param(f\"best_{param}\", value)\n",
        "            mlflow.log_metric(\"best_score\", study.best_value)\n",
        "\n",
        "            # Save the optimization results\n",
        "            save_results(\"ernie_m\", study, config)\n",
        "\n",
        "        logging.info(\"Completed hyperparameter optimization for ERNIE-M\")\n",
        "        logging.info(f\"Best trial: {study.best_trial}\")\n",
        "        print_best_trial_info(study.best_trial)\n",
        "\n",
        "    except optuna.exceptions.TrialPruned:\n",
        "        logging.info(\"Trial was pruned.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during hyperparameter optimization for ERNIE-M: {str(e)}\")\n",
        "        logging.exception(\"Exception details:\")\n",
        "        return None\n",
        "\n",
        "    finally:\n",
        "        # Clean up resources\n",
        "        clear_memory()\n",
        "        if torch.cuda.is_available():\n",
        "            model.cpu()\n",
        "            torch.cuda.empty_cache()\n",
        "        logging.info(f\"GPU memory after optimization: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "        logging.info(\"Memory cleared after ERNIE-M optimization\")\n",
        "\n",
        "    return study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bc_d1V4CcYqm"
      },
      "outputs": [],
      "source": [
        "def save_results(model_name, study, config):\n",
        "    \"\"\"Save the optimization results to disk.\"\"\"\n",
        "    output_dir = os.path.join(config['model']['output_dir'], 'optimization_results')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    result = {\n",
        "        'best_params': study.best_params,\n",
        "        'best_value': study.best_value,\n",
        "        'best_trial': study.best_trial.number\n",
        "    }\n",
        "    with open(os.path.join(output_dir, f\"{model_name}_optimization_results.json\"), 'w') as f:\n",
        "        json.dump(result, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "C1Et1FQycYqm"
      },
      "outputs": [],
      "source": [
        "def print_best_trial_info(trial):\n",
        "    \"\"\"Print detailed information about the best trial.\"\"\"\n",
        "    print(\"\\nBest Trial Information:\")\n",
        "    print(f\"  Value: {trial.value}\")\n",
        "    print(\"  Params:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "    print(f\"  Trial number: {trial.number}\")\n",
        "    print(f\"  DateTime start: {trial.datetime_start}\")\n",
        "    print(f\"  DateTime complete: {trial.datetime_complete}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "enQ83ykvcYqm"
      },
      "outputs": [],
      "source": [
        "def evaluate_classification(model, dataset, evaluator):\n",
        "    \"\"\"\n",
        "    Evaluate classification performance.\n",
        "\n",
        "    This function evaluates the classification performance of the model on the given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        dataset (Dataset): The dataset to evaluate on.\n",
        "        evaluator (ClassificationEvaluator): The evaluator to use.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation results.\n",
        "    \"\"\"\n",
        "    clear_memory()\n",
        "    logging.info(f\"Evaluating classification performance for {dataset.name}\")\n",
        "    results = evaluator.evaluate(dataset)\n",
        "    logging.info(f\"Classification Report:\\n{results['classification_report']}\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    results['confusion_matrix_plot'].savefig(f\"{model}/{dataset.name}_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "    clear_memory()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "9IBjGbCZcYqm"
      },
      "outputs": [],
      "source": [
        "def evaluate_translation(model, tokenizer, eval_dataset, evaluator):\n",
        "    \"\"\"\n",
        "    Evaluate translation performance using the existing dataset structure.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The translation model to evaluate.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "        eval_dataset (Dataset): The evaluation dataset.\n",
        "        evaluator (AfriCOMETEvaluator): The evaluator to use.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation results.\n",
        "    \"\"\"\n",
        "    results = {'translations': {}}\n",
        "    target_languages = ['swh', 'kin', 'lug']  # ISO codes for Swahili, Kinyarwanda, and Luganda\n",
        "    english_code = 'eng'\n",
        "\n",
        "    # Convert dataset to DataFrame if it's not already\n",
        "    if not isinstance(eval_dataset, pd.DataFrame):\n",
        "        if hasattr(eval_dataset, 'data'):\n",
        "            eval_dataset = eval_dataset.data\n",
        "        else:\n",
        "            eval_dataset = pd.DataFrame(eval_dataset)\n",
        "\n",
        "    # Log dataset information\n",
        "    logging.info(f\"Evaluation dataset shape: {eval_dataset.shape}\")\n",
        "    logging.info(f\"Languages in dataset: {eval_dataset['language'].unique()}\")\n",
        "    logging.info(f\"Splits in dataset: {eval_dataset['split'].unique()}\")\n",
        "\n",
        "    for lang in target_languages:\n",
        "        try:\n",
        "            eng_texts = eval_dataset[eval_dataset['language'] == english_code]['text'].tolist()\n",
        "            lang_texts = eval_dataset[eval_dataset['language'] == lang]['text'].tolist()\n",
        "\n",
        "            if not eng_texts or not lang_texts:\n",
        "                logging.warning(f\"No data found for {english_code} to {lang} translation. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            logging.info(f\"Translating {len(eng_texts)} sentences from {english_code} to {lang}\")\n",
        "\n",
        "            inputs = tokenizer(eng_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(model.device)\n",
        "            with torch.no_grad():\n",
        "                generated = model.generate(**inputs, max_length=128)\n",
        "            translations = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "\n",
        "            scores = evaluator.evaluate(eng_texts, translations, lang_texts)\n",
        "            results['translations'][f'{english_code}_to_{lang}'] = scores\n",
        "\n",
        "            logging.info(f\"Translation scores for {english_code} to {lang}: {scores}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during translation evaluation for language {lang}: {str(e)}\")\n",
        "            results['translations'][f'{english_code}_to_{lang}'] = {'error': str(e)}\n",
        "\n",
        "    # Calculate average score\n",
        "    valid_scores = [score['average_score'] for score in results['translations'].values()\n",
        "                    if isinstance(score, dict) and 'average_score' in score]\n",
        "\n",
        "    if valid_scores:\n",
        "        results['average_score'] = np.mean(valid_scores)\n",
        "        logging.info(f\"Overall average translation score: {results['average_score']}\")\n",
        "    else:\n",
        "        results['average_score'] = np.nan\n",
        "        logging.warning(\"No valid scores for translations\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Z2OtIGoVcYqn"
      },
      "outputs": [],
      "source": [
        "def evaluate_generation(model, prompt_texts, evaluator):\n",
        "    \"\"\"\n",
        "    Evaluate text generation performance.\n",
        "\n",
        "    This function evaluates the text generation performance of the model using\n",
        "    the provided evaluator.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The generation model to evaluate.\n",
        "        prompt_texts (list): A list of prompt texts for generation.\n",
        "        evaluator (GenerationEvaluator): The evaluator to use.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation results.\n",
        "    \"\"\"\n",
        "    clear_memory()\n",
        "    logging.info(\"Evaluating text generation performance\")\n",
        "    generated_texts = model.generate(prompt_texts)\n",
        "    generation_results = evaluator.evaluate(prompt_texts, generated_texts)\n",
        "    logging.info(f\"Average perplexity: {generation_results['average_perplexity']}\")\n",
        "    clear_memory()\n",
        "    return generation_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UqiWAfhjcYqn"
      },
      "outputs": [],
      "source": [
        "def evaluate_zero_shot(model, tokenizer, dataset, evaluator):\n",
        "    \"\"\"\n",
        "    Evaluate zero-shot performance.\n",
        "\n",
        "    This function evaluates the zero-shot performance of the model on the given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "        dataset (Dataset): The dataset to evaluate on.\n",
        "        evaluator: The evaluator to use.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation results.\n",
        "    \"\"\"\n",
        "    clear_memory()\n",
        "    logging.info(\"Evaluating zero-shot performance\")\n",
        "\n",
        "    inputs = tokenizer(dataset['source_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs)\n",
        "\n",
        "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    results = evaluator.evaluate(dataset['source_text'].tolist(), generated_texts, dataset['target_text'].tolist())\n",
        "\n",
        "    logging.info(f\"Zero-shot performance: {results}\")\n",
        "    clear_memory()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YzPU13L4cYqn"
      },
      "outputs": [],
      "source": [
        "def evaluate_code_switch(model, tokenizer, dataset, evaluator):\n",
        "    \"\"\"\n",
        "    Evaluate code-switch performance.\n",
        "\n",
        "    This function evaluates the performance of the model on code-switched text.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "        dataset (Dataset): The dataset containing code-switched text.\n",
        "        evaluator: The evaluator to use.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation results.\n",
        "    \"\"\"\n",
        "    clear_memory()\n",
        "    logging.info(\"Evaluating code-switch performance\")\n",
        "\n",
        "    inputs = tokenizer(dataset['code_switched_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs).logits\n",
        "\n",
        "    predictions = torch.argmax(outputs, dim=-1).tolist()\n",
        "\n",
        "    results = evaluator.evaluate(predictions, dataset['label'].tolist())\n",
        "\n",
        "    logging.info(f\"Code-switch performance: {results}\")\n",
        "    clear_memory()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "rJ6cpQcCcYqn"
      },
      "outputs": [],
      "source": [
        "def summarize_results(results, config):\n",
        "    \"\"\"\n",
        "    Summarize evaluation results.\n",
        "\n",
        "    This function summarizes the results from various evaluation tasks and saves\n",
        "    them to files.\n",
        "\n",
        "    Args:\n",
        "        results (dict): A dictionary containing results from all evaluation tasks.\n",
        "        config (dict): A dictionary containing configuration information.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary of evaluation results.\n",
        "    \"\"\"\n",
        "    logging.info(\"Summarizing results...\")\n",
        "\n",
        "    summary = {}\n",
        "    for model_name in config['model']['names']:\n",
        "        summary[model_name] = {\n",
        "            'classification': results['classification'][model_name]['classification_report']['accuracy'],\n",
        "            'translation': results['translation'][model_name]['average_score'] if results['translation'] else None,\n",
        "            'generation': results['generation'][model_name]['average_perplexity'] if results['generation'] else None,\n",
        "            'zero_shot': results['zero_shot'][model_name]['accuracy'] if results['zero_shot'] else None,\n",
        "            'code_switch': results['code_switch'][model_name]['accuracy'] if results['code_switch'] else None\n",
        "        }\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "    logging.info(\"Evaluation Results Summary:\")\n",
        "    logging.info(summary_df)\n",
        "\n",
        "    # Save results\n",
        "    summary_df.to_csv(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
        "\n",
        "    with open(f\"{config['model']['output_dir']}/all_results.txt\", 'w') as f:\n",
        "        f.write(str(results))\n",
        "\n",
        "    return summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "T9CE3TN_cYqn"
      },
      "outputs": [],
      "source": [
        "def plot_results(results, config):\n",
        "    \"\"\"\n",
        "    Plot evaluation results.\n",
        "\n",
        "    Args:\n",
        "        results (dict): A dictionary containing results from all evaluation tasks.\n",
        "        config (dict): A dictionary containing configuration information.\n",
        "    \"\"\"\n",
        "    for model_name in config['model']['names']:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        if model_name in results['classification']:\n",
        "            sns.heatmap(results['classification'][model_name]['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
        "            plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_confusion_matrix.png\")\n",
        "            plt.close()\n",
        "\n",
        "        if model_name in results['translation']:\n",
        "            plt.bar(results['translation'][model_name].keys(), results['translation'][model_name].values())\n",
        "            plt.title(f\"Translation Scores - {model_name}\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_translation_scores.png\")\n",
        "            plt.close()\n",
        "\n",
        "        if model_name in results['generation']:\n",
        "            plt.hist(results['generation'][model_name]['perplexities'], bins=20)\n",
        "            plt.title(f\"Perplexity Distribution - {model_name}\")\n",
        "            plt.xlabel(\"Perplexity\")\n",
        "            plt.ylabel(\"Frequency\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_perplexity_distribution.png\")\n",
        "            plt.close()\n",
        "\n",
        "    # Plot overall performance comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    performance_data = {\n",
        "        model: {\n",
        "            'Classification': results['classification'].get(model, {}).get('accuracy', 0),\n",
        "            'Translation': results['translation'].get(model, {}).get('average_score', 0),\n",
        "            'Generation': 1 / results['generation'].get(model, {}).get('average_perplexity', 1),  # Inverse of perplexity\n",
        "            'Zero-shot': results['zero_shot'].get(model, {}).get('accuracy', 0),\n",
        "            'Code-switch': results['code_switch'].get(model, {}).get('accuracy', 0)\n",
        "        } for model in config['model']['names']\n",
        "    }\n",
        "    df = pd.DataFrame(performance_data).T\n",
        "    sns.heatmap(df, annot=True, cmap='YlGnBu')\n",
        "    plt.title(\"Model Performance Across Tasks\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7iYGSwEScYqn"
      },
      "outputs": [],
      "source": [
        "def log_results_to_mlflow(results, config, best_params):\n",
        "    \"\"\"\n",
        "    Log results to MLflow.\n",
        "\n",
        "    This function logs the evaluation results, model parameters, and artifacts to MLflow.\n",
        "\n",
        "    Args:\n",
        "        results (dict): A dictionary containing results from all evaluation tasks.\n",
        "        config (dict): A dictionary containing configuration information.\n",
        "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
        "    \"\"\"\n",
        "    with mlflow.start_run():\n",
        "        # Log hyperparameters for each model\n",
        "        for model_name, params in best_params.items():\n",
        "            for param, value in params.items():\n",
        "                mlflow.log_param(f\"{model_name}_{param}\", value)\n",
        "\n",
        "        # Log model information\n",
        "        for model_name in config['model']['names']:\n",
        "            mlflow.log_param(f\"{model_name}_model\", model_name)\n",
        "\n",
        "        # Log metrics\n",
        "        for model_name, metrics in results['summary'].items():\n",
        "            for metric, value in metrics.items():\n",
        "                if value is not None:\n",
        "                    mlflow.log_metric(f\"{model_name}_{metric}\", value)\n",
        "\n",
        "        # Log artifacts\n",
        "        mlflow.log_artifact(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
        "        mlflow.log_artifact(f\"{config['model']['output_dir']}/all_results.txt\")\n",
        "        mlflow.log_artifact(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
        "\n",
        "        # Log hyperparameter optimization plots\n",
        "        for model_name in config['model']['names']:\n",
        "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
        "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
        "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
        "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jTx3gqoecYqn"
      },
      "outputs": [],
      "source": [
        "def print_results_summary(results_summary, best_params):\n",
        "    \"\"\"\n",
        "    Print a summary of the evaluation results and best hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        results_summary (dict): A dictionary containing summarized results.\n",
        "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
        "    \"\"\"\n",
        "    print(\"\\n===== EVALUATION RESULTS SUMMARY =====\")\n",
        "\n",
        "    if 'classification' in results_summary:\n",
        "        print(\"\\nClassification Results:\")\n",
        "        for dataset, metrics in results_summary['classification'].items():\n",
        "            print(f\"\\n{dataset}:\")\n",
        "            for metric, value in metrics.items():\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    if 'translation' in results_summary:\n",
        "        print(\"\\nTranslation Results:\")\n",
        "        print(f\"  FLORES-200 Average AfriCOMET Score (A to B): {results_summary['translation']['a_to_b']['average_score']:.4f}\")\n",
        "        print(f\"  FLORES-200 Average AfriCOMET Score (B to A): {results_summary['translation']['b_to_a']['average_score']:.4f}\")\n",
        "\n",
        "    if 'generation' in results_summary:\n",
        "        print(\"\\nGeneration Results:\")\n",
        "        print(f\"  FLORES-200 Average Perplexity: {results_summary['generation']['average_perplexity']:.4f}\")\n",
        "\n",
        "    if 'zero_shot' in results_summary:\n",
        "        print(\"\\nZero-shot Results:\")\n",
        "        print(f\"  Accuracy: {results_summary['zero_shot']['accuracy']:.4f}\")\n",
        "\n",
        "    if 'code_switch' in results_summary:\n",
        "        print(\"\\nCode-switch Results:\")\n",
        "        print(f\"  Accuracy: {results_summary['code_switch']['accuracy']:.4f}\")\n",
        "\n",
        "    print(\"\\nBest Hyperparameters:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    print(\"\\n======================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "f1oA1qQbcYqn"
      },
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config = load_config('/content/MScAI-BigData-FYP/py/config.yaml')\n",
        "# Set the device dynamically based on availability\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'  # Update device setting\n",
        "auth_token = config.get(\"auth_token\")\n",
        "cache_dir = os.path.abspath(config['cache']['dir'])\n",
        "logger = setup_logging(config)\n",
        "set_seed(config['seed'])\n",
        "device = get_device(config['device'])\n",
        "logger.info(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EGXArBFicYqn"
      },
      "outputs": [],
      "source": [
        "# Ensure the cache directory exists\n",
        "os.makedirs(cache_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NXW9aIlrcYqn"
      },
      "outputs": [],
      "source": [
        "# Load and prepare datasets\n",
        "data_loader = DatasetLoader(config)\n",
        "stratified_datasets = data_loader.prepare_stratified_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEOgVwigcYqn",
        "outputId": "65657889-5bb9-4317-a049-dd897cdaffe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset size: 1050\n",
            "eval dataset size: 225\n",
            "benchmark dataset size: 500\n"
          ]
        }
      ],
      "source": [
        "for split, dataset in stratified_datasets.items():\n",
        "    print(f\"{split} dataset size: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXFq0cOOcYqn",
        "outputId": "b8d4b769-2afb-4b9c-ac35-c8f6530651ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verifying train dataset:\n",
            "Dataset type: <class 'pandas.core.frame.DataFrame'>\n",
            "Dataset shape: (1050, 4)\n",
            "Columns: ['text', 'label', 'language', 'split']\n",
            "No missing values found.\n",
            "Number of unique NER tags: 9\n",
            "Unique NER tags: {'B-LOC', 'I-LOC', 'B-DATE', 'B-PER', 'I-ORG', 'I-PER', 'I-DATE', 'O', 'B-ORG'}\n",
            "Text data check passed.\n",
            "Languages in train dataset: ['kin' 'lug' 'swh']\n",
            "Splits in train dataset: ['masakhane']\n",
            "Sample data:\n",
            "                                                   text  \\\n",
            "695   Perezida Paul Kagame aherutse kubikomozaho ati...   \n",
            "1022  Kati obulwadde obulala obulabika ngobunafu bwe...   \n",
            "51    Matayarisho ya uchaguzi Akiwa anajitayarisha k...   \n",
            "493     Hari icyo byafasha amakipe yitegura amarushanwa   \n",
            "390   Sendashonga wari umunyamuryango wa FPR Inkotan...   \n",
            "\n",
            "                                                  label language      split  \n",
            "695   O B-PER I-PER O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
            "1022                          O O O O O O O O O O O O O      lug  masakhane  \n",
            "51    O O O O O O O O O B-DATE O B-PER O O O O O O O...      swh  masakhane  \n",
            "493                                         O O O O O O      kin  masakhane  \n",
            "390   B-PER O O O B-ORG I-ORG O O O O O B-DATE O O O...      kin  masakhane  \n",
            "\n",
            "Verifying eval dataset:\n",
            "Dataset type: <class 'pandas.core.frame.DataFrame'>\n",
            "Dataset shape: (225, 4)\n",
            "Columns: ['text', 'label', 'language', 'split']\n",
            "No missing values found.\n",
            "Number of unique NER tags: 9\n",
            "Unique NER tags: {'B-LOC', 'I-LOC', 'B-DATE', 'I-ORG', 'B-PER', 'I-DATE', 'I-PER', 'O', 'B-ORG'}\n",
            "Text data check passed.\n",
            "Languages in eval dataset: ['lug' 'kin' 'swh']\n",
            "Splits in eval dataset: ['masakhane']\n",
            "Sample data:\n",
            "                                                  text  \\\n",
            "175  Abawagizi be baasigadde bamutenda nti musajja ...   \n",
            "107  Minisiteri y’Imari n’Igenamigambi yakoze gahun...   \n",
            "6    Utafiti huu pia umezingatia mauaji mikononi mw...   \n",
            "151  Ono yasabye poliisi okukozesa kamera ezaateeke...   \n",
            "94   3 zifite agaciro k’amafaranga asaga miliyari 2...   \n",
            "\n",
            "                                                 label language      split  \n",
            "175                                O O O O O O O O O O      lug  masakhane  \n",
            "107  B-ORG I-ORG I-ORG O O O O O O B-LOC O O O O O ...      kin  masakhane  \n",
            "6    O O O O O O O O O O O O O O O O O O O O O O O ...      swh  masakhane  \n",
            "151                        O O O O O O O O O O O O O O      lug  masakhane  \n",
            "94   O O O O O O O O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
            "\n",
            "Verifying benchmark dataset:\n",
            "Dataset type: <class 'pandas.core.frame.DataFrame'>\n",
            "Dataset shape: (500, 4)\n",
            "Columns: ['text', 'label', 'language', 'split']\n",
            "Warning: Missing values found in benchmark dataset:\n",
            "text: 63.20% missing\n",
            "label: 63.20% missing\n",
            "language: 63.20% missing\n",
            "Consider handling these missing values before proceeding.\n",
            "Number of unique NER tags: 9\n",
            "Unique NER tags: {'B-LOC', 'I-LOC', 'B-DATE', 'I-ORG', 'B-PER', 'I-PER', 'I-DATE', 'O', 'B-ORG'}\n",
            "Text data check passed.\n",
            "Languages in benchmark dataset: [nan 'lug' 'kin' 'swh' 'eng']\n",
            "Splits in benchmark dataset: ['flores_200_dev' 'flores_200_devtest' 'masakhane' 'zero_shot']\n",
            "Sample data:\n",
            "                                                  text  \\\n",
            "397                                                NaN   \n",
            "549                                                NaN   \n",
            "525                                                NaN   \n",
            "167  Ono kati yeegasse ku Tonny Mawejje ne Hassa Mo...   \n",
            "509                                                NaN   \n",
            "\n",
            "                                                 label language  \\\n",
            "397                                                NaN      NaN   \n",
            "549                                                NaN      NaN   \n",
            "525                                                NaN      NaN   \n",
            "167  O O O O B-PER I-PER O B-PER I-PER B-ORG I-ORG ...      lug   \n",
            "509                                                NaN      NaN   \n",
            "\n",
            "                  split  \n",
            "397      flores_200_dev  \n",
            "549  flores_200_devtest  \n",
            "525  flores_200_devtest  \n",
            "167           masakhane  \n",
            "509  flores_200_devtest  \n"
          ]
        }
      ],
      "source": [
        "# Verify data integrity\n",
        "if not data_loader.verify_data_integrity(stratified_datasets):\n",
        "    logger.error(\"Data integrity check failed. Please review the datasets.\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSDInGtmcYqn",
        "outputId": "7be71411-85d4-4a07-c0f8-6d295b18b724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TRAIN Dataset ---\n",
            "Shape: (1050, 4)\n",
            "\n",
            "Column Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1050 entries, 695 to 912\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   text      1050 non-null   object\n",
            " 1   label     1050 non-null   object\n",
            " 2   language  1050 non-null   object\n",
            " 3   split     1050 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 41.0+ KB\n",
            "None\n",
            "\n",
            "Sample Data:\n",
            "                                                   text  \\\n",
            "695   Perezida Paul Kagame aherutse kubikomozaho ati...   \n",
            "1022  Kati obulwadde obulala obulabika ngobunafu bwe...   \n",
            "51    Matayarisho ya uchaguzi Akiwa anajitayarisha k...   \n",
            "493     Hari icyo byafasha amakipe yitegura amarushanwa   \n",
            "390   Sendashonga wari umunyamuryango wa FPR Inkotan...   \n",
            "\n",
            "                                                  label language      split  \n",
            "695   O B-PER I-PER O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
            "1022                          O O O O O O O O O O O O O      lug  masakhane  \n",
            "51    O O O O O O O O O B-DATE O B-PER O O O O O O O...      swh  masakhane  \n",
            "493                                         O O O O O O      kin  masakhane  \n",
            "390   B-PER O O O B-ORG I-ORG O O O O O B-DATE O O O...      kin  masakhane  \n",
            "\n",
            "Dataset Statistics:\n",
            "num_samples: 1050\n",
            "avg_text_length: 156.0647619047619\n",
            "num_classes: 802\n",
            "class_distribution: {'O O O O O O O O O O O O O': 16, 'O O O O O O O O O O': 16, 'O O O O O O O O O O O': 15, 'O O O O O O O O O O O O O O O': 14, 'O O O O O O O O O O O O O O O O O O O O': 14, 'O O O O O O O O O O O O O O O O O O O O O': 13, 'O O O O O O O O O O O O O O O O O O': 13, 'O O O O O O': 11, 'O O O O O O O O O O O O': 10, 'O O O O O O O O O': 10, 'O O O O O O O O O O O O O O': 10, 'O O O O O O O O O O O O O O O O O O O O O O O': 9, 'O O O O O O O O O O O O O O O O O O O O O O O O': 9, 'O O O O O': 9, 'O O O O O O O': 9, 'O O O O O O O O O O O O O O O O O O O O O O O O O': 8, 'O O O O O O O O O O O O O O O O': 7, 'O O O O O O O O O O O O O O O O O': 6, 'O O O O': 6, 'O O O O O O O O': 5, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 5, 'O O O O O O O O O O O O O O O O O O O': 5, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O': 5, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 5, 'O O O': 5, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 4, 'O O': 4, 'O O O O O O O O O O O O O O O O O O O O O O O O O O': 4, 'O O O O O B-LOC I-LOC I-LOC O': 3, 'O O O B-PER I-PER': 3, 'O O O O O O O O O O O O O O O O O O O O O O': 3, 'B-PER O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 3, 'B-ORG O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O B-LOC I-LOC O O O O': 2, 'O O B-ORG I-ORG O O O O O O O B-PER I-PER O O O O O O O O O O O': 2, 'B-LOC O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O B-ORG I-ORG O O': 2, 'O O O O O O O O B-DATE O': 2, 'O O O O B-LOC O B-PER I-PER O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O B-PER O O O O O O': 2, 'B-PER O B-PER': 2, 'B-PER O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'B-PER O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O B-DATE O': 2, 'O O O B-DATE O O O O': 2, 'O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-ORG O O O O O O O B-LOC O O O O O O O O B-ORG I-ORG O O O O O B-LOC O': 1, 'O O O B-DATE O O O O O B-LOC O O O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O O O B-ORG I-ORG O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O B-LOC O': 1, 'O O O O O B-LOC O O O O': 1, 'B-PER O B-PER O O O O O O O O O B-LOC O O O B-LOC O': 1, 'O O O O B-LOC B-PER I-PER O O O O B-PER I-PER O O O O O O O O O B-DATE O': 1, 'O O O B-ORG I-ORG O O O O O O O O B-PER I-PER O O B-LOC O B-PER I-PER O B-ORG I-ORG O B-ORG I-ORG O O O O B-PER I-PER O': 1, 'O O O O O B-PER O O O O O O O B-DATE I-DATE O O O B-PER O O O O O O O B-DATE O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O B-PER I-PER O O O O O O O B-DATE I-DATE O O O O O O O B-DATE O O O O O O O B-PER I-PER O O O O O O O O O': 1, 'O O O O O O B-LOC O O': 1, 'O O O O O O O O O B-ORG I-ORG O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O B-ORG I-ORG I-ORG O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O': 1, 'O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-ORG O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-LOC O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O': 1, 'B-ORG O O O O O O O O O O O O O O B-ORG I-ORG O O O O O': 1, 'O O B-DATE I-DATE O O B-LOC I-LOC I-LOC O': 1, 'O O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'B-ORG I-ORG O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O': 1, 'B-PER O O O B-ORG O O O O O O O O': 1, 'O O O B-PER O O O O O O O B-PER O O O O O O O O O': 1, 'O O O O O O B-LOC B-PER I-PER O O O O O O O O O': 1, 'B-PER I-PER I-PER O O O O O B-LOC I-LOC O O O O O O O B-ORG O O O B-LOC O O O B-LOC O O B-LOC O O O O B-LOC I-LOC O O O B-LOC O O O B-LOC O O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG I-ORG O': 1, 'B-DATE I-DATE I-DATE B-PER O O O O O O O B-ORG I-ORG I-ORG I-ORG O': 1, 'O B-DATE O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O O B-ORG O O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'B-PER O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-LOC I-LOC O O O O O B-LOC O O O O O O B-PER O O O B-PER O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O B-DATE I-DATE O O O O B-DATE I-DATE O O O O O O O O': 1, 'B-PER O O O B-DATE B-ORG O O O O O O O O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O B-PER O O O': 1, 'B-ORG O B-ORG I-ORG O O B-DATE O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-LOC O O O B-LOC I-LOC O O O B-LOC O O O O B-LOC O O O O B-LOC O O O O O O O O B-LOC O O B-LOC O O O O B-PER I-PER O O O B-DATE O O B-PER I-PER I-PER O O B-LOC O O O O': 1, 'O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O B-ORG O B-LOC I-LOC I-LOC I-LOC I-LOC O B-PER I-PER O O O O B-PER I-PER O O O O O O B-LOC I-LOC O O B-LOC O O O O O B-LOC O': 1, 'O O B-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O O O O O O O O B-LOC O O O': 1, 'O O B-DATE O O O O B-DATE O O O O O O O O': 1, 'O O O O B-PER I-PER I-PER I-PER I-PER I-PER O': 1, 'O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG I-ORG I-ORG O O O O O B-LOC O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O B-ORG O O B-LOC O O B-LOC O': 1, 'B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O B-PER I-PER O O O O B-LOC O B-LOC I-LOC I-LOC O B-LOC O': 1, 'O O O O O O O O O O O O B-LOC O O B-LOC O O O O O O O O O O O O': 1, 'O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-LOC O': 1, 'O O O O O B-LOC O B-PER I-PER O O O O O O O B-DATE O O O O O O O O B-ORG O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O O': 1, 'O B-PER O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O': 1, 'O B-LOC O B-PER I-PER O O O O O O O B-LOC I-LOC I-LOC O': 1, 'B-PER I-PER O O O O O O O B-ORG O O O O O O O O O O O O O O O O B-PER O O O O O B-ORG O O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O B-LOC O O B-ORG O O O O B-LOC O B-LOC O B-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE I-DATE O': 1, 'O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O B-DATE O O O O O O O O O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O O B-ORG O O B-LOC I-LOC O': 1, 'O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O B-DATE O O O O O O': 1, 'O O B-ORG O B-PER I-PER O O O O B-ORG O O O O O B-DATE I-DATE O': 1, 'O O O O O O O B-LOC O O B-LOC O O O B-LOC O B-LOC O O O O O': 1, 'O B-DATE O O O O O O O O': 1, 'O O O B-PER O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O B-LOC O O B-ORG I-ORG I-ORG O O O O': 1, 'B-PER O O O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O': 1, 'O O B-PER O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O B-PER I-PER O B-LOC O': 1, 'O O B-PER O O O B-DATE O B-DATE O B-DATE O O O O O O O O O O B-LOC O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O O': 1, 'O O O O O O O O O O B-LOC I-LOC O O O O O O B-LOC O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O': 1, 'B-DATE I-DATE I-DATE B-PER O O O O O B-LOC O': 1, 'O O O O B-LOC O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O B-LOC I-LOC I-LOC O': 1, 'O O O O O O B-PER O O O O O O O B-LOC O B-DATE I-DATE I-DATE I-DATE O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O B-LOC O B-LOC O': 1, 'B-LOC I-LOC O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-LOC O O O O B-ORG O O B-ORG O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O B-DATE O O O O O O O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER O O O O O': 1, 'O O B-PER O O O O O O O O O B-LOC O O O O B-DATE O O O O O O O O': 1, 'O B-LOC O O O O O B-ORG': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O O O B-PER I-PER O B-LOC I-LOC I-LOC': 1, 'O O B-DATE I-DATE O O O B-PER O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-LOC O O O O O O B-LOC O': 1, 'B-ORG O O O B-ORG O O O B-ORG O O B-ORG O O B-ORG O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O B-PER I-PER I-PER O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O': 1, 'O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O O B-LOC B-DATE O O O O O O O O B-ORG O O O O O O O O O B-LOC O O B-DATE I-DATE O': 1, 'O B-DATE O O B-LOC O O O O O O': 1, 'O O O B-ORG O O O O O O B-ORG I-ORG O B-ORG I-ORG O B-LOC O O O B-PER I-PER O B-ORG O O O O O B-ORG I-ORG O O B-PER I-PER O O O O O O O O O B-PER I-PER O O O O B-PER I-PER O B-PER I-PER O O O O O O O O O B-PER I-PER O O O O B-PER I-PER O O O B-PER I-PER O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'B-PER O O O B-PER I-PER O O B-PER I-PER O O O O O O B-PER I-PER O O B-PER I-PER O': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-ORG O O O O': 1, 'B-LOC I-LOC': 1, 'O O O O B-LOC O O O O O O O O O O O O O O O O O B-DATE O B-DATE I-DATE O': 1, 'O O B-PER O O B-DATE O O O O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O': 1, 'O O O O B-ORG O O O O O O O O O O O O O': 1, 'O O O B-PER I-PER O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O O O O O O O O O O B-DATE O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O B-DATE O B-DATE I-DATE I-DATE I-DATE O': 1, 'O O O B-DATE O O O O B-DATE I-DATE I-DATE I-DATE O O O O': 1, 'O O B-ORG O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O B-ORG O O B-LOC O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG O O O O B-ORG I-ORG O O B-ORG I-ORG O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-DATE I-DATE O B-ORG O O O O O O O O O B-DATE O B-LOC O B-ORG O O O O O': 1, 'O O B-ORG O O O O O O O B-DATE O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O B-LOC O O O O O O O B-LOC O': 1, 'O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-DATE I-DATE I-DATE I-DATE O O B-LOC O': 1, 'O O O O B-LOC O B-PER I-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O B-PER I-PER O B-DATE O O O O O O O O O': 1, 'B-PER O O O B-ORG O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O': 1, 'O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O': 1, 'O O O O O O O O O O O B-ORG O O O O O O': 1, 'O O O O O O O B-ORG O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG O O O O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O O O O O O O B-LOC O': 1, 'O O O O B-LOC O O B-PER I-PER O O O O O O O O O O B-LOC O O O O O O O B-DATE I-DATE O O O O': 1, 'O O O O O O O O O B-DATE O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O O B-DATE I-DATE I-DATE I-DATE O O O O B-PER O O O': 1, 'O B-DATE I-DATE O O O O B-LOC I-LOC O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O B-LOC B-DATE O O O O O O O O O O O O O': 1, 'B-PER I-PER O B-PER I-PER O O B-ORG I-ORG I-ORG O B-ORG I-ORG': 1, 'O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O O O O O O O B-DATE O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O B-PER O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O B-DATE I-DATE O': 1, 'O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O B-DATE I-DATE O O O O': 1, 'O O B-PER I-PER O O O B-ORG B-DATE I-DATE I-DATE O O O O O O O O O B-LOC I-LOC I-LOC O': 1, 'O O O B-ORG O O O B-ORG O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O': 1, 'O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O B-PER I-PER O O B-LOC O B-PER O B-PER I-PER O B-PER O O B-LOC O B-PER O B-PER O O B-LOC O': 1, 'O O B-PER O O O B-PER O B-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-PER I-PER O': 1, 'O O O O O O O O O B-ORG I-ORG B-PER I-PER O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O': 1, 'O O O O B-PER O O O O B-LOC': 1, 'B-ORG I-ORG I-ORG O B-LOC O B-PER O O O O O O O O O O O O O O B-LOC I-LOC O O B-PER I-PER O O O O O B-PER I-PER O O B-PER O O O O O O O O O O': 1, 'O O B-DATE I-DATE I-DATE O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-ORG O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O B-DATE O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O': 1, 'B-PER O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG O B-ORG O B-ORG O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O': 1, 'O O B-PER I-PER O B-ORG O B-PER I-PER O O O O O O': 1, 'O O O O O B-LOC O B-PER O O O O O O O B-LOC O B-LOC O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O B-ORG O O O O O O B-LOC O B-ORG I-ORG O O O O O O O O O B-LOC O B-LOC I-LOC O': 1, 'O O O O O B-LOC I-LOC O O O': 1, 'O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O O O B-LOC O O O O O O O O O O': 1, 'B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O B-PER I-PER O O B-PER I-PER O O B-PER O O O B-PER O O B-PER O O O O O O O O O O O O': 1, 'O O O O O B-LOC I-LOC O O O O B-PER I-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O B-DATE I-DATE O O O O O O O B-LOC B-PER I-PER I-PER O O O O O O': 1, 'O O O B-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE I-DATE O O O O B-PER O O O B-ORG O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE O': 1, 'B-ORG O O O O O O O': 1, 'O O O O O O O O B-LOC': 1, 'O O O O O O B-PER O B-PER O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-LOC O O O O O O O O B-LOC I-LOC O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O': 1, 'O O O B-DATE O O O O O O O B-LOC O B-LOC': 1, 'B-PER O O O O O O O B-LOC I-LOC O B-LOC O O O O B-DATE O O O B-PER O O': 1, 'O O O B-LOC B-PER I-PER O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O B-DATE I-DATE O': 1, 'B-PER O B-PER O O O O O B-LOC I-LOC B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O B-PER O O O O B-DATE I-DATE O O O O': 1, 'B-PER I-PER O O O O O B-LOC B-PER O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O B-PER I-PER O': 1, 'O O O O B-LOC O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-ORG O O O O O O O': 1, 'O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'B-PER I-PER O O O B-PER O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O O O O O O O B-LOC O O O O O O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O B-PER O': 1, 'B-PER O O O O O B-LOC O O O O O O B-ORG O': 1, 'O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O': 1, 'O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O B-PER I-PER O': 1, 'O B-PER O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O B-LOC O O O B-PER O': 1, 'O O O O O O O O B-LOC I-LOC O B-LOC O O O O O O O O O O': 1, 'B-ORG I-ORG O B-ORG O O O O O O O O O O': 1, 'B-PER O O O B-LOC O B-PER I-PER O O O O B-LOC O O O O O O B-LOC I-LOC O B-PER I-PER O B-PER I-PER O B-LOC I-LOC O O O B-LOC O B-PER O O O O O B-LOC O O B-LOC O O O O O O O': 1, 'O O B-PER O O O O O O O O O O B-PER O O O O O O O O O': 1, 'B-LOC O O B-PER O O O O O O O O O O B-LOC O O O O O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-DATE I-DATE O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O': 1, 'O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O': 1, 'O B-DATE I-DATE O B-LOC I-LOC O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O B-LOC B-PER I-PER O O O O B-LOC O': 1, 'O O O O O O O B-LOC O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O B-ORG O O O O O B-PER O O O O O O O': 1, 'O O O O B-DATE O O O O O O O O O': 1, 'B-PER O O O O O O O O O B-DATE O O O O O O O O': 1, 'B-PER O B-PER O O B-ORG I-ORG I-ORG O O O O O O O': 1, 'B-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O B-LOC O O B-DATE I-DATE O O O O': 1, 'B-PER O O O O B-ORG O O O O O O O B-ORG O O B-PER I-PER O B-PER I-PER O O O O O O O O B-ORG O O': 1, 'O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-ORG O O O': 1, 'O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O B-LOC O O B-LOC O O O O O O O': 1, 'O O O O O O O O B-PER O B-PER O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O B-ORG I-ORG I-ORG O': 1, 'O O O O O O O O O O O O O B-PER O O O O O O O O O O B-PER O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O O O O O O O B-DATE O B-LOC I-LOC O O O O O O O O O': 1, 'O O B-PER I-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O B-DATE O': 1, 'B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O': 1, 'O O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG I-ORG O': 1, 'B-PER O O O B-PER I-PER O B-PER O O O B-PER O': 1, 'B-PER O O O O O O O O O O O O O O O O B-LOC O O': 1, 'O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O B-PER O': 1, 'B-PER I-PER O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O': 1, 'O O O O B-LOC I-LOC O B-PER I-PER O O O B-PER O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O B-PER O O': 1, 'O O O O B-PER O O O O B-LOC O O B-LOC O O O B-LOC O O B-LOC O O': 1, 'O O O O B-DATE I-DATE O O B-ORG I-ORG I-ORG O O O': 1, 'O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-DATE B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O B-LOC O O O O O O': 1, 'B-PER I-PER O O O B-PER I-PER O O B-LOC I-LOC O O O O B-LOC O O O O O O B-DATE O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O O O O O O O O B-PER I-PER O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O B-LOC O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O B-PER O O O B-PER O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O': 1, 'B-PER I-PER O O O O B-ORG I-ORG I-ORG O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O B-ORG I-ORG O O O O O O O O': 1, 'O O B-DATE I-DATE I-DATE I-DATE O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O O O O B-ORG O B-PER I-PER O': 1, 'O O O O O O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O': 1, 'O O B-PER I-PER O O O O O O O O O O O O': 1, 'B-PER I-PER O O O B-LOC O O O O O O O O O B-PER O O O O O B-DATE O O O O O O': 1, 'B-ORG I-ORG I-ORG I-ORG O B-ORG O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O B-PER O B-PER I-PER O O': 1, 'O O B-ORG O O O O B-LOC O O B-PER O O O O O B-ORG O B-DATE O': 1, 'O O O O O B-PER O O O O O O B-LOC I-LOC I-LOC I-LOC O O O O O O O B-PER O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O': 1, 'B-LOC I-LOC O O O O O O O B-DATE I-DATE I-DATE O O O O': 1, 'O O B-ORG O O O O O O O O': 1, 'O O O O O O O O O O B-PER O O O O O O O B-PER O O O O O O O O O O B-PER O O O O B-PER O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG O O O O B-DATE I-DATE I-DATE I-DATE': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O O O O O O B-LOC O O B-DATE O O O O O O O': 1, 'B-PER O B-PER I-PER O O O O O': 1, 'O O O O O B-DATE O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O': 1, 'O O O O O B-LOC O B-DATE O O O O B-LOC O': 1, 'O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O B-ORG I-ORG I-ORG O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O B-ORG O O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O O B-PER O O O O O O O O O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC I-LOC O B-LOC O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O': 1, 'B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC O B-LOC O B-LOC O B-LOC O B-DATE O O O O O': 1, 'B-ORG I-ORG O O B-PER I-PER O O O O O O O O O O O O B-LOC O O O O O O O B-ORG I-ORG O O O B-DATE I-DATE I-DATE O O O O B-LOC O': 1, 'O O O O O O O O O O B-PER O': 1, 'O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'B-ORG O O O O O O O O B-ORG O O O B-PER I-PER I-PER O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O': 1, 'B-PER O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG I-ORG O O O O O O O O O B-ORG O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-ORG O B-DATE O O O O O O O O O O B-LOC O': 1, 'B-ORG O O O O O O O B-ORG I-ORG O B-LOC O': 1, 'B-PER O O O O O O O O O O O O O O O O B-DATE O': 1, 'O B-PER I-PER O O O O O O O O O O B-LOC O O O O': 1, 'O O B-PER O O O B-PER O O O O O O O B-DATE I-DATE O': 1, 'B-PER O O O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O': 1, 'B-ORG I-ORG I-ORG O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE O O O O B-DATE I-DATE O': 1, 'O O B-ORG I-ORG I-ORG O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O B-LOC O O O O O O O O O O O O B-DATE I-DATE O B-DATE I-DATE O': 1, 'O O O O B-ORG O B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O': 1, 'O O O O O O B-DATE I-DATE O O O B-ORG O O O O O O O O O B-LOC O': 1, 'O O O O O B-PER I-PER O O O O O O B-LOC I-LOC I-LOC O O O O B-DATE O O O O O O O B-DATE I-DATE O O O O O B-DATE I-DATE I-DATE O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER O O O O B-PER O O O O B-LOC O O O O O O O': 1, 'B-PER O O O O B-ORG I-ORG O B-ORG O B-ORG O O O B-ORG O O': 1, 'O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-LOC I-LOC I-LOC O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O B-PER O O O O O O O': 1, 'O O O O B-DATE I-DATE O B-PER O O O O O O O O O': 1, 'O O O B-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O O O O O O O O O O O': 1, 'O O B-PER I-PER I-PER O O O O O O B-PER O O B-LOC O B-LOC O O O O O O O O O B-PER O O O O O B-PER I-PER I-PER I-PER I-PER O': 1, 'B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O': 1, 'O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER I-PER O B-ORG O O O O B-PER I-PER O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O B-PER O O B-PER I-PER O B-PER I-PER O O': 1, 'O O O O O O O O O O O B-ORG O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O B-LOC O O O O O O B-PER I-PER O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC B-DATE I-DATE O O B-ORG O O B-ORG O': 1, 'O B-PER O O O': 1, 'B-PER I-PER O B-ORG I-ORG I-ORG O O B-PER O O O O O O O O O O': 1, 'B-ORG O O O O O O B-ORG I-ORG O O O O O O O B-PER O O O O O O O O O O O B-PER O O O O O O O O': 1, 'O O O O B-LOC O O O B-PER I-PER O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O B-PER O O O O O B-PER O O B-PER O O O O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O O B-PER O O O O O O O O O O O O O O O O O': 1, 'B-PER O B-PER O O O B-DATE O': 1, 'B-PER I-PER O O O O O O B-PER I-PER O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O B-LOC O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-PER I-PER O O B-PER I-PER I-PER I-PER I-PER O O O O O O O O O B-LOC O O B-ORG I-ORG I-ORG O O O O O B-LOC O B-ORG O O O O O O O O O O B-LOC O O O B-LOC O O O O': 1, 'O B-PER I-PER I-PER I-PER O B-ORG I-ORG O O O O O B-LOC O': 1, 'O O O O O B-DATE O O O O O O O O O O B-DATE O O O O O B-LOC O': 1, 'O O O O B-PER I-PER O B-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O B-LOC O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O B-DATE I-DATE O O O': 1, 'B-ORG I-ORG O O B-ORG I-ORG O O O O O O B-DATE I-DATE O O O O O B-LOC O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-DATE O O O O O O O O O': 1, 'B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-LOC O O O O O O O': 1, 'O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE I-DATE O O B-ORG I-ORG I-ORG O O O O O O O O': 1, 'O O O O O O O O O O O B-LOC O O O': 1, 'O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O B-DATE O': 1, 'O O O B-PER O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O': 1, 'O O O O O O O O O O O O B-PER O': 1, 'B-PER I-PER O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O B-PER I-PER O O O O B-ORG O O B-ORG O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O B-LOC O O O O O O O O O B-LOC O O O': 1, 'O O O O B-DATE I-DATE I-DATE O O O O O': 1, 'O O O O O O O O B-LOC O O O B-LOC O O O O O O': 1, 'O O O O O O O O O B-LOC I-LOC O O O O B-ORG O O O O B-ORG O O O O O O B-LOC O O O': 1, 'O O O O O O B-ORG O': 1, 'O O O O B-PER I-PER I-PER I-PER O': 1, 'O O O O O O O O O O O O O O O O O O B-ORG O': 1, 'B-PER O O O O O O': 1, 'O O O O O O O O O B-DATE': 1, 'O O B-PER O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O B-LOC O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'O O O B-LOC O O O O O O O B-DATE O O O O O B-LOC I-LOC I-LOC O': 1, 'O O O O O O B-PER I-PER O B-LOC I-LOC O B-PER O O': 1, 'B-PER O O O O O O O O O O O O O O O B-PER O B-ORG I-ORG I-ORG O': 1, 'O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O': 1, 'O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O B-DATE O O B-ORG I-ORG I-ORG O B-ORG O O O O B-LOC O O O O O O O O O O O O B-LOC O': 1, 'O O O B-PER I-PER O O O O O O O B-LOC O O O O O O O O B-DATE O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O': 1, 'O O B-DATE I-DATE I-DATE O': 1, 'B-PER O O O O O': 1, 'B-ORG O B-PER O B-LOC I-LOC O O O O O O O O B-LOC O O O O O O': 1, 'O B-PER O B-LOC O O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O B-LOC O B-DATE O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O O B-ORG I-ORG I-ORG O O O O O O O O O O B-LOC I-LOC O O O B-PER O': 1, 'O O O O O B-LOC O B-LOC O O O B-PER I-PER O O O O O O O B-DATE O O O B-LOC O O O O O': 1, 'B-ORG O O O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O B-PER O B-DATE O O O O O O O O': 1, 'O O O O B-LOC O O O O B-PER I-PER O O O O O B-LOC O O B-LOC O B-DATE O': 1, 'O B-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER I-PER I-PER I-PER O O O O O O O O': 1, 'O O O O O O B-ORG O B-PER I-PER O O O O O B-LOC O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O O B-LOC B-PER I-PER I-PER O O O O O O O B-LOC I-LOC I-LOC O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O': 1, 'O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC B-PER I-PER O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O O O O O B-PER I-PER O O O O O O': 1, 'B-PER O O O O O O O O O O O O B-ORG O O B-LOC I-LOC O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-ORG I-ORG I-ORG O B-PER I-PER I-PER O O O O O O O B-PER O O B-DATE O O O B-LOC O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O B-ORG I-ORG I-ORG O O O O B-LOC O O O O': 1, 'O O O O B-LOC O B-LOC B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O B-LOC O O O B-PER I-PER O B-PER I-PER I-PER O O O O O B-LOC I-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'B-PER O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O B-LOC O O O O B-DATE O O O O B-DATE O': 1, 'O O B-ORG O O B-LOC O O O O B-LOC O O O O O O O O O B-PER O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC B-DATE O O O O O O': 1, 'O B-DATE I-DATE O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O O B-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O': 1, 'O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O B-LOC O B-LOC O': 1, 'B-DATE I-DATE O O O O O O O O O O O O O O O O B-ORG O O O O B-LOC O O O O O O O B-LOC O O O O O O': 1, 'O O O B-LOC O B-ORG I-ORG I-ORG O B-ORG O O O O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O O O': 1, 'B-PER O O O O O B-LOC O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O B-DATE O O O B-DATE O O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O B-ORG O O O O B-DATE O O O O O O O O O O O O O O O O': 1, 'O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O B-PER O B-PER O O O O O': 1, 'O O O B-PER O B-LOC O O O O O O O O O': 1, 'B-PER I-PER O O O O O B-LOC O O O O O B-PER O O O O B-DATE I-DATE O O B-DATE I-DATE O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O B-PER O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'B-PER I-PER O O O B-PER O O O O B-ORG O O O O O O O O O O O O': 1, 'B-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O B-ORG O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER I-PER O O O O O O O O O O O O O': 1, 'O O O B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O B-DATE O O B-LOC O O O B-LOC O O O O': 1, 'O O O O O B-LOC O O O O O O O O O O O': 1, 'O O O O O O O B-PER O O': 1, 'B-DATE I-DATE O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O B-LOC I-LOC I-LOC O O O B-LOC O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O B-PER O B-LOC O O O O B-LOC O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O O B-DATE I-DATE I-DATE O B-PER O O': 1, 'O O B-DATE O O B-PER O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O B-LOC B-DATE O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER O O O O O O O O O O O O O O O O': 1, 'O O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O O O B-PER O O O O O O O O O O O O': 1, 'O O B-PER O O O O B-LOC O O O B-LOC O O B-DATE O': 1, 'O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O B-PER O O O O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER I-PER O O O O B-DATE O O O O': 1, 'O B-DATE O O O O B-PER I-PER O O O O O O O O B-DATE O O O O O B-LOC I-LOC O': 1, 'B-PER I-PER O O O B-LOC O O O O O O O': 1, 'O O O O O B-PER I-PER': 1, 'O O B-PER O O O O O O': 1, 'O O O B-LOC O O O B-PER O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O O B-LOC O B-DATE O O O O O O O': 1, 'O O B-DATE O O O': 1, 'B-PER O O O O B-PER I-PER O O B-LOC O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O O O B-PER O B-PER I-PER O B-PER O B-PER O B-LOC O B-PER O B-LOC O O': 1, 'B-PER O O O O O O B-PER I-PER O': 1, 'O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER O B-PER I-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O O O O O O O O O O O O': 1, 'O O O O O B-ORG I-ORG O O O O O O O O O O O': 1, 'O O O O O O B-LOC B-ORG O B-DATE O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O B-PER I-PER I-PER O O O O B-LOC O O O O O O O': 1, 'O B-ORG I-ORG I-ORG O B-PER I-PER O O B-PER O O O O O O O O O O O O O': 1, 'O O O O B-LOC I-LOC B-PER I-PER I-PER O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O B-LOC O O O O O O O': 1, 'O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'O O O O B-LOC O B-LOC I-LOC O B-LOC I-LOC O B-LOC I-LOC O O O O O O O O O O': 1, 'O B-ORG I-ORG I-ORG O B-PER I-PER I-PER O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O': 1, 'O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O B-PER O O O O B-PER O O O O O B-ORG O O O O O O O B-PER O O O O O O O O O O O B-ORG I-ORG I-ORG B-PER I-PER O O B-DATE O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O B-PER O': 1, 'O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE O B-PER O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O': 1, 'B-PER I-PER B-LOC I-LOC O B-PER O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-PER O O O': 1, 'B-PER O O O B-PER O O O O O O O O O O O O B-PER O B-LOC I-LOC O B-LOC O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O B-ORG O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O B-PER O O O O O O O O O O O O B-PER O O O O O O O B-DATE O O O O O O O B-PER O O O O O O O O': 1, 'B-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O B-LOC O O O B-DATE I-DATE O O O O O B-LOC O O O B-ORG O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER I-PER O O B-PER I-PER I-PER O': 1, 'O O O O O B-LOC I-LOC': 1, 'B-PER I-PER O O O O B-PER O O O O O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O': 1, 'O O O O O O O O O B-DATE O B-PER O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-ORG I-ORG O O O O O B-DATE O O O O O B-LOC O': 1, 'O O O O O B-LOC O B-DATE I-DATE O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'B-PER O B-PER I-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-DATE O B-DATE O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-ORG O O O O O B-DATE O': 1, 'O O O B-LOC O B-LOC O O O O O O O O O O O O B-LOC O B-LOC O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O B-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O B-ORG O O O O O O O O O': 1, 'B-PER I-PER O O O O B-LOC I-LOC I-LOC O': 1, 'O O O B-PER O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O B-LOC I-LOC O': 1, 'O O O O O B-PER O O O B-PER O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-LOC O O O O O O O O': 1, 'O B-DATE O O B-ORG O O O O O O O O O O': 1, 'O O O B-LOC O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O B-DATE I-DATE O O O B-PER O O O O O': 1, 'O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O': 1, 'O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O B-DATE O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O': 1, 'O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O B-LOC O B-LOC O B-DATE O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O': 1, 'O O O O B-LOC O O O O B-PER I-PER O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O': 1, 'O O O B-PER O O O O O B-LOC O O O O O O O O B-PER I-PER O O O O O O B-LOC O': 1, 'B-DATE I-DATE O O O B-LOC O O O O O O O B-PER O O O O O O O O O O': 1, 'B-PER I-PER O O O B-LOC O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG I-ORG I-ORG O O O O B-LOC O O O O O O O O O O O B-LOC O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O B-ORG O O O O O O O O B-LOC O O O O O B-LOC O O O O O O O O O O O O O': 1, 'O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O O O O O O O O O O': 1, 'B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-ORG O O O O O O O B-DATE I-DATE O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O': 1, 'O O O B-LOC O B-LOC O O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC I-LOC O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O O B-LOC O O O O O O O O O O O O O O B-LOC O O O O O B-LOC O': 1, 'O O O O B-LOC O O O O B-LOC O B-LOC O B-LOC O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O B-ORG I-ORG O': 1, 'O O O B-LOC O': 1, 'O O O O O O B-LOC I-LOC O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O B-DATE O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O B-PER I-PER O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O B-LOC B-PER I-PER O': 1, 'O O O O B-LOC O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O B-DATE I-DATE O': 1, 'O O O B-PER I-PER O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O B-DATE I-DATE O O O O O': 1, 'O O B-LOC O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O B-PER I-PER I-PER O O B-PER I-PER O O O O O': 1, 'O O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O B-LOC O B-PER I-PER O O O B-LOC O B-PER I-PER O O O B-LOC I-LOC O B-PER I-PER O': 1, 'B-ORG I-ORG O O O O O O O O B-DATE I-DATE I-DATE O O O B-PER I-PER I-PER O O O O O B-ORG I-ORG O O O O O O O O O O O O O B-ORG I-ORG O': 1, 'O O B-LOC O O B-LOC I-LOC I-LOC O B-DATE O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O B-ORG O O O O O O O O O O O B-DATE O O O O O O': 1, 'O O O O O O O O O O O O B-PER I-PER O O O B-ORG O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O B-LOC O O O O O O O O B-PER O B-PER O': 1, 'O O B-LOC O O O O O O O O O B-LOC O B-ORG O B-ORG O O O O O O B-PER O O': 1, 'O O O O B-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O B-LOC O O O O O O': 1, 'O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-PER I-PER': 1, 'O O O O O B-DATE O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O B-LOC I-LOC O': 1, 'B-DATE I-DATE O B-ORG O O O O O O O O B-ORG I-ORG O O B-LOC O': 1, 'O O B-PER I-PER O O O O O O B-ORG O O O O O O O O O': 1, 'O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG I-ORG I-ORG O B-DATE O B-DATE O B-DATE O B-PER I-PER O': 1, 'O O O O B-LOC O O O O B-DATE O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O B-LOC O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-DATE O': 1, 'O O O O O O O O O O O O O B-ORG O O B-LOC I-LOC O B-LOC I-LOC I-LOC O B-LOC I-LOC O': 1, 'B-PER I-PER O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O B-LOC O O O O B-DATE I-DATE O O O O O O O O O O O O O B-ORG I-ORG O': 1, 'O O O O O O B-PER I-PER O O O O O O O B-ORG O O O O O B-PER O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O B-PER O B-PER O O O O O B-LOC O': 1, 'O O O O B-PER I-PER O O O O O O O O B-LOC I-LOC O O O O O O O O O O O': 1, 'O B-PER I-PER O O B-PER I-PER O O O O O O O B-LOC B-DATE O': 1, 'O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O B-LOC O B-LOC O B-DATE O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER I-PER O O O B-PER O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O B-DATE O O O O O O O B-PER I-PER O O O O O B-LOC O O O O O B-PER O O O O O O O O O O O': 1, 'O O B-ORG I-ORG O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O': 1, 'O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O B-LOC I-LOC O O O O O O O O O B-LOC O O O': 1, 'O O O B-LOC O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O': 1, 'O O B-PER O O O O O O O O O O O B-LOC O O B-PER O O O O O B-DATE I-DATE O O O': 1, 'B-PER O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O B-DATE I-DATE O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O O O': 1, 'B-PER O O O O B-PER I-PER O O O': 1, 'B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O B-PER I-PER O': 1, 'O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER': 1, 'O O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O': 1, 'O O O O O B-LOC B-PER I-PER O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O': 1, 'B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O': 1, 'B-PER O O B-LOC O O O B-LOC O O B-LOC I-LOC O B-DATE I-DATE I-DATE O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG I-ORG I-ORG O O O O O O O O O O O': 1, 'O O O O O O O O O O B-ORG O': 1, 'B-PER O O O O O O O O O O': 1, 'O O O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-ORG I-ORG O O B-DATE O O O O O O B-DATE O': 1, 'O O O O B-LOC O O O O B-LOC O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-ORG I-ORG O O O O O O O O O B-DATE I-DATE O': 1, 'O B-DATE O B-PER I-PER O O O O B-PER O': 1, 'O O O O B-PER O O O O O O O O O O O O B-LOC O O O O O O B-LOC O O O O O O O O O B-LOC O O O O O B-PER O': 1, 'O O B-PER O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O': 1, 'O O O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O O O O O O B-LOC O O O O O O O O B-LOC O': 1, 'B-ORG I-ORG O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O': 1, 'O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-DATE O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O B-LOC I-LOC O O B-LOC O O O B-LOC B-DATE O': 1, 'O B-DATE I-DATE I-DATE O B-ORG O O O O O B-LOC I-LOC O O O O O': 1, 'O O O O O O B-PER I-PER O': 1, 'B-ORG I-ORG O O O O': 1, 'O O O O O O O O O O O O O B-PER O B-ORG O O B-DATE O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O B-PER O O O O O O O B-LOC O O O O O O': 1, 'B-PER I-PER O O O O B-PER O O O O O O O O B-PER O O O O O': 1, 'O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O O O B-ORG O': 1, 'O B-DATE I-DATE O O O O O O O O O': 1, 'O B-DATE I-DATE O O O O O B-LOC O O O O B-PER O B-LOC O O O O O O': 1, 'O O O O O B-DATE I-DATE I-DATE O B-LOC I-LOC O O O B-LOC O': 1, 'B-ORG I-ORG I-ORG I-ORG O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O B-PER O O O O O O O O O O': 1, 'B-PER I-PER O B-PER I-PER O O O B-PER O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O B-PER I-PER O B-PER I-PER O O': 1, 'O B-DATE I-DATE I-DATE O O B-ORG O O B-ORG O O O O O O O O O B-ORG O': 1, 'O O O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O': 1, 'O O O B-PER I-PER O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O B-LOC O': 1, 'O O O O B-LOC O O O O B-LOC I-LOC O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O B-ORG O B-ORG O': 1, 'O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O B-DATE I-DATE O B-ORG O O O O O O B-PER I-PER O O O O O B-DATE I-DATE O B-PER I-PER O O O O': 1, 'O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O O O O O O O O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER': 1, 'O O O O B-PER I-PER I-PER I-PER O O O O O O O O O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O': 1, 'B-LOC O O O O O O B-LOC O O B-DATE I-DATE O O O O O O O B-DATE I-DATE O': 1, 'O O O O O B-PER I-PER O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O': 1, 'O O O O O B-LOC O B-LOC O O O O B-PER O': 1, 'B-PER O O O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O B-PER O O B-ORG I-ORG I-ORG O O O O O O O': 1, 'O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O B-LOC': 1, 'O O O O O O O B-ORG O O O O O O O B-DATE O B-PER O O O O O O B-PER I-PER O O B-LOC O O O O B-PER O O O O O': 1, 'O O O B-PER O O O B-LOC O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O B-PER I-PER O O O O': 1, 'O O O O O B-ORG O B-ORG I-ORG O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O O B-ORG O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG O O O O B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O B-LOC O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O O B-LOC O': 1, 'O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-PER I-PER O': 1, 'O O O B-LOC B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O': 1, 'O B-LOC O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-DATE O O O B-PER I-PER I-PER I-PER I-PER O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O': 1, 'O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O': 1, 'O O O O O B-PER I-PER O B-LOC I-LOC I-LOC O': 1, 'O B-DATE I-DATE O O O O O O O O O O O O O O B-DATE O B-PER O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O B-LOC O O O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O': 1, 'O O O O O B-LOC O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O': 1, 'B-DATE I-DATE I-DATE O O B-DATE I-DATE O O O O O O O O O B-DATE I-DATE O B-DATE I-DATE I-DATE O': 1, 'O B-DATE I-DATE I-DATE O O O O O O B-ORG O B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O': 1, 'B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O O': 1, 'O O O O O B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O': 1, 'O O O O O O O O O O O O O O O O O O O O O B-ORG O B-LOC I-LOC O O O O O O O O O O B-LOC O': 1, 'O O O O O B-PER O O O O O B-LOC O B-DATE O O O O O O O B-LOC O O': 1, 'O O B-PER I-PER O O O O B-DATE I-DATE I-DATE O O O O O B-ORG I-ORG O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O O B-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE O': 1, 'B-PER I-PER O O O B-LOC O': 1, 'B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-DATE I-DATE O O O O O O O': 1, 'B-PER I-PER O O O B-PER O O B-PER I-PER O O B-PER I-PER O O B-PER O O': 1, 'O O B-PER O O O O O O O O O B-LOC I-LOC O O': 1, 'O O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O': 1, 'O O O B-PER I-PER B-ORG I-ORG O B-LOC I-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC I-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O': 1, 'O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O B-DATE I-DATE I-DATE O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O O O O O O O O O O O O O': 1, 'O B-DATE I-DATE O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-ORG I-ORG I-ORG O O O B-LOC O O O O O B-LOC O O O O O B-ORG I-ORG O O O B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O B-PER O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O B-ORG O B-LOC I-LOC O': 1, 'O O O O O O O O O O O O O B-LOC O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O B-DATE O': 1, 'B-ORG O B-ORG O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-ORG O B-PER I-PER O O O O B-ORG O B-PER I-PER O O O B-PER I-PER O B-PER I-PER O O O O B-PER O O O': 1, 'O B-LOC O O B-PER I-PER O O O O O O O O B-DATE I-DATE O O': 1, 'B-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC O B-LOC O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O B-LOC B-DATE O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O O O B-PER I-PER O O O O B-LOC': 1, 'O O B-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O': 1, 'O O B-LOC O O B-LOC O O O O O O O B-PER I-PER I-PER O O O': 1, 'O O O B-ORG O O B-PER I-PER O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O': 1, 'O O B-DATE I-DATE I-DATE O O O O O O O O O O O B-DATE O O O O B-DATE I-DATE O O O O O O O O O O': 1, 'O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O O O O B-LOC O B-LOC O O O B-LOC O O O O O O O O O B-LOC O O O O B-LOC O O O O O O O B-ORG O': 1, 'O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-PER I-PER O B-ORG I-ORG O O O O O O O O O B-PER O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O B-LOC O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O': 1, 'B-DATE I-DATE I-DATE B-ORG O O O O O B-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC O O O': 1, 'O O O O O O O O O O O O O O O B-ORG O B-ORG O': 1, 'O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O': 1, 'O B-PER I-PER I-PER O B-LOC O O B-PER O O O O O O B-LOC I-LOC I-LOC O O O O O O B-LOC O O': 1, 'B-PER O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC I-LOC O B-ORG O O B-DATE I-DATE O O B-DATE O O O O O O O B-DATE O O O B-LOC O O O O B-LOC O B-LOC O O O B-LOC O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-ORG O B-DATE O O O O O O O O O O O B-DATE O O O O': 1, 'O O O O O O O O O O O B-PER O': 1, 'O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O': 1, 'O O B-ORG I-ORG O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O': 1}\n",
            "language_distribution: {'kin': 350, 'lug': 350, 'swh': 350}\n",
            "split_distribution: {'masakhane': 1050}\n",
            "\n",
            "--- EVAL Dataset ---\n",
            "Shape: (225, 4)\n",
            "\n",
            "Column Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 225 entries, 175 to 109\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   text      225 non-null    object\n",
            " 1   label     225 non-null    object\n",
            " 2   language  225 non-null    object\n",
            " 3   split     225 non-null    object\n",
            "dtypes: object(4)\n",
            "memory usage: 8.8+ KB\n",
            "None\n",
            "\n",
            "Sample Data:\n",
            "                                                  text  \\\n",
            "175  Abawagizi be baasigadde bamutenda nti musajja ...   \n",
            "107  Minisiteri y’Imari n’Igenamigambi yakoze gahun...   \n",
            "6    Utafiti huu pia umezingatia mauaji mikononi mw...   \n",
            "151  Ono yasabye poliisi okukozesa kamera ezaateeke...   \n",
            "94   3 zifite agaciro k’amafaranga asaga miliyari 2...   \n",
            "\n",
            "                                                 label language      split  \n",
            "175                                O O O O O O O O O O      lug  masakhane  \n",
            "107  B-ORG I-ORG I-ORG O O O O O O B-LOC O O O O O ...      kin  masakhane  \n",
            "6    O O O O O O O O O O O O O O O O O O O O O O O ...      swh  masakhane  \n",
            "151                        O O O O O O O O O O O O O O      lug  masakhane  \n",
            "94   O O O O O O O O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
            "\n",
            "Dataset Statistics:\n",
            "num_samples: 225\n",
            "avg_text_length: 160.05777777777777\n",
            "num_classes: 187\n",
            "class_distribution: {'O O O O O O O O O O': 6, 'O O O O O O O O O O O': 5, 'O O O': 4, 'O O O O O O O O O O O O': 4, 'O O O O O O O O O O O O O O O O O': 4, 'O O O O O O O O O O O O O O': 4, 'O O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O': 2, 'O O O O O O O': 2, 'O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O': 2, 'O B-PER O O O O O O O O O O O': 2, 'O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O': 2, 'O O O O O O O O B-PER I-PER O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O': 1, 'O O O B-DATE O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O B-LOC O B-PER I-PER I-PER I-PER O B-DATE O O O O O O B-LOC I-LOC O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC B-PER I-PER O O O O B-PER I-PER O B-LOC O O O B-LOC O O O O O O O O O O O O O': 1, 'O O O O O B-PER O B-PER O O B-PER O B-PER O O O O O O O O O O O O O': 1, 'O O O B-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG O O O O B-PER I-PER O O O B-PER O O O O O O O O O': 1, 'O O O B-ORG O O O O O O O O': 1, 'O O O O O O B-LOC O O O O': 1, 'O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O O B-PER': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O B-DATE I-DATE I-DATE O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O O B-PER I-PER I-PER I-PER O O O O O O B-ORG O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O': 1, 'O O O O O B-PER I-PER O O B-PER O O O O O O O B-LOC I-LOC O O O O O B-PER O O O O': 1, 'O O O O B-LOC O O': 1, 'O O O O O O B-LOC O O O O O B-PER O': 1, 'B-ORG I-ORG O O O O O O O B-LOC O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O B-LOC I-LOC O B-LOC O O B-LOC I-LOC O O O O O B-ORG I-ORG I-ORG I-ORG O O O B-DATE O': 1, 'B-PER O O O O O O O O O O O O O O O': 1, 'B-PER O O B-LOC O O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O O B-DATE I-DATE O O O O O O O O O O O O O': 1, 'O O O O O O O B-DATE O O O O O O O O O O B-LOC O O O O O O O O O O O O O': 1, 'O B-PER I-PER O B-PER I-PER I-PER I-PER I-PER O O O O O B-LOC O B-LOC O O O O O O O O O': 1, 'O O O O O O B-DATE I-DATE O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O B-LOC I-LOC I-LOC O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-ORG O O O O B-ORG O O B-PER O': 1, 'B-PER O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O B-ORG O B-PER O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O O O B-DATE I-DATE O O O O O O': 1, 'O O B-DATE I-DATE O B-DATE I-DATE O O B-DATE I-DATE O B-DATE I-DATE O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O O B-PER I-PER O O O O B-PER I-PER O O B-ORG I-ORG O B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG O O O O O O O O O O O O B-ORG I-ORG O': 1, 'O B-ORG I-ORG I-ORG O B-PER O O O O O O O O O O O O O O O O': 1, 'O O B-ORG O O O O O O O O O B-ORG O O O O O O O O O O O O': 1, 'O B-PER I-PER O O B-ORG O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O O O': 1, 'O O B-DATE O O O O B-LOC O O O O B-LOC O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-ORG O': 1, 'O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O': 1, 'O O O O O O B-LOC O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O O O O O O O B-LOC O B-LOC O B-LOC O': 1, 'O O O O B-DATE I-DATE I-DATE O O O O O O O': 1, 'O O B-ORG O O O O O O O O B-DATE O O O B-LOC O O O O O O O': 1, 'O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG O O B-ORG I-ORG I-ORG O O O B-LOC I-LOC I-LOC O O O B-LOC I-LOC O O B-ORG I-ORG O O B-ORG I-ORG O': 1, 'O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'B-PER O O O O O O O O O O B-ORG O O O O O O O O O': 1, 'O B-PER O O O O O O O B-PER O O O O O O O O B-LOC O B-LOC O': 1, 'B-PER O O B-PER O O O O O O O O O O O B-PER O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC B-PER I-PER O O O O O O O O O B-LOC O': 1, 'O O O O O B-DATE O O O O O O O O B-LOC O O O O O B-LOC O B-LOC O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O B-PER I-PER O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC B-DATE O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O': 1, 'O O B-PER O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O B-LOC O O B-DATE O O B-PER I-PER O O O O O B-ORG I-ORG O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-PER I-PER O': 1, 'O O O O O O O O O O O O O O O B-LOC O O': 1, 'O O B-DATE I-DATE I-DATE O B-PER O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O O O O O B-LOC O O O O O O O O O O B-PER O O O O O O O': 1, 'O O O O O B-PER I-PER O B-LOC I-LOC I-LOC O': 1, 'O B-ORG I-ORG I-ORG B-PER I-PER I-PER O O O O O O O O O O O O O': 1, 'O O O B-ORG O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O': 1, 'B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O B-DATE I-DATE O B-LOC I-LOC I-LOC O O O B-LOC O': 1, 'B-LOC O O O O O O O O O B-LOC O B-LOC O': 1, 'O B-PER O O O O O O O O O B-PER O O O O B-PER O O O O O O O O O O B-PER O': 1, 'O B-PER O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG I-ORG O O O O O O B-LOC I-LOC O O O O O B-PER I-PER O O O B-LOC O O O O O O B-ORG I-ORG O O O O O O O O B-PER I-PER O O O O O': 1, 'B-PER O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE I-DATE O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG I-ORG O B-ORG I-ORG O O B-LOC O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O B-LOC O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O B-ORG I-ORG O': 1, 'O O B-ORG O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-DATE O O O O O O B-LOC O O O B-LOC O': 1, 'B-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O': 1, 'O O O B-DATE O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O B-PER O O O O O O O O B-LOC O': 1, 'B-LOC O O O O O O B-ORG O O O O O O': 1, 'O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O': 1, 'O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O B-LOC I-LOC O': 1, 'O O B-LOC O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O B-LOC O': 1, 'O O B-ORG O O O O O O O O O B-LOC O B-LOC O O B-LOC O B-LOC O': 1, 'O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O B-ORG I-ORG O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O B-PER I-PER O O O B-PER I-PER O O O B-ORG I-ORG O O O': 1, 'O O O B-ORG O O O O O O B-ORG O O O O': 1, 'O O O O O O O O O B-LOC O O B-ORG I-ORG O B-ORG O B-ORG O': 1, 'B-DATE I-DATE I-DATE O O O O O O O O O B-LOC O O O O O O': 1, 'B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-LOC O O O O O B-LOC O O O O O O O B-DATE I-DATE O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE I-DATE O O O O B-LOC O O B-DATE I-DATE I-DATE I-DATE O O O O O O O B-DATE O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O O B-ORG I-ORG I-ORG O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O B-LOC O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O B-LOC O B-LOC I-LOC O B-LOC O': 1, 'O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O O O B-LOC I-LOC I-LOC O O O O B-LOC O B-LOC O O O O B-LOC O B-LOC O O O O B-LOC O O': 1, 'B-ORG I-ORG I-ORG I-ORG I-ORG O B-DATE O O O O O O O O O O O O O': 1, 'O B-LOC O O O': 1, 'O O O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O': 1, 'O O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O B-DATE I-DATE O B-DATE O': 1, 'B-ORG I-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O B-PER O O O B-ORG O O O O O O O O O O B-ORG O B-PER O O O O': 1, 'O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O O B-ORG O': 1, 'O O O O O O B-ORG B-PER I-PER I-PER O O B-DATE O O O O O O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER O B-PER O B-PER O O O O O O B-PER O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O': 1, 'O O O O B-LOC O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-PER B-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O B-PER O O O O B-DATE O': 1, 'O O B-ORG O B-PER I-PER O O B-DATE I-DATE O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O': 1, 'O O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O B-DATE O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O O B-PER O B-LOC O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O O O O B-LOC O O O O O B-PER I-PER O O O B-PER I-PER I-PER O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O B-LOC O B-LOC O': 1, 'B-PER O O O O O B-LOC O O O O B-PER O O O B-ORG O O O O O B-PER O O O B-PER O O O O': 1, 'O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O B-LOC O': 1, 'O O B-PER O O B-DATE I-DATE O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1}\n",
            "language_distribution: {'lug': 75, 'kin': 75, 'swh': 75}\n",
            "split_distribution: {'masakhane': 225}\n",
            "\n",
            "--- BENCHMARK Dataset ---\n",
            "Shape: (500, 4)\n",
            "\n",
            "Column Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 500 entries, 397 to 244\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   text      184 non-null    object\n",
            " 1   label     184 non-null    object\n",
            " 2   language  184 non-null    object\n",
            " 3   split     500 non-null    object\n",
            "dtypes: object(4)\n",
            "memory usage: 19.5+ KB\n",
            "None\n",
            "\n",
            "Sample Data:\n",
            "                                                  text  \\\n",
            "397                                                NaN   \n",
            "549                                                NaN   \n",
            "525                                                NaN   \n",
            "167  Ono kati yeegasse ku Tonny Mawejje ne Hassa Mo...   \n",
            "509                                                NaN   \n",
            "\n",
            "                                                 label language  \\\n",
            "397                                                NaN      NaN   \n",
            "549                                                NaN      NaN   \n",
            "525                                                NaN      NaN   \n",
            "167  O O O O B-PER I-PER O B-PER I-PER B-ORG I-ORG ...      lug   \n",
            "509                                                NaN      NaN   \n",
            "\n",
            "                  split  \n",
            "397      flores_200_dev  \n",
            "549  flores_200_devtest  \n",
            "525  flores_200_devtest  \n",
            "167           masakhane  \n",
            "509  flores_200_devtest  \n",
            "\n",
            "Dataset Statistics:\n",
            "num_samples: 500\n",
            "avg_text_length: 142.7554347826087\n",
            "num_classes: 165\n",
            "class_distribution: {'O O O O O O O O O': 5, 'O O O O O O O O': 3, 'O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O': 2, 'O O': 2, 'B-PER O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O B-DATE O O O': 1, 'O O O O O B-PER O O O O O O': 1, 'B-PER O B-LOC O B-LOC I-LOC O B-DATE I-DATE I-DATE I-DATE O O O O O O B-PER O O O O O O O O O O B-LOC O O O B-LOC O': 1, 'O O O O B-PER I-PER O O O O': 1, 'O O O O O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O B-LOC O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O B-LOC B-ORG I-ORG I-ORG O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O B-LOC I-LOC O': 1, 'O O O O O O O O O O O B-ORG I-ORG I-ORG O O O B-ORG I-ORG O O O O O O O O': 1, 'O O O O B-ORG O O O O O O B-PER I-PER O O O B-ORG O': 1, 'O O O B-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O': 1, 'O O O O O O B-LOC O O B-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER O B-PER I-PER B-ORG I-ORG O O O B-DATE I-DATE I-DATE O': 1, 'O O O O B-LOC O O O O O B-PER I-PER I-PER O O O B-ORG O B-LOC O B-PER I-PER I-PER O O': 1, 'B-PER O O O O O O O O O O B-LOC I-LOC B-ORG O O O O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O B-ORG I-ORG O': 1, 'O O O O O O B-LOC O O O O O O O B-LOC O B-LOC O O O O B-LOC O O O B-LOC O O O O B-LOC O O O': 1, 'B-PER O B-PER O O O O O O O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O B-LOC O O O': 1, 'O O O O O O O O O B-DATE O': 1, 'O O B-PER O O O O O O B-ORG O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O': 1, 'O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O B-LOC O B-LOC I-LOC B-DATE': 1, 'O O O O O O O O B-DATE I-DATE O B-DATE I-DATE O O B-DATE I-DATE O B-DATE I-DATE O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O B-PER O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-DATE I-DATE O O O': 1, 'B-PER I-PER O O B-LOC I-LOC O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O O O B-PER I-PER I-PER O B-PER I-PER O O O O B-LOC O O O O O O B-LOC O O O B-LOC O O O O': 1, 'O O O O O O B-DATE O B-PER I-PER O O O O O O B-LOC O B-LOC O O B-DATE I-DATE I-DATE O O O O O O B-LOC O B-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O B-LOC O O O O B-PER I-PER I-PER I-PER O O O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O B-LOC I-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-ORG O O O O O B-ORG I-ORG O O B-LOC O O B-LOC I-LOC O O O O O O O O B-LOC O': 1, 'O O O B-LOC I-LOC O O O B-PER O O O O O O B-LOC O O O O B-ORG I-ORG I-ORG O O B-DATE O O': 1, 'B-PER O B-PER O O O B-DATE O O O O O O O O': 1, 'O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-DATE O O O O O O O O': 1, 'B-PER O O O O O B-DATE O O O O B-LOC O O B-LOC O O O O': 1, 'O O B-ORG O O O B-PER O O O O O O O O O O O O O O O': 1, 'B-ORG O O O O O B-LOC O O B-LOC': 1, 'B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O B-PER I-PER O O O O B-ORG I-ORG I-ORG O B-LOC O O O O O': 1, 'B-LOC I-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O': 1, 'O O O O O O O O O B-DATE I-DATE O': 1, 'O O O O B-PER O O O O O O O O O B-PER I-PER O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O B-PER O O O O O O B-DATE O O O B-LOC B-PER I-PER O O O O O B-LOC B-PER I-PER O O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O O O O B-PER O O O': 1, 'O O B-PER O O O O B-PER O B-PER I-PER I-PER O O O B-PER O O O O O O O B-PER O O O O O O O O O O O': 1, 'O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-DATE O O B-PER I-PER I-PER O O O O O O O O O O O O O B-PER I-PER I-PER O': 1, 'O O B-ORG O B-LOC O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O B-DATE O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-PER O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O': 1, 'O O O B-ORG O B-PER I-PER O B-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-ORG O O O O O O O O O O O': 1, 'O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O B-PER O O O B-PER O O O O O O O O O O O O': 1, 'O B-LOC I-LOC O B-DATE I-DATE O O O O O O B-PER I-PER O B-PER I-PER I-PER O': 1, 'O O B-LOC B-DATE O O O O O O O B-LOC I-LOC O B-PER O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O B-PER I-PER O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O B-DATE O B-ORG I-ORG O B-ORG I-ORG O B-LOC O B-ORG I-ORG O B-ORG I-ORG O B-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O B-DATE O': 1, 'B-PER I-PER O B-ORG O O O B-DATE': 1, 'O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-DATE I-DATE I-DATE O O O': 1, 'O O B-ORG I-ORG O B-PER I-PER O O O O O O O O O B-LOC O O B-LOC O O O B-LOC O': 1, 'O O O O B-LOC O O O': 1, 'O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O B-PER I-PER O O O': 1, 'O O O O O O B-LOC O B-DATE I-DATE': 1, 'O O O B-PER O O O O O O B-LOC O O O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O B-LOC O': 1, 'B-LOC I-LOC O O O O O O O O O B-LOC I-LOC O B-PER I-PER I-PER I-PER O O O O O O B-PER O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC I-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O B-LOC O O B-ORG I-ORG I-ORG O B-DATE I-DATE O': 1, 'O O O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O B-PER O O O O O O O O O O': 1, 'O O O B-PER O O O O O O O O B-LOC O O B-PER O O O O O O O O O O B-LOC O': 1, 'B-LOC I-LOC I-LOC': 1, 'B-PER O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O B-LOC I-LOC O O O O O O B-ORG O O O O B-PER O O O O': 1, 'O O B-ORG I-ORG O B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O O B-LOC O O O O O O O O B-ORG I-ORG O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O B-ORG O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O': 1, 'O B-PER O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O': 1, 'B-DATE I-DATE O O O O O O O O B-PER O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-ORG O': 1, 'O O O O O O O O O B-DATE I-DATE I-DATE O O O B-PER O O O O O O O O O O O': 1, 'O O O O O O O O O B-DATE I-DATE O O O B-DATE I-DATE O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O B-DATE O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-PER O B-PER O B-PER O B-PER O O O O': 1, 'O O O O B-LOC O B-LOC O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O B-LOC O O O O O O O O O O O': 1, 'O O B-DATE O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O': 1, 'O O O O O B-ORG O O O O O O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O': 1, 'O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O B-LOC O O O B-LOC O O O O O O': 1, 'O O O O O O B-LOC O B-LOC O B-LOC O': 1, 'O O O O O B-PER I-PER': 1, 'O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O B-LOC O B-LOC O O O O O': 1, 'O O O O O B-DATE I-DATE O': 1, 'O B-PER O O O O O O O O O O O O O': 1, 'O O O O': 1, 'B-DATE B-LOC O O O O B-LOC B-DATE I-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE O O B-LOC O': 1, 'O O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O': 1, 'O O O O O B-PER I-PER O O O O O O O O O O': 1, 'O B-LOC I-LOC O B-LOC O O O O': 1, 'B-PER O B-PER O O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG O O O O O B-DATE': 1, 'B-PER I-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O B-LOC O O O O O O O O O O O O O O O B-PER I-PER O O O B-LOC I-LOC I-LOC O': 1, 'O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O B-PER O O O O O': 1, 'O O O O O B-DATE I-DATE O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O': 1, 'O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O B-LOC I-LOC O B-LOC I-LOC O B-LOC I-LOC O': 1, 'O O O B-DATE O O O O O': 1, 'O O O O O O B-DATE O O O O B-ORG O O O O O O O O O B-ORG O O O O O O O O O O O': 1, 'B-ORG O O O O O O B-DATE O O O O O O O B-DATE I-DATE O': 1, 'B-PER I-PER O O O B-LOC I-LOC I-LOC O O O O B-PER I-PER O B-PER I-PER O O O O O': 1, 'B-DATE B-LOC I-LOC O O O O O O O B-PER O O O O O O': 1, 'B-PER O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O B-DATE O B-DATE O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O B-PER O B-LOC O O O O O O O O B-PER O O O O O O O B-ORG O': 1}\n",
            "language_distribution: {'kin': 63, 'lug': 59, 'swh': 54, 'eng': 8}\n",
            "split_distribution: {'masakhane': 176, 'flores_200_dev': 158, 'flores_200_devtest': 158, 'zero_shot': 8}\n"
          ]
        }
      ],
      "source": [
        "# Print dataset information\n",
        "data_loader.print_dataset_info(stratified_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWy1n7Z7cYqn",
        "outputId": "f6b87f21-ffca-460c-c651-7278665e31f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/MScAI-BigData-FYP/py/dataset_loader.py:509: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['text'] = dataset['text'].str.lower()\n",
            "/content/MScAI-BigData-FYP/py/dataset_loader.py:512: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['text'] = dataset['text'].str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
            "/content/MScAI-BigData-FYP/py/dataset_loader.py:515: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['text'] = dataset['text'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
            "/content/MScAI-BigData-FYP/py/dataset_loader.py:519: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['label'] = dataset['label'].astype(str)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess datasets\n",
        "preprocessed_datasets = {\n",
        "    key: data_loader.preprocess_dataset(dataset)\n",
        "    for key, dataset in stratified_datasets.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFl7l8_FcYqn",
        "outputId": "9859b502-a349-4b01-b03f-b29b13cb5c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\n",
            "Current working directory: /content\n",
            "Directory contents: ['.config', 'MScAI-BigData-FYP', 'py', '.ipynb_checkpoints', 'model_cache', 'mlruns', 'model_outputs', '.git', 'ipynb']\n",
            "Parent directory contents: ['lib32', 'tmp', 'sbin', 'root', 'lib', 'var', 'etc', 'mnt', 'lib64', 'boot', 'sys', 'home', 'media', 'libx32', 'usr', 'srv', 'opt', 'dev', 'bin', 'run', 'proc', 'content', 'kaggle', '.dockerenv', 'tools', 'datalab', 'python-apt', 'NGC-DL-CONTAINER-LICENSE', 'cuda-keyring_1.0-1_all.deb']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Directory contents: {os.listdir()}\")\n",
        "print(f\"Parent directory contents: {os.listdir('..')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UTisJzEcYqn",
        "outputId": "fe2478aa-689b-437e-9042-98ff05b4a675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added /content/MScAI-BigData-FYP/py/models to Python path\n"
          ]
        }
      ],
      "source": [
        "# Check if the models directory is in the Python path\n",
        "models_dir = os.path.abspath(os.path.join('/content/MScAI-BigData-FYP', 'py', 'models'))\n",
        "if models_dir not in sys.path:\n",
        "    sys.path.append(models_dir)\n",
        "    print(f\"Added {models_dir} to Python path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgzeM84ycYqn",
        "outputId": "41b9ec8b-d1d4-4c56-9361-b2431f9870d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models directory contents: ['afro_xlmr_large.py', '__pycache__', 'ernie_m.py', 'llama2_decoder.py', 'llama3.1']\n"
          ]
        }
      ],
      "source": [
        "# Print contents of the models directory\n",
        "print(f\"Models directory contents: {os.listdir(models_dir)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "-phHbrgecYqn"
      },
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models, tokenizers = {}, {}\n",
        "num_labels = len(preprocessed_datasets['train']['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798,
          "referenced_widgets": [
            "0ffbcde28fc54465aab6030092cf2863",
            "8a9b99d6f4014403ab2e7566bb4d0be8",
            "c32f031f80b646e488a398f28ac3ebb6",
            "103e151c9aae48208668d50fa6d97477",
            "aeb1c770c6224c6ba8f03cb69a59aafd",
            "ff8f57d1fa244a8bbae11ae2cb3a4569",
            "f0eb8151baf3495b8a8a28f0a3695361",
            "47772b8afbcb4731b00c2d991c694173",
            "131abe7378b8493c8c87c306b50cf519",
            "4671f7a5d767488eb193b16bb210a98d",
            "bc0465df0bcd4a93bbba14200c3d6c07"
          ]
        },
        "id": "wzw9sUYIcYqn",
        "outputId": "2b72cce3-b101-4153-886e-a1d895c2606b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model: afro-xlmr-large\n",
            "Cache directory: /content/model_cache\n",
            "Auth token: hf_EF...Ulmqr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully initialized afro-xlmr-large\n",
            "Initializing model: meta-llama/Llama-2-7b-hf\n",
            "Cache directory: /content/model_cache\n",
            "Auth token: hf_EF...Ulmqr\n",
            "Initializing Llama2Decoder with:\n",
            "  model_name: meta-llama/Llama-2-7b-hf\n",
            "  auth_token: hf_EF...Ulmqr\n",
            "  cache_dir: /content/model_cache\n",
            "Set cache directory to: /content/model_cache\n",
            "Model meta-llama/Llama-2-7b-hf not found in cache. Will attempt to download.\n",
            "Initializing tokenizer from meta-llama/Llama-2-7b-hf...\n",
            "Tokenizer parameters:\n",
            "  pretrained_model_name_or_path: meta-llama/Llama-2-7b-hf\n",
            "  use_auth_token: True\n",
            "  cache_dir: /content/model_cache\n",
            "  local_files_only: False\n",
            "Initializing model from meta-llama/Llama-2-7b-hf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ffbcde28fc54465aab6030092cf2863"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully initialized meta-llama/Llama-2-7b-hf\n",
            "Model device: cuda:0\n",
            "Successfully initialized meta-llama/Llama-2-7b-hf\n",
            "Initializing model: ernie-m-large\n",
            "Cache directory: /content/model_cache\n",
            "Auth token: hf_EF...Ulmqr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ErnieMForSequenceClassification were not initialized from the model checkpoint at MoritzLaurer/ernie-m-large-mnli-xnli and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([802]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([802, 1024]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully initialized ernie-m-large\n"
          ]
        }
      ],
      "source": [
        "# Initialize models with the configured parameters and authentication token\n",
        "for model_name in config['model']['names']:\n",
        "    print(f\"Initializing model: {model_name}\")\n",
        "    print(f\"Cache directory: {cache_dir}\")\n",
        "    print(f\"Auth token: {auth_token[:5]}...{auth_token[-5:] if auth_token else None}\")\n",
        "    try:\n",
        "        if model_name == \"meta-llama/Llama-2-7b-hf\":\n",
        "            llama_model = Llama2Decoder(model_name, auth_token=auth_token, cache_dir=cache_dir)\n",
        "            models[model_name] = llama_model.get_model()   # The model is already on the appropriate device\n",
        "            tokenizers[model_name] = llama_model.get_tokenizer()\n",
        "        elif model_name == \"ernie-m-large\":\n",
        "            ernie_model = ErnieM(num_labels)\n",
        "            models[model_name] = ernie_model.get_model()\n",
        "            tokenizers[model_name] = ernie_model.get_tokenizer()\n",
        "        else:\n",
        "            model, tokenizer = get_model(model_name, num_labels=num_labels, auth_token=auth_token, cache_dir=cache_dir)\n",
        "            models[model_name] = model.to('cuda') # Ensure Afro XLMR is on GPU\n",
        "            tokenizers[model_name] = tokenizer\n",
        "\n",
        "        if models[model_name] is not None and tokenizers[model_name] is not None:\n",
        "            print(f\"Successfully initialized {model_name}\")\n",
        "        else:\n",
        "            print(f\"Failed to initialize {model_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing {model_name}: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmHZta2McYqn",
        "outputId": "7bc38251-c84c-4436-e485-f8bfce098913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models: ['afro-xlmr-large', 'meta-llama/Llama-2-7b-hf', 'ernie-m-large']\n",
            "Available tokenizers: ['afro-xlmr-large', 'meta-llama/Llama-2-7b-hf', 'ernie-m-large']\n"
          ]
        }
      ],
      "source": [
        "print(\"Available models:\", list(models.keys()))\n",
        "print(\"Available tokenizers:\", list(tokenizers.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "93CULEB1cYqn"
      },
      "outputs": [],
      "source": [
        "# Create custom datasets for PyTorch\n",
        "datasets = {}\n",
        "\n",
        "model_type = 'encoder_decoder'  # New model type for the combined model\n",
        "\n",
        "# For the combined Afro-XLMR and LLaMA model\n",
        "combined_tokenizer = tokenizers['afro-xlmr-large']  # Assuming you're using the Afro-XLMR tokenizer for the combined model\n",
        "\n",
        "logging.info(f\"Creating datasets for combined Afro-XLMR and LLaMA model with model type: {model_type}\")\n",
        "\n",
        "# Use the key 'combined_afro_xlmr_llama'\n",
        "datasets['combined_afro_xlmr_llama'] = {\n",
        "    'train': CustomDataset(preprocessed_datasets['train'], combined_tokenizer, model_type=model_type),\n",
        "    'eval': CustomDataset(preprocessed_datasets['eval'], combined_tokenizer, model_type=model_type),\n",
        "    'benchmark': CustomDataset(preprocessed_datasets['benchmark'], combined_tokenizer, model_type=model_type)\n",
        "}\n",
        "\n",
        "# For ERNIE-M\n",
        "ernie_m_tokenizer = tokenizers['ernie-m-large']\n",
        "\n",
        "logging.info(f\"Creating datasets for ERNIE-M with model type: {model_type}\")\n",
        "\n",
        "datasets['ernie-m-large'] = {\n",
        "    'train': CustomDataset(preprocessed_datasets['train'], ernie_m_tokenizer, model_type=model_type),\n",
        "    'eval': CustomDataset(preprocessed_datasets['eval'], ernie_m_tokenizer, model_type=model_type),\n",
        "    'benchmark': CustomDataset(preprocessed_datasets['benchmark'], ernie_m_tokenizer, model_type=model_type)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk7O-4ItcYqn",
        "outputId": "e0bb7983-eb4a-477e-aa11-ccac1c0cd03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'combined_afro_xlmr_llama': {'train': <utils.CustomDataset object at 0x7a4c676f2650>, 'eval': <utils.CustomDataset object at 0x7a4c66b95630>, 'benchmark': <utils.CustomDataset object at 0x7a4c66b958d0>}, 'ernie-m-large': {'train': <utils.CustomDataset object at 0x7a4c676f04f0>, 'eval': <utils.CustomDataset object at 0x7a4c5deb8ee0>, 'benchmark': <utils.CustomDataset object at 0x7a4c5deba710>}}\n"
          ]
        }
      ],
      "source": [
        "print(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXXEUiqIcYqo",
        "outputId": "3291f409-c61a-4240-d02f-569574ad101f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available datasets: dict_keys(['combined_afro_xlmr_llama', 'ernie-m-large'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Available datasets:\", datasets.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6Mstm3QcYqo",
        "outputId": "75a8d1e3-5c36-4fec-bf74-ff7e38db5f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in 'combined_afro_xlmr_llama': dict_keys(['train', 'eval', 'benchmark'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Keys in 'combined_afro_xlmr_llama':\", datasets['combined_afro_xlmr_llama'].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo0iak0ccYqo",
        "outputId": "4258dde4-0ffb-49bf-90af-a66b20825eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: <utils.CustomDataset object at 0x7a4c676f2650>\n",
            "Eval dataset: <utils.CustomDataset object at 0x7a4c66b95630>\n"
          ]
        }
      ],
      "source": [
        "print(\"Train dataset:\", datasets['combined_afro_xlmr_llama']['train'])\n",
        "print(\"Eval dataset:\", datasets['combined_afro_xlmr_llama']['eval'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "b536237d86144007813bc468924e69c7",
            "a8f39ddd14c7483a8db8f36a6401b88e",
            "38f4da2609cc4552a519673b9f6df74b",
            "3cb01ef6b44c412e9594f947b1e3f898",
            "0e268cecf57946769cb3155ec3fdda15",
            "728985bdef554b6a9689fc9d3598201f",
            "0a08301ea0eb48879dd1e290fe466779",
            "3d70a8ad4a304a10b9513fa99a01a3d6",
            "797f5acbe2b649cea08f725a352845cd",
            "a6012d11d64242daace423b394c9403a",
            "90240de1d0f54093a9af2324fdb9d638",
            "57c7755647954b9ebb3b72131ad6d6ef",
            "4c0d7c71f75d4143896de9632b4afd55",
            "bb22f20e140f421f92e58a8e7eb73488",
            "5903617761f04264a490fae0ee4f1f46",
            "debeb46bd5364e86b853c9acbfd456c9",
            "45a15ba3d802484a8fcde44ac9bb1f30",
            "6df39642146b4a21a82ab97b427a3c97",
            "21d1942245d84366a936e1b49a9f7d58",
            "6ceb48ebffc8497588dcf03179500f7c",
            "649121c453df48b086c7d76540e27a34",
            "b7c1537c238a448ba52d32d8efa4e44f"
          ]
        },
        "id": "CJPqj8_DcYqo",
        "outputId": "0809f7e3-df50-4c41-d452-21ca3421f410"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b536237d86144007813bc468924e69c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.9.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--masakhane--africomet-mtl/snapshots/91a7e56061446598665d569d89b762387399e3ac/checkpoints/model.ckpt`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57c7755647954b9ebb3b72131ad6d6ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.9.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--masakhane--africomet-mtl/snapshots/91a7e56061446598665d569d89b762387399e3ac/checkpoints/model.ckpt`\n"
          ]
        }
      ],
      "source": [
        "# Initialize evaluators\n",
        "evaluators = {\n",
        "    'combined_afro_xlmr_llama': AfriCOMETEvaluator(\n",
        "        model=None,  # AfriCOMETEvaluator doesn't use the model directly\n",
        "        tokenizer=tokenizers['afro-xlmr-large']\n",
        "    ),\n",
        "    'ernie-m-large': AfriCOMETEvaluator(\n",
        "        model=None,  # AfriCOMETEvaluator doesn't use the model directly\n",
        "        tokenizer=tokenizers['ernie-m-large']\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MzWaDVgKcYqo",
        "outputId": "399b6574-0e19-4e0d-ca9d-e822ad28dfa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-15 11:49:53,677] A new study created in memory with name: no-name-97fb174f-457e-446a-be41-e5450bcced32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CombinedEncoderDecoderTrainer: <class 'trainers.combined_encoder_decoder_trainer.CombinedEncoderDecoderTrainer'>\n",
            "CombinedEncoderDecoderTrainer.__init__: <function CombinedEncoderDecoderTrainer.__init__ at 0x7a4c688f1bd0>\n",
            "Initializing CombinedEncoderDecoderTrainer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py:51: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "Epoch 1/2:   0%|          | 0/1050 [00:00<?, ?it/s]/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "ERROR:root:Unexpected error in train_step: Target size (torch.Size([1, 128])) must be the same as input size (torch.Size([1, 4898]))\n",
            "ERROR:root:Traceback (most recent call last):\n",
            "  File \"/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py\", line 197, in train_step\n",
            "    logits = self.loss_fn(combined_hidden_states, labels.float())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 734, in forward\n",
            "    return F.binary_cross_entropy_with_logits(input, target,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3242, in binary_cross_entropy_with_logits\n",
            "    raise ValueError(f\"Target size ({target.size()}) must be the same as input size ({input.size()})\")\n",
            "ValueError: Target size (torch.Size([1, 128])) must be the same as input size (torch.Size([1, 4898]))\n",
            "\n",
            "WARNING:root:Skipping batch 0 due to error\n",
            "Epoch 1/2:   0%|          | 1/1050 [00:04<1:14:31,  4.26s/it]ERROR:root:Unexpected error in train_step: Target size (torch.Size([1, 128])) must be the same as input size (torch.Size([1, 4898]))\n",
            "ERROR:root:Traceback (most recent call last):\n",
            "  File \"/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py\", line 197, in train_step\n",
            "    logits = self.loss_fn(combined_hidden_states, labels.float())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 734, in forward\n",
            "    return F.binary_cross_entropy_with_logits(input, target,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3242, in binary_cross_entropy_with_logits\n",
            "    raise ValueError(f\"Target size ({target.size()}) must be the same as input size ({input.size()})\")\n",
            "ValueError: Target size (torch.Size([1, 128])) must be the same as input size (torch.Size([1, 4898]))\n",
            "\n",
            "WARNING:root:Skipping batch 1 due to error\n",
            "Epoch 1/2:   0%|          | 2/1050 [00:05<48:51,  2.80s/it]\n",
            "[W 2024-09-15 11:50:01,351] Trial 0 failed with parameters: {'encoder_lr': 4.93558538625628e-06, 'decoder_lr': 2.447817100278944e-06, 'num_train_epochs': 2, 'weight_decay': 0.013540871758646175, 'warmup_steps': 87, 'gradient_accumulation_steps': 128} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-35-e4d2f3e87109>\", line 30, in <lambda>\n",
            "    lambda trial: objective_combined(\n",
            "  File \"<ipython-input-33-8aa3520e4fe0>\", line 85, in objective_combined\n",
            "    train_results = trainer.train(train_dataset, eval_dataset)\n",
            "  File \"/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py\", line 99, in train\n",
            "    loss = self.train_step(batch)\n",
            "  File \"/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py\", line 177, in train_step\n",
            "    decoder_outputs = self.decoder(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1189, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1020, in forward\n",
            "    hidden_states = self.norm(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 125, in forward\n",
            "    return self.weight * hidden_states.to(input_dtype)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1716, in __getattr__\n",
            "    def __getattr__(self, name: str) -> Any:\n",
            "KeyboardInterrupt\n",
            "[W 2024-09-15 11:50:01,354] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-e9ff7cb76a97>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run optimization for combined Afro-XLMR and LLaMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m combined_study = run_combined_optimization(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'afro-xlmr-large'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta-llama/Llama-2-7b-hf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'afro-xlmr-large'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-e4d2f3e87109>\u001b[0m in \u001b[0;36mrun_combined_optimization\u001b[0;34m(encoder, decoder, tokenizer, datasets, config, evaluator)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"optimization_combined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             study.optimize(\n\u001b[0m\u001b[1;32m     30\u001b[0m                 lambda trial: objective_combined(\n\u001b[1;32m     31\u001b[0m                     \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-e4d2f3e87109>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             study.optimize(\n\u001b[0;32m---> 30\u001b[0;31m                 lambda trial: objective_combined(\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-8aa3520e4fe0>\u001b[0m in \u001b[0;36mobjective_combined\u001b[0;34m(trial, encoder, decoder, tokenizer, train_dataset, eval_dataset, config, evaluator)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, eval_dataset)\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;31m# Use mixed precision autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/MScAI-BigData-FYP/py/trainers/combined_encoder_decoder_trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# Get encoder and decoder outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# Extract hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1018\u001b[0m                 \u001b[0mall_self_attns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;31m# add hidden states from the last decoder layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run optimization for combined Afro-XLMR and LLaMA\n",
        "combined_study = run_combined_optimization(\n",
        "    encoder=models['afro-xlmr-large'],  # Encoder\n",
        "    decoder=models['meta-llama/Llama-2-7b-hf'],  # Decoder\n",
        "    tokenizer=tokenizers['afro-xlmr-large'],\n",
        "    datasets=datasets['combined_afro_xlmr_llama'],\n",
        "    config=config,\n",
        "    evaluator=evaluators['combined_afro_xlmr_llama']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQPWZ1jscYqo"
      },
      "outputs": [],
      "source": [
        "# Run optimization for ERNIE-M\n",
        "ernie_study = run_ernie_optimization(\n",
        "    models['ernie-m-large'],\n",
        "    tokenizers['ernie-m-large'],\n",
        "    datasets['ernie-m-large'],\n",
        "    config,\n",
        "    evaluators['ernie-m-large']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhgXet2vcYqo"
      },
      "outputs": [],
      "source": [
        "# Run hyperparameter optimization\n",
        "try:\n",
        "    # Consolidate the results into the studies variable\n",
        "    studies = {\n",
        "        'combined_study': combined_study,\n",
        "        'ernie-m-large': ernie_study\n",
        "    }\n",
        "    # Extract best parameters\n",
        "    best_params = {model_name: study.best_params for model_name, study in studies.items()}\n",
        "\n",
        "    # Log best parameters\n",
        "    for model_name, params in best_params.items():\n",
        "        logger.info(f\"Best hyperparameters for {model_name}: {params}\")\n",
        "        config['training'][model_name] = params\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during or after hyperparameter optimization: {str(e)}\")\n",
        "    logger.exception(\"Exception details:\")\n",
        "    studies = {}  # Initialize an empty dictionary if optimization failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSrIPZBzcYqo"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter analysis\n",
        "logger.info(\"Performing hyperparameter analysis...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMy-X9ircYqo"
      },
      "outputs": [],
      "source": [
        "for model_name, study in studies.items():\n",
        "    # Plot hyperparameter importance\n",
        "    importance_fig = plot_hyperparameter_importance(study)\n",
        "    importance_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
        "\n",
        "    # Plot optimization history\n",
        "    history_fig = plot_optimization_history(study)\n",
        "    history_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
        "\n",
        "    # Plot parallel coordinate\n",
        "    parallel_fig = plot_parallel_coordinate(study)\n",
        "    parallel_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
        "\n",
        "    # Analyze and plot sensitivity\n",
        "    sensitivity = analyze_hyperparameter_sensitivity(study)\n",
        "    sensitivity_fig = plot_sensitivity_analysis(sensitivity)\n",
        "    sensitivity_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")\n",
        "\n",
        "    # Print sensitivity analysis results\n",
        "    print(f\"\\nHyperparameter Sensitivity Analysis for {model_name}:\")\n",
        "    for param, sens in sensitivity:\n",
        "        print(f\"{param}: {sens:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eLV46picYqo"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Hyperparameter analysis complete. Plots saved in output directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1bMRbVscYqo"
      },
      "outputs": [],
      "source": [
        "# Initialize trainers with the best hyperparameters from the studies\n",
        "trainers = {\n",
        "    # Using CombinedEncoderDecoderTrainer for the encoder-decoder setup\n",
        "    'combined_afro_xlmr_llama': CombinedEncoderDecoderTrainer(\n",
        "        encoder=models['afro-xlmr-large'],  # Afro-XLMR as encoder\n",
        "        decoder=models['meta-llama/Llama-2-7b-hf'],  # LLaMA as decoder\n",
        "        tokenizer=tokenizers['afro-xlmr-large'],  # Using afro-xlmr tokenizer for both (or adjust as needed)\n",
        "        config={**config, **studies['combined_afro_xlmr_llama'].best_params}  # Best hyperparameters from the tuning study\n",
        "    ),\n",
        "    'ernie-m-large': EncoderDecoderTrainer(\n",
        "        model=models['ernie-m-large'],  # Single model setup (if it's used as an encoder-decoder internally)\n",
        "        tokenizer=tokenizers['ernie-m-large'],\n",
        "        config={**config, **studies['ernie-m-large'].best_params}  # Best hyperparameters for ERNIE-M\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkcVjUaicYqo"
      },
      "outputs": [],
      "source": [
        "# Train models with the best hyperparameters\n",
        "for model_name, trainer in trainers.items():\n",
        "    logger.info(f\"Training model: {model_name}\")\n",
        "    train_dataset = datasets[model_name]['train']\n",
        "    eval_dataset = datasets[model_name]['eval']\n",
        "\n",
        "    try:\n",
        "        trainer.train(train_dataset, eval_dataset)\n",
        "        logger.info(f\"Training completed for {model_name}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during training of {model_name}: {str(e)}\")\n",
        "        continue  # Move to the next model if there's an error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IWW4XCHcYqo"
      },
      "outputs": [],
      "source": [
        "# Perform evaluations\n",
        "results = {\n",
        "    'classification': {},\n",
        "    'translation': {},\n",
        "    'generation': {},\n",
        "    'zero_shot': {},\n",
        "    'code_switch': {},\n",
        "    'hyperparameter_studies': studies\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6wAyvROcYqo"
      },
      "outputs": [],
      "source": [
        "# Evaluation for all models using AfriCOMETEvaluator\n",
        "for model_name, model in models.items():\n",
        "    tokenizer = tokenizers[model_name]\n",
        "    evaluator = evaluators[model_name]  # Assuming all evaluators are AfriCOMETEvaluator\n",
        "\n",
        "    logger.info(f\"Evaluating model: {model_name}\")\n",
        "\n",
        "    try:\n",
        "        # Assuming all evaluators are AfriCOMETEvaluator\n",
        "        flores_data = data_loader.load_flores_200_benchmark()\n",
        "        if flores_data:\n",
        "            results['translation'][model_name] = evaluate_translation(model, tokenizer, flores_data, evaluator)\n",
        "        else:\n",
        "            logger.warning(f\"FLORES data not available for model: {model_name}\")\n",
        "\n",
        "        # Zero-shot and code-switch evaluations\n",
        "        zero_shot_data = datasets[model_name]['benchmark'][datasets[model_name]['benchmark']['split'] == 'zero_shot']\n",
        "        if not zero_shot_data.empty:\n",
        "            results['zero_shot'][model_name] = evaluate_zero_shot(model, tokenizer, zero_shot_data, evaluator)\n",
        "        else:\n",
        "            logger.info(f\"No zero-shot data for model: {model_name}\")\n",
        "\n",
        "        code_switch_data = datasets[model_name]['benchmark'][datasets[model_name]['benchmark']['split'] == 'code_switch']\n",
        "        if not code_switch_data.empty:\n",
        "            results['code_switch'][model_name] = evaluate_code_switch(model, tokenizer, code_switch_data, evaluator)\n",
        "        else:\n",
        "            logger.info(f\"No code-switch data for model: {model_name}\")\n",
        "\n",
        "        logger.info(f\"Completed evaluation for {model_name}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during evaluation of {model_name}: {str(e)}\")\n",
        "        continue  # Move to the next model if there's an error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grL4kUwycYqo"
      },
      "outputs": [],
      "source": [
        "# Summarize and log results\n",
        "results_summary = summarize_results(results, config)\n",
        "plot_results(results, config)\n",
        "log_results_to_mlflow(results, config, best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPqnfkeqcYqo"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Evaluation complete!\")\n",
        "print(\"Evaluation completed successfully. Results and visualizations have been saved and logged to MLflow.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM8c-ngmcYqo"
      },
      "outputs": [],
      "source": [
        "# Display results and visualizations\n",
        "display(Image(filename=f\"{config['model']['output_dir']}/overall_performance_heatmap.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffF9P4fdcYqo"
      },
      "outputs": [],
      "source": [
        "# Display hyperparameter optimization results\n",
        "for model_name in config['model']['names']:\n",
        "    print(f\"\\nHyperparameter Optimization Results for {model_name}:\")\n",
        "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\"))\n",
        "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\"))\n",
        "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\"))\n",
        "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JAPJNPUcYqp"
      },
      "outputs": [],
      "source": [
        "print_results_summary(results_summary, best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTd_GPo9cYqp"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluation notebook execution complete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ffbcde28fc54465aab6030092cf2863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a9b99d6f4014403ab2e7566bb4d0be8",
              "IPY_MODEL_c32f031f80b646e488a398f28ac3ebb6",
              "IPY_MODEL_103e151c9aae48208668d50fa6d97477"
            ],
            "layout": "IPY_MODEL_aeb1c770c6224c6ba8f03cb69a59aafd"
          }
        },
        "8a9b99d6f4014403ab2e7566bb4d0be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff8f57d1fa244a8bbae11ae2cb3a4569",
            "placeholder": "​",
            "style": "IPY_MODEL_f0eb8151baf3495b8a8a28f0a3695361",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c32f031f80b646e488a398f28ac3ebb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47772b8afbcb4731b00c2d991c694173",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_131abe7378b8493c8c87c306b50cf519",
            "value": 2
          }
        },
        "103e151c9aae48208668d50fa6d97477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4671f7a5d767488eb193b16bb210a98d",
            "placeholder": "​",
            "style": "IPY_MODEL_bc0465df0bcd4a93bbba14200c3d6c07",
            "value": " 2/2 [00:03&lt;00:00,  1.72s/it]"
          }
        },
        "aeb1c770c6224c6ba8f03cb69a59aafd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff8f57d1fa244a8bbae11ae2cb3a4569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0eb8151baf3495b8a8a28f0a3695361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47772b8afbcb4731b00c2d991c694173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131abe7378b8493c8c87c306b50cf519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4671f7a5d767488eb193b16bb210a98d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0465df0bcd4a93bbba14200c3d6c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b536237d86144007813bc468924e69c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8f39ddd14c7483a8db8f36a6401b88e",
              "IPY_MODEL_38f4da2609cc4552a519673b9f6df74b",
              "IPY_MODEL_3cb01ef6b44c412e9594f947b1e3f898"
            ],
            "layout": "IPY_MODEL_0e268cecf57946769cb3155ec3fdda15"
          }
        },
        "a8f39ddd14c7483a8db8f36a6401b88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_728985bdef554b6a9689fc9d3598201f",
            "placeholder": "​",
            "style": "IPY_MODEL_0a08301ea0eb48879dd1e290fe466779",
            "value": "Fetching 4 files: 100%"
          }
        },
        "38f4da2609cc4552a519673b9f6df74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d70a8ad4a304a10b9513fa99a01a3d6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_797f5acbe2b649cea08f725a352845cd",
            "value": 4
          }
        },
        "3cb01ef6b44c412e9594f947b1e3f898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6012d11d64242daace423b394c9403a",
            "placeholder": "​",
            "style": "IPY_MODEL_90240de1d0f54093a9af2324fdb9d638",
            "value": " 4/4 [00:00&lt;00:00, 335.30it/s]"
          }
        },
        "0e268cecf57946769cb3155ec3fdda15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "728985bdef554b6a9689fc9d3598201f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a08301ea0eb48879dd1e290fe466779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d70a8ad4a304a10b9513fa99a01a3d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "797f5acbe2b649cea08f725a352845cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6012d11d64242daace423b394c9403a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90240de1d0f54093a9af2324fdb9d638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57c7755647954b9ebb3b72131ad6d6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c0d7c71f75d4143896de9632b4afd55",
              "IPY_MODEL_bb22f20e140f421f92e58a8e7eb73488",
              "IPY_MODEL_5903617761f04264a490fae0ee4f1f46"
            ],
            "layout": "IPY_MODEL_debeb46bd5364e86b853c9acbfd456c9"
          }
        },
        "4c0d7c71f75d4143896de9632b4afd55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a15ba3d802484a8fcde44ac9bb1f30",
            "placeholder": "​",
            "style": "IPY_MODEL_6df39642146b4a21a82ab97b427a3c97",
            "value": "Fetching 4 files: 100%"
          }
        },
        "bb22f20e140f421f92e58a8e7eb73488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d1942245d84366a936e1b49a9f7d58",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ceb48ebffc8497588dcf03179500f7c",
            "value": 4
          }
        },
        "5903617761f04264a490fae0ee4f1f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_649121c453df48b086c7d76540e27a34",
            "placeholder": "​",
            "style": "IPY_MODEL_b7c1537c238a448ba52d32d8efa4e44f",
            "value": " 4/4 [00:00&lt;00:00, 197.18it/s]"
          }
        },
        "debeb46bd5364e86b853c9acbfd456c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a15ba3d802484a8fcde44ac9bb1f30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df39642146b4a21a82ab97b427a3c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21d1942245d84366a936e1b49a9f7d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ceb48ebffc8497588dcf03179500f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "649121c453df48b086c7d76540e27a34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c1537c238a448ba52d32d8efa4e44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}