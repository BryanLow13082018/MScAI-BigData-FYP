{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow --ignore-installed embedchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna-integration lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers datasets peft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 85.06 GB\n",
      "Current GPU memory usage: 59.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os\n",
    "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'  # Suppress Git warnings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Enable CUDA launch blocking for debugging\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"  # Enable CUDA device-side assertions\n",
    "os.environ['MLFLOW_FLATTEN_PARAMS'] = 'true' # Flatten parameters parameters for logging\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress info messages\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN custom operations\n",
    "\n",
    "import sys\n",
    "sys.path.append('../py')  # Add the parent directory to the Python path\n",
    "import gc\n",
    "import torch\n",
    "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import mlflow\n",
    "import optuna\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from IPython.display import Image, display\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from packaging import version\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "from dataset_loader import DatasetLoader\n",
    "from utils import get_model, set_seed, load_config, get_device, CustomDataset\n",
    "from models.llama2_decoder import Llama2Decoder  # Import Llama2Decoder model\n",
    "from models.ernie_m import ErnieM  # Import ErnieM model\n",
    "from evaluators.africomet_evaluator import AfriCOMETEvaluator\n",
    "from classifiers.zeroshot_classifier import ZeroShotClassifier\n",
    "from classifiers.codeswitch_classifier import CodeSwitchClassifier\n",
    "from trainers.encoder_decoder_trainer import EncoderDecoderTrainer\n",
    "from trainers.combined_encoder_decoder_trainer import CombinedEncoderDecoderTrainer\n",
    "from hyperparameter_analysis import (plot_hyperparameter_importance, plot_study_optimization_history,\n",
    "                                     plot_parallel_coordinate, analyze_hyperparameter_sensitivity,\n",
    "                                     plot_sensitivity_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(config):\n",
    "    \"\"\"\n",
    "    Set up logging configuration based on the provided config.\n",
    "    \n",
    "    This function initializes the logging system with the specified log level,\n",
    "    format, and output file from the configuration.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing logging configuration.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger object.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, config['logging']['log_level']),\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        filename=config['logging']['log_file']\n",
    "    )\n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clear unused memory to prevent out-of-memory errors.\n",
    "    \n",
    "    This function uses Python's garbage collector and PyTorch's CUDA memory \n",
    "    cache clearing (if available) to free up memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    logging.info(f\"Cleared GPU memory. Current allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_loader):\n",
    "    \"\"\"\n",
    "    Load datasets using the provided DatasetLoader object.\n",
    "    \n",
    "    This function attempts to load all datasets specified in the configuration\n",
    "    using the DatasetLoader. It includes error handling for common issues.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DatasetLoader): An instance of the DatasetLoader class.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary of loaded datasets, or None if loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Loading and preparing datasets...\")\n",
    "        return data_loader.load_datasets()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading datasets: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_combined(trial, encoder, decoder, encoder_tokenizer, decoder_tokenizer, train_dataset, eval_dataset, evaluator):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization of the combined Afro-XLMR and LLaMA model, including intent recognition and slot filling tasks.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The Optuna trial object used for hyperparameter suggestions.\n",
    "        encoder (torch.nn.Module): The Afro-XLMR encoder model to be trained and evaluated.\n",
    "        decoder (torch.nn.Module): The LLaMA decoder model to be trained and evaluated.\n",
    "        encoder_tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the encoder.\n",
    "        decoder_tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the decoder.\n",
    "        train_dataset (Dataset): The dataset used for training the model.\n",
    "        eval_dataset (Dataset): The dataset used for evaluating the model.\n",
    "        evaluator (Evaluator): The evaluator object used to compute evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation metric to be minimized (lower is better).\n",
    "    \"\"\"\n",
    "    # Initialize Accelerator\n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(mixed_precision='no', kwargs_handlers=[ddp_kwargs])\n",
    "    \n",
    "    # Hyperparameter suggestions based on config\n",
    "    try:\n",
    "        hyperparams = config.get('hyperparameters', {})\n",
    "        \n",
    "        # Learning rates\n",
    "        lr_min = float(hyperparams.get('learning_rate_min', 1e-6))\n",
    "        lr_max = float(hyperparams.get('learning_rate_max', 1e-4))\n",
    "        \n",
    "        if lr_min >= lr_max:\n",
    "            logging.error(f\"Invalid learning rate range: min ({lr_min}) must be less than max ({lr_max})\")\n",
    "            return float('inf')\n",
    "        \n",
    "        encoder_lr = trial.suggest_float('encoder_lr', lr_min, lr_max, log=True)\n",
    "        decoder_lr = trial.suggest_float('decoder_lr', lr_min, lr_max, log=True)\n",
    "        \n",
    "        # Number of training epochs\n",
    "        num_train_epochs = trial.suggest_int('num_train_epochs', \n",
    "                                             int(hyperparams.get('num_train_epochs_min', 1)), \n",
    "                                             int(hyperparams.get('num_train_epochs_max', 3)))\n",
    "        \n",
    "        per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', \n",
    "                                                                hyperparams.get('batch_sizes', [1, 2, 4, 8]))\n",
    "        \n",
    "        weight_decay = trial.suggest_float('weight_decay', \n",
    "                                           float(hyperparams.get('weight_decay_min', 0.01)), \n",
    "                                           float(hyperparams.get('weight_decay_max', 0.1)), \n",
    "                                           log=True)\n",
    "        \n",
    "        warmup_steps = trial.suggest_int('warmup_steps', \n",
    "                                         int(hyperparams.get('warmup_steps_min', 0)), \n",
    "                                         int(hyperparams.get('warmup_steps_max', 1000)))\n",
    "        \n",
    "        gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', \n",
    "                                                                hyperparams.get('gradient_accumulation_steps', [1, 2, 4, 8]))\n",
    "        \n",
    "        # Add hyperparameters for intent and slot tasks\n",
    "        intent_loss_weight = trial.suggest_float('intent_loss_weight', 0.1, 1.0)\n",
    "        slot_loss_weight = trial.suggest_float('slot_loss_weight', 0.1, 1.0)\n",
    "\n",
    "        # Create trial config\n",
    "        trial_config = {\n",
    "            \"encoder_lr\": encoder_lr,\n",
    "            \"decoder_lr\": decoder_lr,\n",
    "            \"num_train_epochs\": num_train_epochs,\n",
    "            \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "            \"per_device_eval_batch_size\": per_device_train_batch_size,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "            \"fp16\": False,\n",
    "            \"evaluation_strategy\": \"steps\",\n",
    "            \"eval_steps\": 200,\n",
    "            \"save_steps\": 200,\n",
    "            \"logging_steps\": 50,\n",
    "            \"max_grad_norm\": 1.0,\n",
    "            \"output_dir\": config['model']['output_dir'],\n",
    "            \"seed\": config['seed'],\n",
    "            \"device\": config['device'],\n",
    "            \"cache_dir\": config['cache']['dir'],\n",
    "            \"gradient_checkpointing\": True,  \n",
    "            \"intent_loss_weight\": intent_loss_weight, \n",
    "            \"slot_loss_weight\": slot_loss_weight,  \n",
    "            \"num_intent_classes\": 50,  \n",
    "            \"num_slot_classes\": 100  \n",
    "        }\n",
    "    \n",
    "        logging.info(f\"Trial {trial.number}: Starting with hyperparameters: {trial_config}\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error converting config values to numeric types: {str(e)}\")\n",
    "        return float('inf')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in hyperparameter suggestion: {str(e)}\")\n",
    "        return float('inf')\n",
    "\n",
    "    # Enable gradient checkpointing for both encoder and decoder\n",
    "    encoder.gradient_checkpointing_enable()\n",
    "    decoder.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = CombinedEncoderDecoderTrainer(encoder, decoder, encoder_tokenizer, decoder_tokenizer, config=trial_config, accelerator=accelerator, batch_size=64)\n",
    "\n",
    "    # Prepare models and optimizers\n",
    "    trainer.encoder, trainer.decoder, trainer.projection, trainer.encoder_optimizer, trainer.decoder_optimizer = accelerator.prepare(\n",
    "        trainer.encoder, trainer.decoder, trainer.projection, trainer.encoder_optimizer, trainer.decoder_optimizer\n",
    "    )\n",
    "\n",
    "    # Log dataset sizes\n",
    "    logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    logging.info(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "    # Validate datasets\n",
    "    trainer.validate_dataset(train_dataset, \"Training\")\n",
    "    trainer.validate_dataset(eval_dataset, \"Evaluation\")\n",
    "\n",
    "    best_metric = float('inf')  # Initialize for early stopping\n",
    "    patience_counter = 0\n",
    "    patience_threshold = trainer.patience  # Use the patience from the trainer configuration\n",
    "\n",
    "    # Train the combined model\n",
    "    for epoch in range(num_train_epochs):\n",
    "        logging.info(f\"Starting epoch {epoch + 1}/{num_train_epochs}\")\n",
    "        train_result = trainer.train(train_dataset, eval_dataset)\n",
    "\n",
    "        # Clear cache after training\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Evaluate the model after each epoch\n",
    "        try:\n",
    "            # Use the evaluate() method here\n",
    "            eval_metrics = trainer.evaluate(eval_dataset)\n",
    "            \n",
    "            # Generate translations for FLORES evaluation\n",
    "            generated_results = trainer.generate_batch(eval_dataset)\n",
    "            translated_texts, _, _ = zip(*generated_results)  # We only need translated texts here\n",
    "\n",
    "            # Extract source texts and reference texts for FLORES evaluation\n",
    "            source_texts = [encoder_tokenizer.decode(eval_dataset[i]['input_ids'], skip_special_tokens=True) for i in range(len(eval_dataset))]\n",
    "            reference_texts = [encoder_tokenizer.decode(eval_dataset[i]['labels'], skip_special_tokens=True) for i in range(len(eval_dataset))]\n",
    "\n",
    "            # Evaluate translation with FLORES\n",
    "            print(\"Evaluating translation results with Africomet\")\n",
    "            translation_results = evaluator.evaluate(source_texts, translated_texts, reference_texts)\n",
    "            print(\"Complete evaluating translation results\")\n",
    "\n",
    "            # Combine all evaluation results\n",
    "            eval_results = {\n",
    "                'translation_score': translation_results.get('average_score', 0),\n",
    "                'intent_accuracy': eval_metrics['intent_accuracy'],\n",
    "                'slot_f1': eval_metrics['slot_f1'],\n",
    "                'eval_loss': eval_metrics['eval_loss']\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation: {str(e)}\")\n",
    "            logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return float('inf')\n",
    "        \n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_train_epochs} Evaluation results: {eval_results}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        current_metric = eval_results['eval_loss']  # Use eval_loss for early stopping\n",
    "        if current_metric < best_metric:\n",
    "            best_metric = current_metric\n",
    "            patience_counter = 0\n",
    "            # Save the best model state\n",
    "            best_model_state = {\n",
    "                'encoder': trainer.encoder.state_dict(),\n",
    "                'decoder': trainer.decoder.state_dict(),\n",
    "                'projection': trainer.projection.state_dict()\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            logging.info(f\"Early stopping patience: {patience_counter}/{patience_threshold}\")\n",
    "\n",
    "        if patience_counter >= patience_threshold:\n",
    "            logging.info(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state if it exists\n",
    "    if 'best_model_state' in locals():\n",
    "        trainer.encoder.load_state_dict(best_model_state['encoder'])\n",
    "        trainer.decoder.load_state_dict(best_model_state['decoder'])\n",
    "        trainer.projection.load_state_dict(best_model_state['projection'])\n",
    "        logging.info(\"Loaded best model state\")\n",
    "\n",
    "    # Set trial user attributes for logging\n",
    "    trial.set_user_attr('intent_accuracy', eval_results['intent_accuracy'])\n",
    "    trial.set_user_attr('slot_f1', eval_results['slot_f1'])\n",
    "\n",
    "    # Return the combined metric for optimization (lower is better)\n",
    "    return eval_results['translation_score'] - (eval_results['intent_accuracy'] + eval_results['slot_f1']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combined_optimization(encoder, decoder, encoder_tokenizer, decoder_tokenizer, datasets, config, evaluator):\n",
    "    \"\"\"\n",
    "    Run hyperparameter optimization for the combined Afro-XLMR and LLaMA model with improved efficiency, including intent recognition and slot filling tasks.\n",
    "    \"\"\"\n",
    "    mlflow.end_run()  # End any existing runs\n",
    "    \n",
    "    logging.info(\"Starting hyperparameter optimization for combined Afro-XLMR and LLaMA (including intent and slot tasks)\")\n",
    "    log_gpu_memory(\"Before optimization\")\n",
    "\n",
    "    # Enable gradient checkpointing for both models\n",
    "    encoder.gradient_checkpointing_enable()\n",
    "    decoder.gradient_checkpointing_enable()\n",
    "\n",
    "    # Check PyTorch version\n",
    "    pytorch_version = version.parse(torch.__version__)\n",
    "    logger.info(f\"PyTorch version: {pytorch_version}\")\n",
    "    \n",
    "    # Prepare the objective function with fixed arguments and memory management\n",
    "    def memory_managed_objective(trial):\n",
    "        clear_memory()\n",
    "        result = objective_combined(\n",
    "            trial, encoder, decoder, encoder_tokenizer, decoder_tokenizer,\n",
    "            datasets['train'], datasets['eval'], evaluator\n",
    "        )\n",
    "        clear_memory()\n",
    "        return result\n",
    "\n",
    "    total_delay = 0\n",
    "    n_trials = config['hyperparameters']['n_trials']\n",
    "    \n",
    "    try:\n",
    "        # Set up MLflow\n",
    "        experiment_name = \"combined_optimization\"\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        except mlflow.exceptions.MlflowException:\n",
    "            experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "        \n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        with mlflow.start_run(run_name=\"optimization_combined_with_intent_slot\"):\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "\n",
    "            for trial_num in range(n_trials):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                study.optimize(\n",
    "                    memory_managed_objective,\n",
    "                    n_trials=n_trials, \n",
    "                    timeout=3600,\n",
    "                    catch=(Exception,),\n",
    "                    n_jobs=1\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                trial_time = end_time - start_time\n",
    "                \n",
    "                if trial_num < n_trials - 1:  # Don't measure delay after the last trial\n",
    "                    delay_start = time.time()\n",
    "                    clear_memory()\n",
    "                    delay_end = time.time()\n",
    "                    delay = delay_end - delay_start\n",
    "                    total_delay += delay\n",
    "                    logging.info(f\"Delay after trial {trial_num + 1}: {delay:.2f} seconds\")\n",
    "                \n",
    "                logging.info(f\"Trial {trial_num + 1} completed in {trial_time:.2f} seconds\")\n",
    "                log_gpu_memory(f\"After trial {trial_num + 1}\")\n",
    "\n",
    "                # Log intermediate results\n",
    "                if study.trials[-1].state == optuna.trial.TrialState.COMPLETE:\n",
    "                    last_trial = study.trials[-1]\n",
    "                    logging.info(f\"Trial {trial_num + 1} results:\")\n",
    "                    logging.info(f\"  Translation score: {last_trial.value:.4f}\")\n",
    "                    \n",
    "                    intent_accuracy = last_trial.user_attrs.get('intent_accuracy', 'N/A')\n",
    "                    slot_f1 = last_trial.user_attrs.get('slot_f1', 'N/A')\n",
    "                    \n",
    "                    logging.info(f\"  Intent accuracy: {intent_accuracy if isinstance(intent_accuracy, str) else f'{intent_accuracy:.4f}'}\")\n",
    "                    logging.info(f\"  Slot F1 score: {slot_f1 if isinstance(slot_f1, str) else f'{slot_f1:.4f}'}\")\n",
    "        \n",
    "            if study.best_trial:\n",
    "                log_best_params(study)\n",
    "                \n",
    "                # Log best results for all tasks\n",
    "                logging.info(\"Best trial results:\")\n",
    "                logging.info(f\"  Translation score: {study.best_trial.value:.4f}\")\n",
    "                \n",
    "                best_intent_accuracy = study.best_trial.user_attrs.get('intent_accuracy', 'N/A')\n",
    "                best_slot_f1 = study.best_trial.user_attrs.get('slot_f1', 'N/A')\n",
    "                \n",
    "                logging.info(f\"  Intent accuracy: {best_intent_accuracy if isinstance(best_intent_accuracy, str) else f'{best_intent_accuracy:.4f}'}\")\n",
    "                logging.info(f\"  Slot F1 score: {best_slot_f1 if isinstance(best_slot_f1, str) else f'{best_slot_f1:.4f}'}\")\n",
    "            else:\n",
    "                logging.warning(\"No completed trials found.\")\n",
    "                \n",
    "    except optuna.exceptions.OptunaError as e:\n",
    "        logging.error(f\"Optuna error during hyperparameter optimization: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during hyperparameter optimization: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")\n",
    "        return None\n",
    "    finally:\n",
    "        avg_delay = total_delay / (n_trials - 1) if n_trials > 1 else 0\n",
    "        logging.info(f\"Average delay between trials: {avg_delay:.2f} seconds\")\n",
    "        clear_memory()\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_combined_optimization(encoder, decoder, encoder_tokenizer, decoder_tokenizer, datasets, config, evaluator):\n",
    "#     \"\"\"\n",
    "#     Run a single trial with a small batch for testing purposes.\n",
    "#     \"\"\"\n",
    "#     mlflow.end_run()  # End any existing runs\n",
    "    \n",
    "#     logging.info(\"Starting single trial test for combined Afro-XLMR and LLaMA (including intent and slot tasks)\")\n",
    "#     log_gpu_memory(\"Before optimization\")\n",
    "\n",
    "#     # Enable gradient checkpointing for both models\n",
    "#     encoder.gradient_checkpointing_enable()\n",
    "#     decoder.gradient_checkpointing_enable()\n",
    "\n",
    "#     # Prepare the objective function with fixed arguments and memory management\n",
    "#     def memory_managed_objective(trial):\n",
    "#         clear_memory()\n",
    "#         result = objective_combined(\n",
    "#             trial, encoder, decoder, encoder_tokenizer, decoder_tokenizer,\n",
    "#             datasets['train'], datasets['eval'], evaluator\n",
    "#         )\n",
    "#         clear_memory()\n",
    "#         return result\n",
    "\n",
    "#     try:\n",
    "#         # Set up MLflow\n",
    "#         experiment_name = \"combined_optimization_test\"\n",
    "#         try:\n",
    "#             experiment_id = mlflow.create_experiment(experiment_name)\n",
    "#         except mlflow.exceptions.MlflowException:\n",
    "#             experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "        \n",
    "#         mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "#         with mlflow.start_run(run_name=\"single_trial_test\"):\n",
    "#             study = optuna.create_study(direction='minimize')\n",
    "\n",
    "#             # Run a single trial\n",
    "#             study.optimize(\n",
    "#                 memory_managed_objective,\n",
    "#                 n_trials=1,\n",
    "#                 timeout=3600,\n",
    "#                 catch=(Exception,),\n",
    "#                 n_jobs=1\n",
    "#             )\n",
    "\n",
    "#             if study.trials[-1].state == optuna.trial.TrialState.COMPLETE:\n",
    "#                 last_trial = study.trials[-1]\n",
    "#                 logging.info(\"Trial results:\")\n",
    "#                 logging.info(f\"  Translation score: {last_trial.value:.4f}\")\n",
    "#                 intent_accuracy = last_trial.user_attrs.get('intent_accuracy', 'N/A')\n",
    "#                 slot_f1 = last_trial.user_attrs.get('slot_f1', 'N/A')\n",
    "#                 logging.info(f\"  Intent accuracy: {intent_accuracy if isinstance(intent_accuracy, str) else f'{intent_accuracy:.4f}'}\")\n",
    "#                 logging.info(f\"  Slot F1 score: {slot_f1 if isinstance(slot_f1, str) else f'{slot_f1:.4f}'}\")\n",
    "#             else:\n",
    "#                 logging.warning(\"Trial was not completed successfully.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error during single trial test: {str(e)}\")\n",
    "#         logging.exception(\"Exception details:\")\n",
    "#         return None\n",
    "#     finally:\n",
    "#         clear_memory()\n",
    "    \n",
    "#     return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gpu_memory(stage):\n",
    "    \"\"\"Log GPU memory information.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        current_device = torch.cuda.current_device()\n",
    "        total_memory = torch.cuda.get_device_properties(current_device).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(current_device)\n",
    "        reserved_memory = torch.cuda.memory_reserved(current_device)\n",
    "        available_memory = total_memory - allocated_memory - reserved_memory\n",
    "        logging.info(f\"{stage} - GPU Memory (GB): \"\n",
    "                    f\"Total: {total_memory / 1e9:.2f}, \"\n",
    "                    f\"Allocated: {allocated_memory / 1e9:.2f}, \"\n",
    "                    f\"Reserved: {reserved_memory / 1e9:.2f}, \"\n",
    "                    f\"Available: {available_memory / 1e9:.2f}\")\n",
    "    else:\n",
    "        logging.info(\"CUDA is not available. Cannot log GPU memory.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_best_params(study):\n",
    "    \"\"\"Log best parameters from the study.\"\"\"\n",
    "    best_params = study.best_params\n",
    "    for param, value in best_params.items():\n",
    "        mlflow.log_param(f\"best_{param}\", value)\n",
    "    mlflow.log_metric(\"best_score\", study.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_name, study, config):\n",
    "    \"\"\"Save the optimization results to disk.\"\"\"\n",
    "    output_dir = os.path.join(config['model']['output_dir'], 'optimization_results')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    result = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'best_trial': study.best_trial.number\n",
    "    }\n",
    "    with open(os.path.join(output_dir, f\"{model_name}_optimization_results.json\"), 'w') as f:\n",
    "        json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_trial_info(trial):\n",
    "    \"\"\"Print detailed information about the best trial.\"\"\"\n",
    "    print(\"\\nBest Trial Information:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    print(f\"  Trial number: {trial.number}\")\n",
    "    print(f\"  DateTime start: {trial.datetime_start}\")\n",
    "    print(f\"  DateTime complete: {trial.datetime_complete}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(model, tokenizer, datasets, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluate translation performance using the FLORES-200 dataset structure.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The translation model to evaluate.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        datasets (Dict[str, Any]): The dictionary containing all datasets.\n",
    "        evaluator (AfriCOMETEvaluator): The evaluator to use.\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation results.\n",
    "    \"\"\"\n",
    "    results = {'translations': {}}\n",
    "    target_languages = ['swh', 'kin', 'lug']  # ISO codes for Swahili, Kinyarwanda, and Luganda\n",
    "    english_code = 'eng'\n",
    "    \n",
    "    flores_dataset = datasets\n",
    "    \n",
    "    # Log dataset information\n",
    "    for split, df in flores_dataset.items():\n",
    "        logging.info(f\"FLORES-200 '{split}' dataset shape: {df.shape}\")\n",
    "        logging.info(f\"Source languages in '{split}': {df['src_lang'].unique()}\")\n",
    "        logging.info(f\"Target languages in '{split}': {df['tgt_lang'].unique()}\")\n",
    "    \n",
    "    for split, df in flores_dataset.items():\n",
    "        results['translations'][split] = {}\n",
    "        for lang in target_languages:\n",
    "            try:\n",
    "                eng_to_lang = df[(df['src_lang'] == english_code) & (df['tgt_lang'] == lang)]\n",
    "                \n",
    "                if eng_to_lang.empty:\n",
    "                    logging.warning(f\"No data found for {english_code} to {lang} translation in '{split}'. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                eng_texts = eng_to_lang['src_text'].tolist()\n",
    "                lang_texts = eng_to_lang['tgt_text'].tolist()\n",
    "                \n",
    "                logging.info(f\"Translating {len(eng_texts)} sentences from {english_code} to {lang} in '{split}'\")\n",
    "                \n",
    "                inputs = tokenizer(eng_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    generated = model.generate(**inputs, max_length=128)\n",
    "                translations = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "                \n",
    "                scores = evaluator.evaluate(eng_texts, translations, lang_texts)\n",
    "                results['translations'][split][f'{english_code}_to_{lang}'] = scores\n",
    "                \n",
    "                logging.info(f\"Translation scores for {english_code} to {lang} in '{split}': {scores}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during translation evaluation for language {lang} in '{split}': {str(e)}\")\n",
    "                results['translations'][split][f'{english_code}_to_{lang}'] = {'error': str(e)}\n",
    "    \n",
    "    # Calculate average score across all splits and language pairs\n",
    "    all_scores = [score['average_score'] for split_scores in results['translations'].values()\n",
    "                  for score in split_scores.values() \n",
    "                  if isinstance(score, dict) and 'average_score' in score]\n",
    "    \n",
    "    if all_scores:\n",
    "        results['average_score'] = np.mean(all_scores)\n",
    "        logging.info(f\"Overall average translation score: {results['average_score']}\")\n",
    "    else:\n",
    "        results['average_score'] = np.nan\n",
    "        logging.warning(\"No valid scores for translations\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zero_shot(zero_shot_classifier):\n",
    "    \"\"\"\n",
    "    Evaluate the model's zero-shot classification performance.\n",
    "    \n",
    "    Args:\n",
    "        datasets (Dict[str, Any]): The dictionary of datasets from DatasetLoader.\n",
    "        zero_shot_classifier (object): The zero-shot classifier object.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics.\n",
    "    \"\"\"\n",
    "    if 'experimental' not in datasets or 'zero_shot' not in datasets['experimental']:\n",
    "        raise ValueError(\"Zero-shot dataset not found in the provided datasets.\")\n",
    "    \n",
    "    zero_shot_data = datasets['experimental']['zero_shot']\n",
    "    \n",
    "    if 'text' not in zero_shot_data.columns or 'intent_label' not in zero_shot_data.columns or 'slot_labels' not in zero_shot_data.columns:\n",
    "        raise ValueError(\"Zero-shot data must contain 'text', 'intent_label', and 'slot_labels' columns.\")\n",
    "\n",
    "    texts = zero_shot_data['text'].tolist()\n",
    "    true_intents = zero_shot_data['intent_label'].tolist()\n",
    "    true_slots = zero_shot_data['slot_labels'].tolist()\n",
    "\n",
    "    # Intent classification\n",
    "    intent_candidate_labels = list(set(true_intents))\n",
    "    intent_results = zero_shot_classifier.classify(texts, intent_candidate_labels)\n",
    "    predicted_intents = [result['labels'][0] for result in intent_results]\n",
    "\n",
    "    # Slot filling (assuming we have a method for this in the zero_shot_classifier)\n",
    "    slot_candidate_labels = list(set([slot for slots in true_slots for slot in slots]))\n",
    "    slot_results = zero_shot_classifier.classify_slots(texts, slot_candidate_labels)\n",
    "    predicted_slots = [result['labels'] for result in slot_results]\n",
    "\n",
    "    # Calculate metrics\n",
    "    intent_accuracy = accuracy_score(true_intents, predicted_intents)\n",
    "    intent_f1 = f1_score(true_intents, predicted_intents, average='weighted')\n",
    "    \n",
    "    # Flatten the slot labels and predictions\n",
    "    true_slots_flat = [slot for slots in true_slots for slot in slots]\n",
    "    pred_slots_flat = [slot for slots in predicted_slots for slot in slots]\n",
    "    slot_f1 = f1_score(true_slots_flat, pred_slots_flat, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'intent_accuracy': intent_accuracy,\n",
    "        'intent_f1': intent_f1,\n",
    "        'slot_f1': slot_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_code_switch(code_switch_classifier):\n",
    "    \"\"\"\n",
    "    Evaluate the model's code-switch classification performance.\n",
    "    \n",
    "    Args:\n",
    "        datasets (Dict[str, Any]): The dictionary of datasets from DatasetLoader.\n",
    "        code_switch_classifier (object): The code-switch classifier object.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    datasets\n",
    "    if 'experimental' not in datasets or 'code_switch' not in datasets['experimental']:\n",
    "        raise ValueError(\"Code-switch dataset not found in the provided datasets.\")\n",
    "    \n",
    "    code_switch_data = datasets['experimental']['code_switch']\n",
    "    \n",
    "    if 'text' not in code_switch_data.columns or 'language' not in code_switch_data.columns or 'intent_label' not in code_switch_data.columns or 'slot_labels' not in code_switch_data.columns:\n",
    "        raise ValueError(\"Code-switch data must contain 'text', 'language', 'intent_label', and 'slot_labels' columns.\")\n",
    "\n",
    "    texts = code_switch_data['text'].tolist()\n",
    "    true_languages = code_switch_data['language'].tolist()\n",
    "    true_intents = code_switch_data['intent_label'].tolist()\n",
    "    true_slots = code_switch_data['slot_labels'].tolist()\n",
    "\n",
    "    # Perform code-switch classification\n",
    "    classification_results = code_switch_classifier.classify(texts)\n",
    "    predicted_languages = [result['predicted_language'] for result in classification_results]\n",
    "    predicted_intents = [result['predicted_intent'] for result in classification_results]\n",
    "    predicted_slots = [result['predicted_slots'] for result in classification_results]\n",
    "\n",
    "    # Calculate metrics\n",
    "    language_accuracy = accuracy_score(true_languages, predicted_languages)\n",
    "    intent_accuracy = accuracy_score(true_intents, predicted_intents)\n",
    "    intent_f1 = f1_score(true_intents, predicted_intents, average='weighted')\n",
    "\n",
    "    # Flatten the slot labels and predictions\n",
    "    true_slots_flat = [slot for slots in true_slots for slot in slots]\n",
    "    pred_slots_flat = [slot for slots in predicted_slots for slot in slots]\n",
    "    slot_f1 = f1_score(true_slots_flat, pred_slots_flat, average='weighted')\n",
    "\n",
    "    # Calculate per-language accuracy\n",
    "    unique_languages = list(set(true_languages))\n",
    "    per_language_accuracy = {}\n",
    "    for lang in unique_languages:\n",
    "        lang_mask = [tl == lang for tl in true_languages]\n",
    "        lang_true = [tl for tl, mask in zip(true_languages, lang_mask) if mask]\n",
    "        lang_pred = [pl for pl, mask in zip(predicted_languages, lang_mask) if mask]\n",
    "        per_language_accuracy[lang] = accuracy_score(lang_true, lang_pred)\n",
    "\n",
    "    # Create confusion matrix for languages\n",
    "    cm = confusion_matrix(true_languages, predicted_languages, labels=unique_languages)\n",
    "\n",
    "    return {\n",
    "        'language_accuracy': language_accuracy,\n",
    "        'intent_accuracy': intent_accuracy,\n",
    "        'intent_f1': intent_f1,\n",
    "        'slot_f1': slot_f1,\n",
    "        'per_language_accuracy': per_language_accuracy,\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results, config):\n",
    "    \"\"\"\n",
    "    Summarize evaluation results.\n",
    "    \n",
    "    This function summarizes the results from various evaluation tasks and saves\n",
    "    them to files.\n",
    "\n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of evaluation results.\n",
    "    \"\"\"\n",
    "    logging.info(\"Summarizing results...\")\n",
    "\n",
    "    summary = {}\n",
    "    for model_name in config['model']['names']:\n",
    "        summary[model_name] = {\n",
    "            'translation': results['translation'].get(model_name),\n",
    "            'intent_recognition': results['intent_recognition'].get(model_name),\n",
    "            'slot_filling': results['slot_filling'].get(model_name),\n",
    "            'zero_shot': results['zero_shot'].get(model_name, {}).get('accuracy') if 'zero_shot' in results else None,\n",
    "            'code_switch': results['code_switch'].get(model_name, {}).get('accuracy') if 'code_switch' in results else None,\n",
    "        }\n",
    "\n",
    "        # Add FLORES results if available\n",
    "        if 'flores' in results['translation'].get(model_name, {}):\n",
    "            summary[model_name]['flores_translation'] = results['translation'][model_name]['flores'].get('average_score')\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    logging.info(\"Evaluation Results Summary:\")\n",
    "    logging.info(summary_df)\n",
    "\n",
    "    # Save results\n",
    "    summary_df.to_csv(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
    "    \n",
    "    with open(f\"{config['model']['output_dir']}/all_results.txt\", 'w') as f:\n",
    "        f.write(str(results))\n",
    "\n",
    "    # Add this line to add summary to results\n",
    "    results['summary'] = summary\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, config):\n",
    "    \"\"\"\n",
    "    Plot evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "    \"\"\"\n",
    "    for model_name in config['model']['names']:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        if model_name in results['translation']:\n",
    "            plt.bar(results['translation'][model_name].keys(), results['translation'][model_name].values())\n",
    "            plt.title(f\"Translation Scores - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_translation_scores.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        if model_name in results['generation']:\n",
    "            plt.hist(results['generation'][model_name]['perplexities'], bins=20)\n",
    "            plt.title(f\"Perplexity Distribution - {model_name}\")\n",
    "            plt.xlabel(\"Perplexity\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_perplexity_distribution.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        # Plot intent recognition confusion matrix\n",
    "        if model_name in results['intent_recognition']:\n",
    "            sns.heatmap(results['intent_recognition'][model_name]['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f\"Intent Recognition Confusion Matrix - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_intent_confusion_matrix.png\")\n",
    "            plt.close()\n",
    "\n",
    "        # Plot slot filling F1 scores\n",
    "        if model_name in results['slot_filling']:\n",
    "            plt.bar(results['slot_filling'][model_name]['f1_scores'].keys(), results['slot_filling'][model_name]['f1_scores'].values())\n",
    "            plt.title(f\"Slot Filling F1 Scores - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_slot_f1_scores.png\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Plot overall performance comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    performance_data = {\n",
    "        model: {\n",
    "            'Classification': results['classification'].get(model, {}).get('accuracy', 0),\n",
    "            'Translation': results['translation'].get(model, {}).get('average_score', 0),\n",
    "            'Generation': 1 / results['generation'].get(model, {}).get('average_perplexity', 1),  # Inverse of perplexity\n",
    "            'Zero-shot': results['zero_shot'].get(model, {}).get('accuracy', 0),\n",
    "            'Code-switch': results['code_switch'].get(model, {}).get('accuracy', 0),\n",
    "            'Intent Recognition': results['intent_recognition'].get(model, {}).get('accuracy', 0),  # New\n",
    "            'Slot Filling': results['slot_filling'].get(model, {}).get('f1_score', 0)  # New\n",
    "        } for model in config['model']['names']\n",
    "    }\n",
    "    df = pd.DataFrame(performance_data).T\n",
    "    sns.heatmap(df, annot=True, cmap='YlGnBu')\n",
    "    plt.title(\"Model Performance Across Tasks\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_mlflow(results, config, best_params):\n",
    "    \"\"\"\n",
    "    Log results to MLflow.\n",
    "    \n",
    "    This function logs the evaluation results, model parameters, and artifacts to MLflow.\n",
    "\n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters for each model\n",
    "        for model_name, params in best_params.items():\n",
    "            for param, value in params.items():\n",
    "                mlflow.log_param(f\"{model_name}_{param}\", value)\n",
    "        \n",
    "        # Log model information\n",
    "        for model_name in config['model']['names']:\n",
    "            mlflow.log_param(f\"{model_name}_model\", model_name)\n",
    "\n",
    "        # Log metrics\n",
    "        for model_name, metrics in results['summary'].items():\n",
    "            for metric, value in metrics.items():\n",
    "                if value is not None:\n",
    "                    mlflow.log_metric(f\"{model_name}_{metric}\", value)\n",
    "\n",
    "        # Log artifacts\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/all_results.txt\")\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
    "        \n",
    "        # Log hyperparameter optimization plots\n",
    "        for model_name in config['model']['names']:   \n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_intent_confusion_matrix.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_slot_f1_scores.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification': {}, 'translation': {}, 'generation': {}, 'zero_shot': {}, 'code_switch': {}, 'intent_recognition': {}, 'slot_filling': {}, 'hyperparameter_studies': {'combined_study': <optuna.study.study.Study object at 0x7f8f262249d0>}, 'summary': {'afro-xlmr-large': {'translation': None, 'intent_recognition': None, 'slot_filling': None, 'zero_shot': None, 'code_switch': None}, 'meta-llama/Llama-2-7b-hf': {'translation': None, 'intent_recognition': None, 'slot_filling': None, 'zero_shot': None, 'code_switch': None}}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_summary(results_summary, best_params):\n",
    "    \"\"\"\n",
    "    Print a summary of the evaluation results and best hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        results_summary (dict): A dictionary containing summarized results.\n",
    "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
    "    \"\"\"\n",
    "    print(\"\\n===== EVALUATION RESULTS SUMMARY =====\")\n",
    "\n",
    "    if 'classification' in results_summary:\n",
    "        print(\"\\nClassification Results:\")\n",
    "        for dataset, metrics in results_summary['classification'].items():\n",
    "            print(f\"\\n{dataset}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    if 'translation' in results_summary:\n",
    "        print(\"\\nTranslation Results:\")\n",
    "        print(f\"  FLORES-200 Average AfriCOMET Score (A to B): {results_summary['translation']['a_to_b']['average_score']:.4f}\")\n",
    "        print(f\"  FLORES-200 Average AfriCOMET Score (B to A): {results_summary['translation']['b_to_a']['average_score']:.4f}\")\n",
    "\n",
    "    if 'generation' in results_summary:\n",
    "        print(\"\\nGeneration Results:\")\n",
    "        print(f\"  FLORES-200 Average Perplexity: {results_summary['generation']['average_perplexity']:.4f}\")\n",
    "\n",
    "    if 'zero_shot' in results_summary:\n",
    "        print(\"\\nZero-shot Results:\")\n",
    "        print(f\"  Accuracy: {results_summary['zero_shot']['accuracy']:.4f}\")\n",
    "\n",
    "    if 'code_switch' in results_summary:\n",
    "        print(\"\\nCode-switch Results:\")\n",
    "        print(f\"  Accuracy: {results_summary['code_switch']['accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "    print(\"\\n======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = load_config('../py/config.yaml')\n",
    "# Set the device dynamically based on availability\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'  # Update device setting\n",
    "auth_token = config.get(\"auth_token\")\n",
    "cache_dir = os.path.abspath(config['cache']['dir'])\n",
    "logger = setup_logging(config)\n",
    "set_seed(config['seed'])\n",
    "device = get_device(config['device'])\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the cache directory exists\n",
    "os.makedirs(cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging is configured.\n",
      "Masakhane dir: /workspace/Msc-FYP/Datasets/Masakhane\n",
      "FLORES-200 dir: /workspace/Msc-FYP/Datasets/FLORES-200\n",
      "Experiments dir: /workspace/Msc-FYP/Datasets/Experiments\n",
      "OntoNotes dir: /workspace/Msc-FYP/Datasets/OntoNotes_5.0\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare datasets\n",
    "data_loader = DatasetLoader(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for Masakhane datasets in: /workspace/Msc-FYP/Datasets/Masakhane/annotation_quality_corpus\n",
      "Attempting to load swahili dataset from: /workspace/Msc-FYP/Datasets/Masakhane/annotation_quality_corpus/swahili.txt\n",
      "Loaded 3006 samples from /workspace/Msc-FYP/Datasets/Masakhane/annotation_quality_corpus/swahili.txt\n",
      "/workspace/Msc-FYP/ipynb/../py/dataset_loader.py:429: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[col].fillna(default, inplace=True)\n",
      "Successfully loaded swahili dataset with 3006 samples\n",
      "Attempting to load kinyarwanda dataset from: /workspace/Msc-FYP/Datasets/Masakhane/annotation_quality_corpus/kinyarwanda.txt\n",
      "Loaded 3015 samples from /workspace/Msc-FYP/Datasets/Masakhane/annotation_quality_corpus/kinyarwanda.txt\n",
      "/workspace/Msc-FYP/ipynb/../py/dataset_loader.py:429: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[col].fillna(default, inplace=True)\n",
      "Successfully loaded kinyarwanda dataset with 3015 samples\n",
      "Attempting to load luganda dataset from: /workspace/Msc-FYP/Datasets/Masakhane/annotation_quality_corpus/luganda.txt\n",
      "Loaded 2003 samples from /workspace/Msc-FYP/Datasets/Masakhane/annotation_quality_corpus/luganda.txt\n",
      "/workspace/Msc-FYP/ipynb/../py/dataset_loader.py:429: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataset[col].fillna(default, inplace=True)\n",
      "Successfully loaded luganda dataset with 2003 samples\n",
      "Loaded 3 datasets from Masakhane annotation_quality_corpus: swahili, kinyarwanda, luganda\n",
      "FLORES-200 directory: /workspace/Msc-FYP/Datasets/FLORES-200\n",
      "Checking split directory: /workspace/Msc-FYP/Datasets/FLORES-200/dev\n",
      "Files in dev directory: ['mal_Mlym.dev', 'zul_Latn.dev', 'taq_Tfng.dev', 'mya_Mymr.dev', 'slk_Latn.dev', 'uig_Arab.dev', 'scn_Latn.dev', 'ace_Arab.dev', 'tha_Thai.dev', 'npi_Deva.dev', 'ukr_Cyrl.dev', 'tam_Taml.dev', 'umb_Latn.dev', 'ace_Latn.dev', 'tgl_Latn.dev', 'swe_Latn.dev', 'tsn_Latn.dev', 'war_Latn.dev', 'mos_Latn.dev', 'ydd_Hebr.dev', 'tat_Cyrl.dev', 'tel_Telu.dev', 'tum_Latn.dev', 'tzm_Tfng.dev', 'zsm_Latn.dev', 'zho_Hant.dev', 'sun_Latn.dev', 'tur_Latn.dev', 'uzn_Latn.dev', 'sot_Latn.dev', 'yue_Hant.dev', 'acm_Arab.dev', 'xho_Latn.dev', 'yor_Latn.dev', 'ssw_Latn.dev', 'vie_Latn.dev', 'wol_Latn.dev', 'srd_Latn.dev', 'srp_Cyrl.dev', 'szl_Latn.dev', 'urd_Arab.dev', 'tso_Latn.dev', 'zho_Hans.dev', 'sin_Sinh.dev', 'vec_Latn.dev', 'taq_Latn.dev', 'swh_Latn.dev', 'aeb_Arab.dev', 'acq_Arab.dev', 'twi_Latn.dev', 'tpi_Latn.dev', 'ory_Orya.dev', 'nso_Latn.dev', 'slv_Latn.dev', 'tuk_Latn.dev', 'hin_Deva.dev', 'shn_Mymr.dev', 'tir_Ethi.dev', 'por_Latn.dev', 'tgk_Cyrl.dev', 'bod_Tibt.dev', 'pol_Latn.dev', 'sna_Latn.dev', 'spa_Latn.dev', 'pap_Latn.dev', 'lvs_Latn.dev', 'gaz_Latn.dev', 'snd_Arab.dev', 'som_Latn.dev', 'smo_Latn.dev', 'nya_Latn.dev', 'nus_Latn.dev', 'kat_Geor.dev', 'sat_Olck.dev', 'san_Deva.dev', 'min_Arab.dev', 'pag_Latn.dev', 'kan_Knda.dev', 'pbt_Arab.dev', 'awa_Deva.dev', 'rus_Cyrl.dev', 'sag_Latn.dev', 'ron_Latn.dev', 'run_Latn.dev', 'pes_Arab.dev', 'guj_Gujr.dev', 'dzo_Tibt.dev', 'oci_Latn.dev', 'hne_Deva.dev', 'kas_Deva.dev', 'pan_Guru.dev', 'khm_Khmr.dev', 'prs_Arab.dev', 'quy_Latn.dev', 'min_Latn.dev', 'heb_Hebr.dev', 'bho_Deva.dev', 'lao_Laoo.dev', 'hye_Armn.dev', 'asm_Beng.dev', 'bel_Cyrl.dev', 'mai_Deva.dev', 'ben_Beng.dev', 'mkd_Cyrl.dev', 'plt_Latn.dev', 'mar_Deva.dev', 'kaz_Cyrl.dev', 'ell_Grek.dev', 'knc_Arab.dev', 'mni_Beng.dev', 'lin_Latn.dev', 'mlt_Latn.dev', 'ckb_Arab.dev', 'bul_Cyrl.dev', 'khk_Cyrl.dev', 'als_Latn.dev', 'lij_Latn.dev', 'knc_Latn.dev', 'kir_Cyrl.dev', 'bjn_Arab.dev', 'kas_Arab.dev', 'kab_Latn.dev', 'fon_Latn.dev', 'lit_Latn.dev', 'nld_Latn.dev', 'mag_Deva.dev', 'eus_Latn.dev', 'cjk_Latn.dev', 'fij_Latn.dev', 'bak_Cyrl.dev', 'ita_Latn.dev', 'nob_Latn.dev', 'kik_Latn.dev', 'mri_Latn.dev', 'kin_Latn.dev', 'jpn_Jpan.dev', 'arb_Arab.dev', 'fao_Latn.dev', 'kbp_Latn.dev', 'deu_Latn.dev', 'amh_Ethi.dev', 'fra_Latn.dev', 'fin_Latn.dev', 'nno_Latn.dev', 'kon_Latn.dev', 'kac_Latn.dev', 'kmb_Latn.dev', 'kor_Hang.dev', 'dyu_Latn.dev', 'kmr_Latn.dev', 'ind_Latn.dev', 'lim_Latn.dev', 'cym_Latn.dev', 'luo_Latn.dev', 'kam_Latn.dev', 'isl_Latn.dev', 'lug_Latn.dev', 'gla_Latn.dev', 'ceb_Latn.dev', 'crh_Latn.dev', 'lua_Latn.dev', 'hat_Latn.dev', 'ibo_Latn.dev', 'dan_Latn.dev', 'ltz_Latn.dev', 'bug_Latn.dev', 'glg_Latn.dev', 'hun_Latn.dev', 'ewe_Latn.dev', 'hau_Latn.dev', 'kea_Latn.dev', 'gle_Latn.dev', 'lus_Latn.dev', 'arz_Arab.dev', 'cat_Latn.dev', 'bjn_Latn.dev', 'ast_Latn.dev', 'jav_Latn.dev', 'ars_Arab.dev', 'ilo_Latn.dev', 'ces_Latn.dev', 'fur_Latn.dev', 'bem_Latn.dev', 'ltg_Latn.dev', 'ary_Arab.dev', 'hrv_Latn.dev', 'grn_Latn.dev', 'bam_Latn.dev', 'arb_Latn.dev', 'azj_Latn.dev', 'fuv_Latn.dev', 'apc_Arab.dev', 'epo_Latn.dev', 'est_Latn.dev', 'eng_Latn.dev', 'dik_Latn.dev', 'ayr_Latn.dev', 'lmo_Latn.dev', 'bos_Latn.dev', 'azb_Arab.dev', 'ban_Latn.dev', 'ajp_Arab.dev', 'aka_Latn.dev', 'afr_Latn.dev']\n",
      "Looking for English file: /workspace/Msc-FYP/Datasets/FLORES-200/dev/eng_Latn.dev\n",
      "Processing English file: /workspace/Msc-FYP/Datasets/FLORES-200/dev/eng_Latn.dev\n",
      "Error reading English file /workspace/Msc-FYP/Datasets/FLORES-200/dev/eng_Latn.dev: Error tokenizing data. C error: Expected 2 fields in line 2, saw 4\n",
      "\n",
      "Checking split directory: /workspace/Msc-FYP/Datasets/FLORES-200/devtest\n",
      "Files in devtest directory: ['tpi_Latn.devtest', 'tzm_Tfng.devtest', 'mya_Mymr.devtest', 'zho_Hans.devtest', 'yor_Latn.devtest', 'umb_Latn.devtest', 'sna_Latn.devtest', 'smo_Latn.devtest', 'ben_Beng.devtest', 'pol_Latn.devtest', 'guj_Gujr.devtest', 'aka_Latn.devtest', 'vie_Latn.devtest', 'acq_Arab.devtest', 'scn_Latn.devtest', 'ukr_Cyrl.devtest', 'uig_Arab.devtest', 'tum_Latn.devtest', 'aeb_Arab.devtest', 'ajp_Arab.devtest', 'kan_Knda.devtest', 'ace_Arab.devtest', 'slv_Latn.devtest', 'zsm_Latn.devtest', 'zho_Hant.devtest', 'zul_Latn.devtest', 'sag_Latn.devtest', 'taq_Latn.devtest', 'swh_Latn.devtest', 'acm_Arab.devtest', 'xho_Latn.devtest', 'ydd_Hebr.devtest', 'tel_Telu.devtest', 'tam_Taml.devtest', 'war_Latn.devtest', 'ace_Latn.devtest', 'prs_Arab.devtest', 'tat_Cyrl.devtest', 'rus_Cyrl.devtest', 'ron_Latn.devtest', 'tir_Ethi.devtest', 'afr_Latn.devtest', 'tso_Latn.devtest', 'vec_Latn.devtest', 'uzn_Latn.devtest', 'urd_Arab.devtest', 'ell_Grek.devtest', 'wol_Latn.devtest', 'tha_Thai.devtest', 'khm_Khmr.devtest', 'twi_Latn.devtest', 'tur_Latn.devtest', 'tuk_Latn.devtest', 'tsn_Latn.devtest', 'shn_Mymr.devtest', 'yue_Hant.devtest', 'ssw_Latn.devtest', 'oci_Latn.devtest', 'run_Latn.devtest', 'taq_Tfng.devtest', 'nso_Latn.devtest', 'srp_Cyrl.devtest', 'bod_Tibt.devtest', 'sun_Latn.devtest', 'tgk_Cyrl.devtest', 'szl_Latn.devtest', 'mni_Beng.devtest', 'san_Deva.devtest', 'npi_Deva.devtest', 'snd_Arab.devtest', 'sat_Olck.devtest', 'slk_Latn.devtest', 'sin_Sinh.devtest', 'sot_Latn.devtest', 'gle_Latn.devtest', 'pap_Latn.devtest', 'tgl_Latn.devtest', 'ory_Orya.devtest', 'min_Arab.devtest', 'plt_Latn.devtest', 'kat_Geor.devtest', 'srd_Latn.devtest', 'pbt_Arab.devtest', 'spa_Latn.devtest', 'por_Latn.devtest', 'mag_Deva.devtest', 'ckb_Arab.devtest', 'swe_Latn.devtest', 'mos_Latn.devtest', 'mlt_Latn.devtest', 'mal_Mlym.devtest', 'pag_Latn.devtest', 'pan_Guru.devtest', 'min_Latn.devtest', 'mar_Deva.devtest', 'hin_Deva.devtest', 'som_Latn.devtest', 'pes_Arab.devtest', 'luo_Latn.devtest', 'nya_Latn.devtest', 'gla_Latn.devtest', 'knc_Arab.devtest', 'lim_Latn.devtest', 'lvs_Latn.devtest', 'quy_Latn.devtest', 'dzo_Tibt.devtest', 'ltg_Latn.devtest', 'lus_Latn.devtest', 'ltz_Latn.devtest', 'nus_Latn.devtest', 'nno_Latn.devtest', 'lao_Laoo.devtest', 'mai_Deva.devtest', 'fin_Latn.devtest', 'khk_Cyrl.devtest', 'lug_Latn.devtest', 'bul_Cyrl.devtest', 'bho_Deva.devtest', 'awa_Deva.devtest', 'hye_Armn.devtest', 'asm_Beng.devtest', 'bjn_Arab.devtest', 'nob_Latn.devtest', 'kas_Deva.devtest', 'kor_Hang.devtest', 'lin_Latn.devtest', 'lua_Latn.devtest', 'bak_Cyrl.devtest', 'kbp_Latn.devtest', 'nld_Latn.devtest', 'mri_Latn.devtest', 'eus_Latn.devtest', 'dan_Latn.devtest', 'lij_Latn.devtest', 'kea_Latn.devtest', 'isl_Latn.devtest', 'glg_Latn.devtest', 'bel_Cyrl.devtest', 'hau_Latn.devtest', 'ita_Latn.devtest', 'kaz_Cyrl.devtest', 'mkd_Cyrl.devtest', 'lit_Latn.devtest', 'ilo_Latn.devtest', 'fra_Latn.devtest', 'amh_Ethi.devtest', 'jpn_Jpan.devtest', 'bug_Latn.devtest', 'ces_Latn.devtest', 'bos_Latn.devtest', 'eng_Latn.devtest', 'kac_Latn.devtest', 'kin_Latn.devtest', 'apc_Arab.devtest', 'azj_Latn.devtest', 'fon_Latn.devtest', 'bem_Latn.devtest', 'ibo_Latn.devtest', 'kmr_Latn.devtest', 'ind_Latn.devtest', 'gaz_Latn.devtest', 'est_Latn.devtest', 'hne_Deva.devtest', 'kas_Arab.devtest', 'arz_Arab.devtest', 'ary_Arab.devtest', 'kir_Cyrl.devtest', 'cym_Latn.devtest', 'ars_Arab.devtest', 'lmo_Latn.devtest', 'bam_Latn.devtest', 'azb_Arab.devtest', 'kmb_Latn.devtest', 'dik_Latn.devtest', 'deu_Latn.devtest', 'cjk_Latn.devtest', 'kon_Latn.devtest', 'epo_Latn.devtest', 'jav_Latn.devtest', 'als_Latn.devtest', 'fao_Latn.devtest', 'ewe_Latn.devtest', 'dyu_Latn.devtest', 'knc_Latn.devtest', 'arb_Arab.devtest', 'fij_Latn.devtest', 'hun_Latn.devtest', 'kab_Latn.devtest', 'ceb_Latn.devtest', 'grn_Latn.devtest', 'heb_Hebr.devtest', 'cat_Latn.devtest', 'fur_Latn.devtest', 'crh_Latn.devtest', 'kik_Latn.devtest', 'hrv_Latn.devtest', 'bjn_Latn.devtest', 'fuv_Latn.devtest', 'kam_Latn.devtest', 'hat_Latn.devtest', 'arb_Latn.devtest', 'ban_Latn.devtest', 'ayr_Latn.devtest', 'ast_Latn.devtest']\n",
      "Looking for English file: /workspace/Msc-FYP/Datasets/FLORES-200/devtest/eng_Latn.devtest\n",
      "Processing English file: /workspace/Msc-FYP/Datasets/FLORES-200/devtest/eng_Latn.devtest\n",
      "Error reading English file /workspace/Msc-FYP/Datasets/FLORES-200/devtest/eng_Latn.devtest: Error tokenizing data. C error: Expected 1 fields in line 2, saw 3\n",
      "\n",
      "No FLORES-200 data was loaded\n",
      "FLORES-200 benchmark data is empty or failed to load\n",
      "Split file not found: /workspace/Msc-FYP/Datasets/OntoNotes_5.0/train.txt\n",
      "Split file not found: /workspace/Msc-FYP/Datasets/OntoNotes_5.0/development.txt\n",
      "Split file not found: /workspace/Msc-FYP/Datasets/OntoNotes_5.0/test.txt\n",
      "masakhane is not a DataFrame. Current type: <class 'dict'>\n",
      "experimental is not a DataFrame. Current type: <class 'dict'>\n",
      "ontonotes is not a DataFrame. Current type: <class 'dict'>\n",
      "Loaded datasets: ['masakhane', 'experimental', 'ontonotes']\n",
      "Masakhane swahili dataset shape: (3006, 3)\n",
      "Reduced Masakhane swahili dataset shape: (500, 4)\n",
      "Masakhane kinyarwanda dataset shape: (3015, 3)\n",
      "Reduced Masakhane kinyarwanda dataset shape: (500, 4)\n",
      "Masakhane luganda dataset shape: (2003, 3)\n",
      "Reduced Masakhane luganda dataset shape: (500, 4)\n",
      "FLORES-200 benchmark data not found\n",
      "Final dataset sizes - Train: 1050, Eval: 225, Benchmark: 225\n",
      "Class distribution in train set:\n",
      "  Class O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O: 16\n",
      "  Class O O O O O O O O O B-DATE O B-PER O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O: 11\n",
      "  Class B-PER O O O B-ORG I-ORG O O O O O B-DATE O O O O O B-LOC O: 1\n",
      "  Class O O O O O B-LOC O B-DATE I-DATE O O O O O O O O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class B-PER O B-PER I-PER I-PER O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-DATE O B-DATE O O O O O O O O O B-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-ORG O O O O O B-DATE O: 1\n",
      "  Class O O O B-LOC O B-LOC O O O O O O O O O O O O B-LOC O B-LOC O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O B-PER O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O B-ORG O O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 4\n",
      "  Class B-PER I-PER O O O O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O B-PER O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O B-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 5\n",
      "  Class O O O O O O O O O O O O: 10\n",
      "  Class O O O O O B-PER O O O B-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O B-LOC O O O O O O O O: 1\n",
      "  Class O B-DATE O O B-ORG O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O: 6\n",
      "  Class O O O O O O O O O O O O O O O O O O: 14\n",
      "  Class O O O O O O O O O O O: 14\n",
      "  Class O O O B-LOC O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-PER O O O O O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC I-LOC: 1\n",
      "  Class O O O O O O B-PER I-PER O O B-PER I-PER I-PER O: 1\n",
      "  Class B-PER O O O O O O O O O O B-LOC O O O B-DATE I-DATE O O O O O B-LOC O O O B-ORG O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-ORG I-ORG O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC I-LOC I-LOC O: 3\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 5\n",
      "  Class O O O O O O B-LOC B-ORG O B-DATE O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O: 8\n",
      "  Class O O O O O O B-PER O O O O O O O O O O O O O B-PER I-PER I-PER O O O O B-LOC O O O O O O O: 1\n",
      "  Class O B-ORG I-ORG I-ORG O B-PER I-PER O O B-PER O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC I-LOC B-PER I-PER I-PER O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O B-LOC O O O O O O O: 1\n",
      "  Class O O O O O O O O O: 10\n",
      "  Class O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O B-LOC I-LOC O B-LOC I-LOC O B-LOC I-LOC O O O O O O O O O O: 1\n",
      "  Class O B-ORG I-ORG I-ORG O B-PER I-PER I-PER O O B-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O: 9\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O: 9\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O: 1\n",
      "  Class O O O O O B-PER O B-PER I-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O: 1\n",
      "  Class B-DATE I-DATE I-DATE O B-PER O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O: 1\n",
      "  Class B-PER I-PER B-LOC I-LOC O B-PER O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O O O B-PER I-PER O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O: 14\n",
      "  Class O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class B-PER O O O B-PER O O O O O O O O O O O O B-PER O B-LOC I-LOC O B-LOC O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O B-ORG O O O O O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O B-PER O O O O O O O O O O O O B-PER O O O O O O O B-DATE O O O O O O O B-PER O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O: 13\n",
      "  Class O B-PER I-PER O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER O O O O B-PER O O O O O B-ORG O O O O O O O B-PER O O O O O O O O O O O B-ORG I-ORG I-ORG B-PER I-PER O O B-DATE O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O B-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O: 4\n",
      "  Class B-PER I-PER O O O O O O B-DATE I-DATE O O O B-PER O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O: 9\n",
      "  Class O O O O B-LOC I-LOC O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O: 1\n",
      "  Class O O O O O O O B-LOC O O O O O O O O O O O O O O B-LOC O O O O O B-LOC O: 1\n",
      "  Class O O O O B-LOC O O O O B-LOC O B-LOC O B-LOC O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O B-ORG I-ORG O: 1\n",
      "  Class O O O B-LOC O: 1\n",
      "  Class O O O O O O B-LOC I-LOC O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O B-DATE O O O O O O O O O: 1\n",
      "  Class B-LOC O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O B-PER I-PER O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O B-LOC B-PER I-PER O: 1\n",
      "  Class O O O O B-LOC O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class O O O B-PER I-PER O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O B-DATE I-DATE O O O O O: 1\n",
      "  Class O O B-ORG I-ORG O O O O O O O B-PER I-PER O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O B-LOC O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class O O O O O O O O O O: 16\n",
      "  Class O O O O O O O O O O O O O O O O O O O: 5\n",
      "  Class O O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O B-PER I-PER I-PER O O B-PER I-PER O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O B-PER I-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC O B-LOC O O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O: 1\n",
      "  Class O O O O B-ORG O O O O O O O B-DATE I-DATE O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O B-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O: 1\n",
      "  Class O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O B-LOC O B-LOC O B-DATE O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O: 1\n",
      "  Class O O O O B-LOC O O O O B-PER I-PER O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O: 1\n",
      "  Class O O: 4\n",
      "  Class O O O B-PER O O O O O B-LOC O O O O O O O O B-PER I-PER O O O O O O B-LOC O: 1\n",
      "  Class B-DATE I-DATE O O O B-LOC O O O O O O O B-PER O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O B-LOC O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG I-ORG I-ORG O O O O B-LOC O O O O O O O O O O O B-LOC O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O B-DATE O O O O O O O: 1\n",
      "  Class O O O O O O B-PER O O O O O O: 2\n",
      "  Class O O O O O O O O O O O O O O O O O: 6\n",
      "  Class O O O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O: 14\n",
      "  Class O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O B-ORG O O O O O O O O B-LOC O O O O O B-LOC O O O O O O O O O O O O O: 1\n",
      "  Class O B-LOC O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-DATE O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O B-PER I-PER O: 1\n",
      "  Class B-PER O O O O B-PER I-PER O O B-LOC O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O O O B-PER O B-PER I-PER O B-PER O B-PER O B-LOC O B-PER O B-LOC O O: 1\n",
      "  Class O O O O O O O O B-ORG I-ORG I-ORG O B-PER I-PER I-PER O O O O O O O B-PER O O B-DATE O O O B-LOC O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O B-DATE I-DATE I-DATE O: 1\n",
      "  Class O O O B-PER I-PER: 3\n",
      "  Class O O O O O O O O O O B-ORG I-ORG I-ORG O O O O B-LOC O O O O: 1\n",
      "  Class O O O O B-LOC O B-LOC B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O B-LOC O O O B-PER I-PER O B-PER I-PER I-PER O O O O O B-LOC I-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O O: 5\n",
      "  Class O O O O O O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class B-PER O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O B-LOC O O O O B-DATE O O O O B-DATE O: 1\n",
      "  Class O O B-ORG O O B-LOC O O O O B-LOC O O O O O O O O O B-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class B-PER O B-PER: 2\n",
      "  Class O B-DATE I-DATE O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O: 1\n",
      "  Class O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class B-DATE I-DATE O O O O O O O O O O O O O O O O B-ORG O O O O B-LOC O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class O O O B-LOC O B-ORG I-ORG I-ORG O B-ORG O O O O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O B-LOC O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-LOC O O O O: 1\n",
      "  Class B-PER O O O O O B-LOC O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O B-DATE O O O B-DATE O O: 1\n",
      "  Class O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC B-DATE O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O: 10\n",
      "  Class O O O O O O O B-PER I-PER O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class O O B-DATE I-DATE I-DATE O: 1\n",
      "  Class B-PER O O O O O: 1\n",
      "  Class B-ORG O B-PER O B-LOC I-LOC O O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class O B-PER O B-LOC O O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O B-LOC O B-DATE O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-DATE O O O B-ORG I-ORG I-ORG O O O O O O O O O O B-LOC I-LOC O O O B-PER O: 1\n",
      "  Class O O O O O B-LOC O B-LOC O O O B-PER I-PER O O O O O O O B-DATE O O O B-LOC O O O O O: 1\n",
      "  Class B-ORG O O O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O B-PER O B-DATE O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O O O O B-PER I-PER O O O O O B-LOC O O B-LOC O B-DATE O: 1\n",
      "  Class O O B-LOC O O O O O O O O O O O O O: 1\n",
      "  Class O B-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-LOC O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER I-PER I-PER I-PER O O O O O O O O: 1\n",
      "  Class O O O O O O B-ORG O B-PER I-PER O O O O O B-LOC O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O O O B-LOC B-PER I-PER I-PER O O O O O O O B-LOC I-LOC I-LOC O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O: 1\n",
      "  Class O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC B-PER I-PER O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG O O O O B-DATE O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O: 5\n",
      "  Class O O O O O O O B-PER I-PER O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O B-DATE O O B-PER O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O O O B-LOC B-DATE O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-PER O O O O O O O O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O B-LOC O O O B-LOC O O B-DATE O: 1\n",
      "  Class O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O B-PER O O O O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER I-PER O O O O B-DATE O O O O: 1\n",
      "  Class O B-DATE O O O O B-PER I-PER O O O O O O O O B-DATE O O O O O B-LOC I-LOC O: 1\n",
      "  Class B-PER I-PER O O O B-LOC O O O O O O O: 1\n",
      "  Class O O O O O B-PER I-PER: 1\n",
      "  Class O O B-PER O O O O O O: 1\n",
      "  Class O O O B-LOC O O O B-PER O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC B-PER I-PER O O O O B-LOC O B-DATE O O O O O O O: 1\n",
      "  Class O O B-DATE O O O: 1\n",
      "  Class O O O O O O B-DATE I-DATE I-DATE O B-PER O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O O O B-PER O B-LOC O O O O B-LOC O O O O O O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER I-PER O B-LOC I-LOC I-LOC O O O B-LOC O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O B-PER O B-PER O O O O O: 1\n",
      "  Class O O O B-PER O B-LOC O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O B-LOC O O O O O B-PER O O O O B-DATE I-DATE O O B-DATE I-DATE O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O B-PER O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O B-PER O O O O B-ORG O O O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O: 1\n",
      "  Class O B-ORG O O O O O O B-LOC O: 1\n",
      "  Class O O O O B-PER O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O B-DATE O O B-LOC O O O B-LOC O O O O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-PER O O: 1\n",
      "  Class B-DATE I-DATE O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O B-ORG O O B-LOC I-LOC O O O B-LOC O: 1\n",
      "  Class B-PER I-PER O O O B-LOC O B-PER I-PER O O O B-LOC O B-PER I-PER O O O B-LOC I-LOC O B-PER I-PER O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC O O O: 1\n",
      "  Class O O O O O B-PER I-PER O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O B-DATE I-DATE O O O O O O O O O O O O O O B-DATE O B-PER O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O B-LOC O O O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class B-DATE I-DATE I-DATE O O B-DATE I-DATE O O O O O O O O O B-DATE I-DATE O B-DATE I-DATE I-DATE O: 1\n",
      "  Class O B-DATE I-DATE I-DATE O O O O O O B-ORG O B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O: 1\n",
      "  Class B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O O: 1\n",
      "  Class O O O O O B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O B-ORG O B-LOC I-LOC O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O B-PER O O O O O B-LOC O B-DATE O O O O O O O B-LOC O O: 1\n",
      "  Class O O B-PER I-PER O O O O B-DATE I-DATE I-DATE O O O O O B-ORG I-ORG O O O: 1\n",
      "  Class O B-PER I-PER O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class B-PER I-PER O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O B-DATE O: 2\n",
      "  Class B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-DATE I-DATE O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O B-DATE O O O B-PER I-PER I-PER I-PER I-PER O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O: 1\n",
      "  Class O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O: 1\n",
      "  Class B-PER I-PER: 1\n",
      "  Class O O O O B-PER I-PER I-PER I-PER O O O O O O O O O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O: 1\n",
      "  Class B-LOC O O O O O O B-LOC O O B-DATE I-DATE O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O: 7\n",
      "  Class O O O O O B-PER I-PER O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O B-LOC O O O O B-PER O: 1\n",
      "  Class B-PER O O O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O B-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O B-PER O O B-ORG I-ORG I-ORG O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O O O O O O O O O O O O O O O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O O O B-DATE O B-PER O O O O O O B-PER I-PER O O B-LOC O O O O B-PER O O O O O: 1\n",
      "  Class O O O B-PER O O O B-LOC O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O B-PER I-PER O O O O: 1\n",
      "  Class O O O O O B-ORG O B-ORG I-ORG O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O O B-ORG O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-ORG O O O O B-LOC O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O B-LOC O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O O B-LOC O: 1\n",
      "  Class O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-PER I-PER O: 1\n",
      "  Class B-PER I-PER O O O B-PER O O B-PER I-PER O O B-PER I-PER O O B-PER O O: 1\n",
      "  Class B-LOC I-LOC O O O O O B-LOC: 1\n",
      "  Class O O O O B-LOC O B-PER I-PER O O O O O O O O: 2\n",
      "  Class O O B-PER O O O O O O O O O B-LOC I-LOC O O: 1\n",
      "  Class B-ORG O B-ORG O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-PER I-PER O B-ORG I-ORG O O O O O O O O O B-PER O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O: 1\n",
      "  Class B-DATE I-DATE I-DATE B-ORG O O O O O B-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-ORG O B-ORG O: 1\n",
      "  Class O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O: 1\n",
      "  Class O B-PER I-PER I-PER O B-LOC O O B-PER O O O O O O B-LOC I-LOC I-LOC O O O O O O B-LOC O O: 1\n",
      "  Class B-PER O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-ORG I-ORG O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC I-LOC O B-ORG O O B-DATE I-DATE O O B-DATE O O O O O O O B-DATE O O O B-LOC O O O O B-LOC O B-LOC O O O B-LOC O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O B-DATE O O O O: 1\n",
      "  Class O O O O O: 9\n",
      "  Class O O O O O O O O O O O B-PER O: 1\n",
      "  Class O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O: 1\n",
      "  Class O O B-ORG I-ORG O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O B-LOC O O O B-LOC O O O O O O O O O B-LOC O O O O B-LOC O O O O O O O B-ORG O: 1\n",
      "  Class O O O O O O O B-DATE I-DATE I-DATE O: 1\n",
      "  Class O O B-DATE I-DATE I-DATE O O O O O O O O O O O B-DATE O O O O B-DATE I-DATE O O O O O O O O O O: 1\n",
      "  Class O O O B-PER I-PER B-ORG I-ORG O B-LOC I-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC I-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O: 1\n",
      "  Class O O O O O O O O O O O O O O B-PER O O: 1\n",
      "  Class O O O O B-DATE I-DATE I-DATE O O B-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-DATE O O O O O O O O O O O O O O: 1\n",
      "  Class O B-DATE I-DATE O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-ORG I-ORG I-ORG O O O B-LOC O O O O O B-LOC O O O O O B-ORG I-ORG O O O B-LOC O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-PER O O B-PER I-PER O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O B-ORG O B-LOC I-LOC O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O O O O O O B-ORG O B-PER I-PER O O O O B-ORG O B-PER I-PER O O O B-PER I-PER O B-PER I-PER O O O O B-PER O O O: 1\n",
      "  Class O B-LOC O O B-PER I-PER O O O O O O O O B-DATE I-DATE O O: 1\n",
      "  Class B-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-LOC O B-LOC B-DATE O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O: 1\n",
      "  Class O O O O O O B-PER I-PER O O O O B-LOC: 1\n",
      "  Class O O B-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC O O B-LOC O O O O O O O B-PER I-PER I-PER O O O: 1\n",
      "  Class O O O B-ORG O O B-PER I-PER O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-DATE I-DATE O B-ORG O O O O O O B-PER I-PER O O O O O B-DATE I-DATE O B-PER I-PER O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O B-PER O: 1\n",
      "  Class O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O B-PER O B-PER O O O O O B-LOC O: 1\n",
      "  Class O O O O B-LOC I-LOC O O O O: 2\n",
      "  Class O O O O B-PER I-PER O O O O O O O O B-LOC I-LOC O O O O O O O O O O O: 1\n",
      "  Class O B-PER I-PER O O B-PER I-PER O O O O O O O B-LOC B-DATE O: 1\n",
      "  Class O O O O O O O O B-DATE O: 2\n",
      "  Class O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O O O O O B-LOC O B-LOC O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER I-PER O O O B-PER O O O O O O O O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O: 1\n",
      "  Class O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O B-LOC I-LOC O O O O O O O O O B-LOC O O O: 1\n",
      "  Class O O O O: 6\n",
      "  Class O O O B-LOC O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O B-LOC O O B-PER O O O O O B-DATE I-DATE O O O: 1\n",
      "  Class B-PER O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O O O O O O O O B-DATE I-DATE O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O B-PER I-PER O O O: 1\n",
      "  Class B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O B-DATE O O O O O O O B-PER I-PER O O O O O B-LOC O O O O O B-PER O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O O O O O O B-LOC I-LOC O: 1\n",
      "  Class O O O O O O B-PER I-PER O O O O O O O B-ORG O O O O O B-PER O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O B-ORG O O O O O O O O O O B-DATE I-DATE O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-PER I-PER O O O B-ORG O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O: 1\n",
      "  Class O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O B-LOC O O O O O O O O B-PER O B-PER O: 1\n",
      "  Class O O B-LOC O O O O O O O O O B-LOC O B-ORG O B-ORG O O O O O O B-PER O O: 1\n",
      "  Class O O O O B-PER I-PER O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-PER I-PER: 1\n",
      "  Class O O B-LOC O O B-LOC I-LOC I-LOC O B-DATE O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-DATE O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-DATE O B-ORG O O O O O O O O B-ORG I-ORG O O B-LOC O: 1\n",
      "  Class O O B-PER I-PER O O O O O O B-ORG O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG I-ORG I-ORG O B-DATE O B-DATE O B-DATE O B-PER I-PER O: 1\n",
      "  Class O O O O B-LOC O O O O B-DATE O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O B-ORG O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O B-ORG O O B-LOC I-LOC O B-LOC I-LOC I-LOC O B-LOC I-LOC O: 1\n",
      "  Class B-PER I-PER O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O B-LOC O O O O B-DATE I-DATE O O O O O O O O O O O O O B-ORG I-ORG O: 1\n",
      "  Class B-PER I-PER O O O O O O O O O O O O O B-PER I-PER O: 1\n",
      "  Class O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-PER I-PER: 1\n",
      "  Class O O O O O O B-LOC O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O B-LOC I-LOC O O B-LOC O O O B-LOC B-DATE O: 1\n",
      "  Class O B-DATE I-DATE I-DATE O B-ORG O O O O O B-LOC I-LOC O O O O O: 1\n",
      "  Class O O O O O O B-PER I-PER O: 1\n",
      "  Class B-ORG I-ORG O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-PER O B-ORG O O B-DATE O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-PER O O O O B-PER O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-PER O O O O O O O O B-PER O O O O O: 1\n",
      "  Class O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O O O B-ORG O: 1\n",
      "  Class O B-DATE I-DATE O O O O O O O O O: 1\n",
      "  Class O B-DATE I-DATE O O O O O B-LOC O O O O B-PER O B-LOC O O O O O O: 1\n",
      "  Class O O O O O B-DATE I-DATE I-DATE O B-LOC I-LOC O O O B-LOC O: 1\n",
      "  Class B-ORG I-ORG I-ORG I-ORG O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O B-PER O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O B-PER I-PER O O O B-PER O O O O: 1\n",
      "  Class O O O O O O O O O B-PER I-PER O O O O O B-PER I-PER O B-PER I-PER O O: 1\n",
      "  Class O B-DATE I-DATE I-DATE O O B-ORG O O B-ORG O O O O O O O O O B-ORG O: 1\n",
      "  Class O O O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O: 1\n",
      "  Class O O O B-PER I-PER O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O B-LOC O: 1\n",
      "  Class O O O O B-LOC O O O O B-LOC I-LOC O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O B-ORG O B-ORG O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG O: 1\n",
      "  Class O O O O O O O O O O O O O B-DATE O O O O: 1\n",
      "  Class O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-DATE O O B-ORG I-ORG I-ORG O B-ORG O O O O B-LOC O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O B-PER O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC B-PER I-PER O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER I-PER O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O: 1\n",
      "  Class B-PER O O B-LOC O O O B-LOC O O B-LOC I-LOC O B-DATE I-DATE I-DATE O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG I-ORG I-ORG O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-ORG O: 1\n",
      "  Class B-PER O O O O O O O O O O: 1\n",
      "  Class O O O B-PER I-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O B-ORG I-ORG O O B-DATE O O O O O O B-DATE O: 1\n",
      "  Class O O O O B-LOC O O O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-ORG I-ORG O O O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class O B-DATE O B-PER I-PER O O O O B-PER O: 1\n",
      "  Class O O O O B-PER O O O O O O O O O O O O B-LOC O O O O O O B-LOC O O O O O O O O O B-LOC O O O O O B-PER O: 1\n",
      "  Class O O B-PER O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O: 1\n",
      "  Class O O O O O O B-ORG I-ORG I-ORG O O O: 1\n",
      "  Class O O O O O O O B-LOC O O O O O O O O B-LOC O: 1\n",
      "  Class B-ORG I-ORG O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O: 1\n",
      "  Class B-ORG I-ORG O O O O O O O O B-DATE I-DATE I-DATE O O O B-PER I-PER I-PER O O O O O B-ORG I-ORG O O O O O O O O O O O O O B-ORG I-ORG O: 1\n",
      "  Class O O O B-PER I-PER O O O O O O O B-LOC O O O O O O O O B-DATE O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-ORG O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O B-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O B-ORG O O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG O O O O B-ORG I-ORG O O B-ORG I-ORG O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-DATE I-DATE O B-ORG O O O O O O O O O B-DATE O B-LOC O B-ORG O O O O O: 1\n",
      "  Class O O B-ORG O O O O O O O B-DATE O O O O O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O O O B-LOC O O O O O O O B-LOC O: 1\n",
      "  Class O O O B-DATE O O O O: 2\n",
      "  Class O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-DATE I-DATE I-DATE I-DATE O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O O O B-LOC O B-PER I-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O B-PER I-PER O B-DATE O O O O O O O O O: 1\n",
      "  Class B-PER O O O B-ORG O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O: 1\n",
      "  Class O O O B-DATE O O O O B-DATE I-DATE I-DATE I-DATE O O O O: 1\n",
      "  Class O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG O O O O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O: 1\n",
      "  Class O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O B-LOC O O B-PER I-PER O O O O O O O O O O B-LOC O O O O O O O B-DATE I-DATE O O O O: 1\n",
      "  Class O O O O O O O O O B-DATE O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O O B-DATE I-DATE I-DATE I-DATE O O O O B-PER O O O: 1\n",
      "  Class O B-DATE I-DATE O O O O B-LOC I-LOC O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-ORG O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-DATE O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O B-DATE O B-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O: 1\n",
      "  Class O O O B-PER I-PER O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER I-PER O B-LOC I-LOC I-LOC: 1\n",
      "  Class O O B-DATE I-DATE O O O B-PER O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-LOC O O O O O O B-LOC O: 1\n",
      "  Class B-ORG O O O B-ORG O O O B-ORG O O B-ORG O O B-ORG O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O B-PER I-PER I-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O: 1\n",
      "  Class O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC O O O O O O B-LOC B-DATE O O O O O O O O B-ORG O O O O O O O O O B-LOC O O B-DATE I-DATE O: 1\n",
      "  Class O B-DATE O O B-LOC O O O O O O: 1\n",
      "  Class O O O B-ORG O O O O O O B-ORG I-ORG O B-ORG I-ORG O B-LOC O O O B-PER I-PER O B-ORG O O O O O B-ORG I-ORG O O B-PER I-PER O O O O O O O O O B-PER I-PER O O O O B-PER I-PER O B-PER I-PER O O O O O O O O O B-PER I-PER O O O O B-PER I-PER O O O B-PER I-PER O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-DATE I-DATE I-DATE O: 1\n",
      "  Class B-PER O O O B-PER I-PER O O B-PER I-PER O O O O O O B-PER I-PER O O B-PER I-PER O: 1\n",
      "  Class O O O O O O O O B-LOC B-PER I-PER O O O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-PER O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O B-ORG O O O O: 1\n",
      "  Class B-LOC I-LOC: 1\n",
      "  Class O O O O B-LOC O O O O O O O O O O O O O O O O O B-DATE O B-DATE I-DATE O: 1\n",
      "  Class O O B-PER O O B-DATE O O O O O O O O: 1\n",
      "  Class O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-LOC O O B-LOC B-DATE O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O B-PER I-PER O O B-ORG I-ORG I-ORG O B-ORG I-ORG: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O: 1\n",
      "  Class B-PER O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG O B-ORG O B-ORG O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O B-ORG O B-PER I-PER O O O O O O: 1\n",
      "  Class O O O O O B-LOC O B-PER O O O O O O O B-LOC O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O B-ORG O O O O O O B-LOC O B-ORG I-ORG O O O O O O O O O B-LOC O B-LOC I-LOC O: 1\n",
      "  Class O O O O O B-LOC I-LOC O O O: 1\n",
      "  Class O O O O B-ORG I-ORG I-ORG O O O: 1\n",
      "  Class O O O O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O B-DATE O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O B-PER I-PER O O B-PER I-PER O O B-PER O O O B-PER O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O B-DATE I-DATE O O O O O O O B-LOC B-PER I-PER I-PER O O O O O O: 1\n",
      "  Class O O O B-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE I-DATE O O O O B-PER O O O B-ORG O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE O: 1\n",
      "  Class B-ORG O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC: 1\n",
      "  Class O O O O O O B-PER O B-PER O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O B-LOC O O O O O O O O B-LOC I-LOC O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O: 1\n",
      "  Class O O O B-DATE O O O O O O O B-LOC O B-LOC: 1\n",
      "  Class O O O O O B-LOC I-LOC O O O O B-PER I-PER O O O O O O O O O: 1\n",
      "  Class O O B-ORG O O O O O O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER I-PER O O B-LOC O B-PER O B-PER I-PER O B-PER O O B-LOC O B-PER O B-PER O O B-LOC O: 1\n",
      "  Class O O B-DATE I-DATE I-DATE O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O O B-PER O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O B-DATE I-DATE O: 1\n",
      "  Class O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O B-DATE I-DATE O O O O: 1\n",
      "  Class O O B-PER I-PER O O O B-ORG B-DATE I-DATE I-DATE O O O O O O O O O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O O O O B-PER O O O O O: 1\n",
      "  Class O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O O O O O O O B-DATE O O O O O O O: 1\n",
      "  Class O O O O B-PER O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O B-PER O B-PER O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-PER I-PER O: 1\n",
      "  Class O O O O O O O O O B-ORG I-ORG B-PER I-PER O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O: 1\n",
      "  Class O O O O B-PER O O O O B-LOC: 1\n",
      "  Class B-ORG I-ORG I-ORG O B-LOC O B-PER O O O O O O O O O O O O O O B-LOC I-LOC O O B-PER I-PER O O O O O B-PER I-PER O O B-PER O O O O O O O O O O: 1\n",
      "  Class O B-LOC O O O O O B-ORG: 1\n",
      "  Class O O B-PER O O O O O O O O O B-LOC O O O O B-DATE O O O O O O O O: 1\n",
      "  Class O B-DATE O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class B-PER O O O B-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC B-PER I-PER O O O O O O O O O: 1\n",
      "  Class B-PER I-PER I-PER O O O O O B-LOC I-LOC O O O O O O O B-ORG O O O B-LOC O O O B-LOC O O B-LOC O O O O B-LOC I-LOC O O O B-LOC O O O B-LOC O O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG I-ORG O: 1\n",
      "  Class B-DATE I-DATE I-DATE B-PER O O O O O O O B-ORG I-ORG I-ORG I-ORG O: 1\n",
      "  Class O O B-PER I-PER O O O O B-ORG O O O O O O O O O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class O O O O O O O O O O O B-LOC O O O B-LOC I-LOC O O O B-LOC O O O O B-LOC O O O O B-LOC O O O O O O O O B-LOC O O B-LOC O O O O B-PER I-PER O O O B-DATE O O B-PER I-PER I-PER O O B-LOC O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC O O O O: 1\n",
      "  Class O O O O B-ORG O B-LOC I-LOC I-LOC I-LOC I-LOC O B-PER I-PER O O O O B-PER I-PER O O O O O O B-LOC I-LOC O O B-LOC O O O O O B-LOC O: 1\n",
      "  Class B-ORG O O O O O O O O O O O O O O B-ORG I-ORG O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG I-ORG O B-LOC O: 1\n",
      "  Class B-PER O B-PER O O O O O O O O O B-LOC O O O B-LOC O: 1\n",
      "  Class O O O O B-LOC B-PER I-PER O O O O B-PER I-PER O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O B-ORG I-ORG O O O O O O O O B-PER I-PER O O B-LOC O B-PER I-PER O B-ORG I-ORG O B-ORG I-ORG O O O O B-PER I-PER O: 1\n",
      "  Class O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-ORG O O O O O O O B-LOC O O O O O O O O B-ORG I-ORG O O O O O B-LOC O: 1\n",
      "  Class O O O O O B-PER O O O O O O O B-DATE I-DATE O O O B-PER O O O O O O O B-DATE O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O B-PER I-PER O O O O O O O B-DATE I-DATE O O O O O O O B-DATE O O O O O O O B-PER I-PER O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-ORG I-ORG O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O: 1\n",
      "  Class O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-ORG O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-LOC O O O O O O O O O O O O O O B-PER O O: 1\n",
      "  Class O O O O O O O O O O B-LOC O O O O O O O O B-LOC O O O: 1\n",
      "  Class O O B-DATE I-DATE O O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O B-DATE O O O O B-DATE O O O O O O O O: 1\n",
      "  Class O B-PER O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O: 1\n",
      "  Class O O B-PER O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-LOC I-LOC O O O O O O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O: 1\n",
      "  Class O O O O O O B-PER O O O O O O O B-LOC O B-DATE I-DATE I-DATE I-DATE O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O B-LOC O B-LOC O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-LOC O O O O B-ORG O O B-ORG O: 1\n",
      "  Class B-LOC O O O O O O O O O O B-DATE O O O O O O O O O: 1\n",
      "  Class O B-LOC O O B-ORG I-ORG I-ORG O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG I-ORG I-ORG O O O O O B-LOC O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O: 1\n",
      "  Class O O O O O O O O O B-LOC O O O O O O O O O O O O B-ORG O O B-LOC O O B-LOC O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O B-PER I-PER O O O O B-LOC O B-LOC I-LOC I-LOC O B-LOC O: 1\n",
      "  Class O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-LOC O: 1\n",
      "  Class O O O O O B-LOC O B-PER I-PER O O O O O O O B-DATE O O O O O O O O B-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O O B-ORG O O O O O O O O O O O O O O O O B-PER O O O O O B-ORG O O O O O O: 1\n",
      "  Class O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O B-DATE O O O O O O O O O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O O B-ORG O O B-LOC I-LOC O: 1\n",
      "  Class O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O B-DATE O O O O O O: 1\n",
      "  Class O O B-ORG O B-PER I-PER O O O O B-ORG O O O O O B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O B-LOC O O B-LOC O O O B-LOC O B-LOC O O O O O: 1\n",
      "  Class O O O O B-PER I-PER I-PER I-PER I-PER I-PER O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O O O O O O O B-LOC O O B-ORG O O O O B-LOC O B-LOC O B-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class O B-LOC O B-PER I-PER O O O O O O O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O O B-LOC O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-PER O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-PER I-PER O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O B-LOC I-LOC I-LOC O: 1\n",
      "  Class B-DATE I-DATE I-DATE B-PER O O O O O B-LOC O: 1\n",
      "  Class O O B-PER O O O B-DATE O B-DATE O B-DATE O O O O O O O O O O B-LOC O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O O O O O B-PER I-PER O B-LOC O: 1\n",
      "  Class O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O B-ORG I-ORG I-ORG O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O B-LOC O: 1\n",
      "  Class O O O B-DATE O O O O O B-LOC O O O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O: 1\n",
      "  Class O O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class B-ORG O B-ORG I-ORG O O B-DATE O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O B-DATE B-ORG O O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class B-PER O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-LOC I-LOC O O O O O B-LOC O O O O O O B-PER O O O B-PER O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O B-DATE I-DATE O O O O B-DATE I-DATE O O O O O O O O: 1\n",
      "  Class O B-DATE O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-PER O O O O O O O B-PER O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O B-LOC I-LOC O B-LOC O O O O B-DATE O O O B-PER O O: 1\n",
      "  Class O O O B-ORG O O O B-ORG O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O: 1\n",
      "  Class O O O O O O O B-LOC O O O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER I-PER O O O O B-PER O O O O B-LOC O O O O O O O: 1\n",
      "  Class B-PER O O O O B-ORG I-ORG O B-ORG O B-ORG O O O B-ORG O O: 1\n",
      "  Class O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O B-LOC I-LOC I-LOC O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O B-PER O O O O O O O: 1\n",
      "  Class O O O O B-DATE I-DATE O B-PER O O O O O O O O O: 1\n",
      "  Class O O O B-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER I-PER O O O O O O B-LOC I-LOC I-LOC O O O O B-DATE O O O O O O O B-DATE I-DATE O O O O O B-DATE I-DATE I-DATE O O: 1\n",
      "  Class O O B-PER I-PER I-PER O O O O O O B-PER O O B-LOC O B-LOC O O O O O O O O O B-PER O O O O O B-PER I-PER I-PER I-PER I-PER O: 1\n",
      "  Class O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER I-PER I-PER O B-ORG O O O O B-PER I-PER O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O B-PER O O B-PER I-PER O B-PER I-PER O O: 1\n",
      "  Class O O O B-LOC B-PER I-PER O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O O O O O O O O O O B-LOC O O O O O O B-PER I-PER O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-LOC B-DATE I-DATE O O B-ORG O O B-ORG O: 1\n",
      "  Class O B-PER O O O: 1\n",
      "  Class B-PER I-PER O B-ORG I-ORG I-ORG O O B-PER O O O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O B-ORG I-ORG O O O O O O O B-PER O O O O O O O O O O O B-PER O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O O B-ORG I-ORG O O O O O O O O O B-ORG O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-ORG O B-PER O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC I-LOC O B-LOC O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC O B-LOC O B-LOC O B-LOC O B-DATE O O O O O: 1\n",
      "  Class B-ORG I-ORG O O B-PER I-PER O O O O O O O O O O O O B-LOC O O O O O O O B-ORG I-ORG O O O B-DATE I-DATE I-DATE O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O B-PER O: 1\n",
      "  Class O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O O O B-ORG O O O B-PER I-PER I-PER O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O: 1\n",
      "  Class B-PER O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-ORG O B-DATE O O O O O O O O O O B-LOC O: 1\n",
      "  Class B-ORG O O O O O O O B-ORG I-ORG O B-LOC O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O B-PER I-PER O O O O O O O O O O B-LOC O O O O: 1\n",
      "  Class O O B-PER O O O B-PER O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class B-PER O O O O B-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O: 1\n",
      "  Class B-ORG I-ORG I-ORG O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE O O O O B-DATE I-DATE O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O B-LOC O O O O O O O O O O O O B-DATE I-DATE O B-DATE I-DATE O: 1\n",
      "  Class O O O O B-LOC O O O B-PER I-PER O O O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-DATE I-DATE O O O B-ORG O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O B-PER O B-PER O O O O O B-PER O O B-PER O O O O O O O O: 1\n",
      "  Class O O O B-PER O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC O O O O O O O O O O O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O O B-ORG O O B-ORG O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O B-LOC O O O O O O O O O B-LOC O O O: 1\n",
      "  Class O O O O B-DATE I-DATE I-DATE O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O O O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC I-LOC O O O O B-ORG O O O O B-ORG O O O O O O B-LOC O O O: 1\n",
      "  Class O O O O O O B-ORG O: 1\n",
      "  Class O O O O B-PER I-PER I-PER I-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O B-ORG O: 1\n",
      "  Class O O O O O O O O O O O O B-PER O: 1\n",
      "  Class B-PER O O O O O O: 1\n",
      "  Class O O B-PER O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-DATE O O B-LOC O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class O O O B-LOC O O O O O O O B-DATE O O O O O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O B-PER I-PER O B-LOC I-LOC O B-PER O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O B-PER O B-ORG I-ORG I-ORG O: 1\n",
      "  Class O O O O O O O O O O O O O O B-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O: 1\n",
      "  Class O O O O O O O O O B-DATE: 1\n",
      "  Class O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O B-DATE I-DATE O O O: 1\n",
      "  Class O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class B-PER O B-PER O O O B-DATE O: 1\n",
      "  Class B-PER I-PER O O O O O O B-PER I-PER O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-PER I-PER O O B-PER I-PER I-PER I-PER I-PER O O O O O O O O O B-LOC O O B-ORG I-ORG I-ORG O O O O O B-LOC O B-ORG O O O O O O O O O O B-LOC O O O B-LOC O O O O: 1\n",
      "  Class O B-PER I-PER I-PER I-PER O B-ORG I-ORG O O O O O B-LOC O: 1\n",
      "  Class O O O O O B-DATE O O O O O O O O O O B-DATE O O O O O B-LOC O: 1\n",
      "  Class O O O O B-PER I-PER O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O B-LOC O O O O O B-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER O O O B-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG O O B-ORG I-ORG O O O O O O B-DATE I-DATE O O O O O B-LOC O O O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-DATE O O O O O O O O O: 1\n",
      "  Class B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O: 2\n",
      "  Class B-PER O O O B-LOC O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-DATE I-DATE O O B-ORG I-ORG I-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-LOC O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG O O O O O O O O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O O O O B-ORG I-ORG I-ORG O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O: 1\n",
      "  Class O B-DATE I-DATE O B-LOC I-LOC O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O: 1\n",
      "  Class O O O B-LOC O O O O O O O O O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O B-LOC B-PER I-PER O O O O B-LOC O: 1\n",
      "  Class O O O O O O O B-LOC O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-DATE I-DATE O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O B-ORG O O O O O B-PER O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O B-DATE O O O O O O O O: 1\n",
      "  Class B-PER O B-PER O O B-ORG I-ORG I-ORG O O O O O O O: 1\n",
      "  Class B-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O B-DATE I-DATE O O O O: 1\n",
      "  Class B-PER O O O O B-ORG O O O O O O O B-ORG O O B-PER I-PER O B-PER I-PER O O O O O O O O B-ORG O O: 1\n",
      "  Class O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-ORG O O O: 1\n",
      "  Class O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O B-LOC O O B-LOC O O O O O O O: 1\n",
      "  Class O O O O O O O O B-PER O B-PER O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-DATE O O O O O O O O O: 1\n",
      "  Class O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC O O B-PER O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class B-PER O B-PER O O O O O B-LOC I-LOC B-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O B-PER O O O O B-DATE I-DATE O O O O: 1\n",
      "  Class B-PER I-PER O O O O O B-LOC B-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O B-PER I-PER O: 1\n",
      "  Class O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-ORG O O O O O O O: 1\n",
      "  Class O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class B-PER I-PER O O O B-PER O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class O O O O O O O B-LOC O O O O O O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O B-PER O: 1\n",
      "  Class B-PER O O O O O B-LOC O O O O O O B-ORG O: 1\n",
      "  Class O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O B-PER I-PER O: 1\n",
      "  Class O B-PER O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O B-LOC O O O B-PER O: 1\n",
      "  Class O O O O O O O O B-LOC I-LOC O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-ORG I-ORG O O: 2\n",
      "  Class B-ORG I-ORG O B-ORG O O O O O O O O O O: 1\n",
      "  Class B-PER O O O B-LOC O B-PER I-PER O O O O B-LOC O O O O O O B-LOC I-LOC O B-PER I-PER O B-PER I-PER O B-LOC I-LOC O O O B-LOC O B-PER O O O O O B-LOC O O B-LOC O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O B-PER O O O O O O O O O: 1\n",
      "  Class O O O O O O B-PER O O O O O O O O O O O O O B-ORG I-ORG I-ORG O: 1\n",
      "  Class O O O O O O O O O O O O O B-PER O O O O O O O O O O B-PER O O O O O O O O O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class O O O O O O O O O B-DATE O B-LOC I-LOC O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-ORG I-ORG I-ORG O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-DATE I-DATE I-DATE I-DATE O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O O O O O O B-ORG O B-PER I-PER O: 1\n",
      "  Class O O O O O O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O: 1\n",
      "  Class O O B-PER I-PER O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O B-LOC O O O O O O O O O B-PER O O O O O B-DATE O O O O O O: 1\n",
      "  Class B-ORG I-ORG I-ORG I-ORG O B-ORG O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-PER I-PER O O O O O B-PER O B-PER I-PER O O: 1\n",
      "  Class O O B-ORG O O O O B-LOC O O B-PER O O O O O B-ORG O B-DATE O: 1\n",
      "  Class O O O O O B-PER O O O O O O B-LOC I-LOC I-LOC I-LOC O O O O O O O B-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O B-ORG I-ORG O O O O O O O O: 1\n",
      "  Class O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O: 1\n",
      "  Class O O B-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-PER O O O O O O O B-PER O O O O O O O O O O B-PER O O O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-ORG I-ORG O O O O B-DATE I-DATE I-DATE I-DATE: 1\n",
      "  Class O O O O O O O O B-LOC B-PER I-PER O O O O O O O O B-LOC O O B-DATE O O O O O O O: 1\n",
      "  Class B-PER O B-PER I-PER O O O O O: 1\n",
      "  Class O O O O O B-DATE O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O: 1\n",
      "  Class O O O O O B-LOC O B-DATE O O O O B-LOC O: 1\n",
      "  Class O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O B-DATE I-DATE I-DATE O O O O: 1\n",
      "  Class B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O B-PER O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-LOC O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O: 1\n",
      "  Class O O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG I-ORG O: 1\n",
      "  Class B-PER O O O B-PER I-PER O B-PER O O O B-PER O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O B-LOC O O: 1\n",
      "  Class O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O B-PER O: 1\n",
      "  Class B-PER I-PER O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O: 1\n",
      "  Class O O B-PER I-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC I-LOC O B-PER I-PER O O O B-PER O: 1\n",
      "  Class O O O O B-PER O O O O B-LOC O O B-LOC O O O B-LOC O O B-LOC O O: 1\n",
      "  Class O O O O B-DATE I-DATE O O B-ORG I-ORG I-ORG O O O: 1\n",
      "  Class O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-DATE B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O B-LOC O O O O O O: 1\n",
      "  Class B-PER I-PER O O O B-PER I-PER O O B-LOC I-LOC O O O O B-LOC O O O O O O B-DATE O O O O O O O: 1\n",
      "  Class O O B-PER I-PER O O O O O O O O O O O O O O O O O B-PER I-PER O O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC O O B-LOC O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O B-PER O: 1\n",
      "  Class O O O O O O B-PER O O O B-PER O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O: 1\n",
      "Class distribution in eval set:\n",
      "  Class O O O O O O O O O O: 6\n",
      "  Class B-ORG I-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O: 4\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER O B-PER O O O O O O O O B-LOC O: 1\n",
      "  Class B-LOC O O O O O O B-ORG O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O: 1\n",
      "  Class O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O B-LOC I-LOC O: 1\n",
      "  Class O O O B-DATE O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O: 1\n",
      "  Class O O O O O O O O O: 2\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O B-LOC O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O B-LOC O: 1\n",
      "  Class O O O B-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O B-ORG I-ORG O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class B-PER O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O O O O O O O: 2\n",
      "  Class O O B-ORG O O O O O O O O O B-LOC O B-LOC O O B-LOC O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O: 4\n",
      "  Class O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O: 1\n",
      "  Class O O O O O O O B-ORG I-ORG O O O O O O B-LOC I-LOC O O O O O B-PER I-PER O O O B-LOC O O O O O O B-ORG I-ORG O O O O O O O O B-PER I-PER O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER I-PER O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O B-ORG I-ORG I-ORG B-PER I-PER I-PER O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O B-DATE I-DATE O B-LOC I-LOC I-LOC O O O B-LOC O: 1\n",
      "  Class B-LOC O O O O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class O B-PER O O O O O O O O O B-PER O O O O B-PER O O O O O O O O O O B-PER O: 1\n",
      "  Class O B-PER I-PER O O O O O B-LOC O O O O O O O O O O B-PER O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O: 2\n",
      "  Class B-LOC O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O: 5\n",
      "  Class B-DATE I-DATE I-DATE I-DATE O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG I-ORG O B-ORG I-ORG O O B-LOC O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O B-LOC O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O B-ORG I-ORG O: 1\n",
      "  Class O O B-ORG O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O: 4\n",
      "  Class O O O O O O O O O O O B-DATE O O O O O O B-LOC O O O B-LOC O: 1\n",
      "  Class B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O B-PER I-PER O O O B-PER I-PER O O O B-ORG I-ORG O O O: 1\n",
      "  Class O O O B-ORG O O O O O O B-ORG O O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O O B-ORG I-ORG O B-ORG O B-ORG O: 1\n",
      "  Class O B-PER O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O: 1\n",
      "  Class O O B-ORG O B-PER I-PER O O B-DATE I-DATE O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O: 1\n",
      "  Class O O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O O O O O O O O O B-DATE O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class O O B-PER O B-LOC O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-LOC O O O O B-LOC O O O O O B-PER I-PER O O O B-PER I-PER I-PER O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O B-LOC O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class B-PER O O O O O B-LOC O O O O B-PER O O O B-ORG O O O O O B-PER O O O B-PER O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O: 4\n",
      "  Class O O O O O O O O O O O O O O B-LOC O O B-LOC O: 1\n",
      "  Class O O B-PER O O B-DATE I-DATE O O O O O O: 1\n",
      "  Class O O O B-PER B-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O B-PER O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-PER I-PER O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG I-ORG O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-LOC O O O O O B-LOC O O O O O O O B-DATE I-DATE O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE I-DATE O O O O B-LOC O O B-DATE I-DATE I-DATE I-DATE O O O O O O O B-DATE O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O O B-ORG I-ORG I-ORG O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O B-LOC O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O B-LOC O B-LOC I-LOC O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC O O O O O O O B-LOC I-LOC I-LOC O O O O B-LOC O B-LOC O O O O B-LOC O B-LOC O O O O B-LOC O O: 1\n",
      "  Class B-ORG I-ORG I-ORG I-ORG I-ORG O B-DATE O O O O O O O O O O O O O: 1\n",
      "  Class B-DATE I-DATE I-DATE O O O O O O O O O B-LOC O O O O O O: 1\n",
      "  Class O B-LOC O O O: 1\n",
      "  Class O O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O O B-DATE I-DATE O B-DATE O: 1\n",
      "  Class O O O B-LOC O O O O O: 1\n",
      "  Class B-PER O B-PER O O O B-ORG O O O O O O O O O O B-ORG O B-PER O O O O: 1\n",
      "  Class O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O O B-ORG O: 1\n",
      "  Class O O O O O O B-ORG B-PER I-PER I-PER O O B-DATE O O O O O O O B-PER I-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-PER O B-PER O B-PER O O O O O O B-PER O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O: 1\n",
      "  Class O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG O O B-ORG I-ORG I-ORG O O O B-LOC I-LOC I-LOC O O O B-LOC I-LOC O O B-ORG I-ORG O O B-ORG I-ORG O: 1\n",
      "  Class O O B-DATE I-DATE I-DATE O B-PER O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC B-PER I-PER O O O O B-PER I-PER O B-LOC O O O B-LOC O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC O B-PER I-PER I-PER I-PER O B-DATE O O O O O O B-LOC I-LOC O O O O O O O O O: 1\n",
      "  Class O O B-ORG O O O O B-ORG O O B-PER O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O B-LOC O O: 1\n",
      "  Class O O O O O B-PER I-PER O O B-PER O O O O O O O B-LOC I-LOC O O O O O B-PER O O O O: 1\n",
      "  Class O O O O B-LOC O O: 1\n",
      "  Class O O O O O O B-LOC O O O O O B-PER O: 1\n",
      "  Class B-ORG I-ORG O O O O O O O B-LOC O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O B-LOC I-LOC O B-LOC O O B-LOC I-LOC O O O O O B-ORG I-ORG I-ORG I-ORG O O O B-DATE O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O B-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class B-PER O O O O O O O O B-LOC O O B-DATE I-DATE O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-DATE O O O O O O O O O O B-LOC O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER I-PER O B-PER I-PER I-PER I-PER I-PER O O O O O B-LOC O B-LOC O O O O O O O O O: 1\n",
      "  Class O O O O O O B-DATE I-DATE O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O B-LOC I-LOC I-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-LOC O O B-PER I-PER I-PER I-PER O O O O O O B-ORG O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O B-DATE I-DATE I-DATE O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-PER I-PER O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O O B-DATE O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER O B-PER O O B-PER O B-PER O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG I-ORG O O O O B-PER I-PER O O O B-PER O O O O O O O O O: 1\n",
      "  Class O O O B-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O O O: 1\n",
      "  Class O O O O O O O B-LOC O O B-PER: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O: 2\n",
      "  Class O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-ORG O B-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-DATE I-DATE I-DATE O O O O O O O: 1\n",
      "  Class O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class B-PER O O O O O O O O O O B-ORG O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O B-PER O O O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class B-PER O O B-PER O O O O O O O O O O O B-PER O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG O O O O O O O O B-DATE O O O B-LOC O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-DATE O O O O O O O O B-LOC O O O O O B-LOC O B-LOC O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O: 1\n",
      "  Class O B-PER I-PER O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC B-DATE O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O: 1\n",
      "  Class O O B-PER O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O B-LOC O O B-DATE O O B-PER I-PER O O O O O B-ORG I-ORG O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-PER I-PER O: 1\n",
      "  Class O O O O O O O O O O O O O O O B-LOC O O: 1\n",
      "  Class O O O O O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O B-LOC B-PER I-PER O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O B-LOC B-PER I-PER O O O O O O O O O B-LOC O B-LOC O B-LOC O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O B-DATE I-DATE O O O O O O: 1\n",
      "  Class O O B-DATE I-DATE O B-DATE I-DATE O O B-DATE I-DATE O B-DATE I-DATE O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class O O O O B-PER I-PER O O O O B-PER I-PER O O B-ORG I-ORG O B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG O O O O O O O O O O O O B-ORG I-ORG O: 1\n",
      "  Class O B-ORG I-ORG I-ORG O B-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG O O O O O O O O O B-ORG O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class O B-PER I-PER O O B-ORG O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O O O: 1\n",
      "  Class O O B-DATE O O O O B-LOC O O O O B-LOC O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O B-ORG O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "Class distribution in benchmark set:\n",
      "  Class O O O O O O O O B-LOC O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O: 4\n",
      "  Class O O O O O O O O O O B-LOC I-LOC O O O O O O: 1\n",
      "  Class O O O O B-ORG O O O O O O B-PER I-PER O O O B-ORG O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O B-PER O O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O B-PER O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class B-DATE I-DATE O O O O O O O O B-PER O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-DATE O O O O B-ORG O O O O O O O O O B-ORG O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O: 4\n",
      "  Class O O O O O O O O O O O: 3\n",
      "  Class O O O O O O O O: 3\n",
      "  Class O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-PER O B-LOC O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O B-LOC O B-LOC O O O O B-LOC O O O B-LOC O O O O B-LOC O O O: 1\n",
      "  Class O O O O O B-PER O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-DATE O B-PER I-PER O O O O O O B-LOC O B-LOC O O B-DATE I-DATE I-DATE O O O O O O B-LOC O B-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O: 1\n",
      "  Class B-PER O B-LOC O B-LOC I-LOC O B-DATE I-DATE I-DATE I-DATE O O O O O O B-PER O O O O O O O O O O B-LOC O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O: 4\n",
      "  Class O O O O O O O O O O O O: 2\n",
      "  Class O O O O O B-PER I-PER: 1\n",
      "  Class O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O B-LOC O O B-ORG I-ORG I-ORG O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O: 1\n",
      "  Class O O O B-DATE I-DATE I-DATE O O O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O O B-ORG O O O O O O O O O O: 1\n",
      "  Class O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-DATE I-DATE O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O O O O O B-PER I-PER I-PER O O O B-ORG O B-LOC O B-PER I-PER I-PER O O: 1\n",
      "  Class O O O O O O O O O O O O O B-DATE I-DATE O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O B-LOC O O B-ORG I-ORG I-ORG O B-DATE I-DATE O: 1\n",
      "  Class B-PER I-PER O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O B-ORG I-ORG O B-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG O B-ORG O B-ORG I-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-LOC O B-LOC O B-LOC O: 1\n",
      "  Class O O O O O B-DATE I-DATE O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-DATE I-DATE I-DATE O O O B-PER O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG O B-PER I-PER O B-DATE O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-LOC I-LOC O O O O O O B-ORG O O O O B-PER O O O O: 1\n",
      "  Class O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O B-DATE O O B-PER I-PER I-PER O O O O O O O O O O O O O B-PER I-PER I-PER O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O: 2\n",
      "  Class O O O O O O O O O O O B-ORG I-ORG I-ORG O O O B-ORG I-ORG O O O O O O O O: 1\n",
      "  Class O O O O O B-PER O O O O O O: 1\n",
      "  Class O O O O B-PER I-PER O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O B-DATE O: 1\n",
      "  Class O B-ORG I-ORG O O O O O O O O: 1\n",
      "  Class O O O O O O O B-DATE O O O O O O O O: 1\n",
      "  Class O O O B-LOC I-LOC O O O B-PER O O O O O O B-LOC O O O O B-ORG I-ORG I-ORG O O B-DATE O O: 1\n",
      "  Class O B-LOC I-LOC O B-DATE I-DATE O O O O O O B-PER I-PER O B-PER I-PER I-PER O: 1\n",
      "  Class O O O O O B-PER O O B-ORG O O O O O O O O O O O B-PER O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-LOC O O O: 1\n",
      "  Class O O O O O O O O O: 8\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O: 3\n",
      "  Class O O B-DATE O O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O B-LOC O O O O O O O O O O O O O O O B-PER I-PER O O O B-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-ORG O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-PER I-PER O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class B-PER O O O O O O O O B-LOC O B-PER O O O B-PER O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O B-LOC O B-PER I-PER O O O O B-ORG I-ORG I-ORG O B-LOC O O O O O: 1\n",
      "  Class B-ORG O O O O O B-LOC O O B-LOC: 1\n",
      "  Class O O O O O O O O O O O O O O B-PER O O O O O: 1\n",
      "  Class O B-PER O O O O B-LOC O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-ORG O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O O O B-LOC I-LOC O B-PER I-PER I-PER I-PER O O O O O O B-PER O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-PER O O O O O O O O O B-PER I-PER O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O B-ORG O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O O O: 1\n",
      "  Class O O O O O B-ORG O O O O O O O O O O O: 1\n",
      "  Class B-LOC O O O O O O B-PER I-PER O O O: 1\n",
      "  Class O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O: 2\n",
      "  Class O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC I-LOC: 1\n",
      "  Class O O O O O O O O B-LOC O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O B-LOC I-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O B-LOC I-LOC O B-LOC I-LOC O B-LOC I-LOC O: 1\n",
      "  Class O O B-ORG O B-LOC O B-PER I-PER O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-PER O O O O O O O O B-LOC O O B-PER O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O B-LOC B-ORG I-ORG I-ORG O O O O O: 1\n",
      "  Class O O B-ORG I-ORG O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O B-PER O O O O O O O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class B-DATE I-DATE O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O B-LOC I-LOC B-ORG O O O O O O O O O O O O O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O B-DATE O O O: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O O O O B-PER I-PER O B-PER I-PER B-ORG I-ORG O O O B-DATE I-DATE I-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O B-DATE O B-DATE O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O B-PER O O O O O O: 1\n",
      "  Class B-PER O O O O O O: 1\n",
      "  Class O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O B-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O B-DATE I-DATE O O O B-DATE I-DATE O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O B-LOC O B-DATE I-DATE I-DATE I-DATE O O O O B-PER I-PER O B-PER I-PER O O O O O O O O O O B-PER O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE I-DATE I-DATE I-DATE O O O O B-PER I-PER O B-PER I-PER O O O O O O O B-PER O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O: 1\n",
      "  Class B-PER I-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O B-LOC O B-LOC O: 1\n",
      "  Class O O O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class B-PER O B-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER O O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-LOC B-DATE O O O O O O O B-LOC I-LOC O B-PER O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O B-PER I-PER O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O: 2\n",
      "  Class O O B-DATE O O O O O O B-ORG O O O O B-DATE I-DATE O B-ORG O B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O: 1\n",
      "  Class O O O B-DATE O O O O O O O O O O O O: 1\n",
      "  Class O O O B-DATE O B-ORG I-ORG O B-ORG I-ORG O B-LOC O B-ORG I-ORG O B-ORG I-ORG O B-LOC I-LOC I-LOC I-LOC O: 1\n",
      "  Class B-PER O O O O O B-DATE O O O O B-LOC O O B-LOC O O O O: 1\n",
      "  Class O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O: 1\n",
      "  Class O O B-DATE I-DATE I-DATE O B-ORG I-ORG I-ORG O B-ORG O O O O O O B-ORG O B-ORG O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-LOC O O O O O O O O O: 1\n",
      "  Class O B-PER O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-ORG O O O O O B-ORG I-ORG O O B-LOC O O B-LOC I-LOC O O O O O O O O B-LOC O: 1\n",
      "  Class B-PER O O O O O O O O O O: 1\n",
      "  Class B-DATE B-LOC I-LOC O O O O O O O B-PER O O O O O O: 1\n",
      "  Class O O O O O O B-PER O O O O O O B-DATE O O O B-LOC B-PER I-PER O O O O O B-LOC B-PER I-PER O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O O B-DATE O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-ORG I-ORG O B-PER I-PER O O O O O O O O O B-LOC O O B-LOC O O O B-LOC O: 1\n",
      "  Class B-PER O B-PER O O O B-DATE O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O B-DATE I-DATE O B-DATE I-DATE O O B-DATE I-DATE O B-DATE I-DATE O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC O O O O O O O O O O: 1\n",
      "  Class O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O B-LOC O O O B-LOC O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-LOC O: 1\n",
      "  Class O O B-ORG O O O B-PER O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-ORG O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O O O O O B-PER O O O: 1\n",
      "  Class O O O O O O O O O B-LOC O O O O B-PER I-PER I-PER I-PER O O O O O O O O O O O O O B-LOC O O O O O O O O O O O: 1\n",
      "  Class B-LOC O O O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O: 1\n",
      "  Class O O O B-PER O O O O O O B-LOC O O O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O B-LOC O: 1\n",
      "  Class O O O O B-LOC O O O B-PER I-PER I-PER O B-PER I-PER O O O O B-LOC O O O O O O B-LOC O O O B-LOC O O O O: 1\n",
      "  Class O B-ORG I-ORG O B-ORG I-ORG O B-ORG I-ORG O B-ORG I-ORG O O O O O O: 1\n",
      "  Class O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-PER I-PER O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-ORG I-ORG O: 1\n",
      "  Class O O O B-PER O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG I-ORG O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O B-LOC I-LOC O O O O O O B-LOC: 1\n",
      "  Class B-PER O O O O O O O O O O O O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-DATE B-LOC O O O O B-LOC B-DATE I-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE O O B-LOC O: 1\n",
      "  Class O O O O O B-ORG O O O O O O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O B-DATE O: 1\n",
      "  Class O O O O O: 1\n",
      "  Class B-PER I-PER O O O B-LOC I-LOC I-LOC O O O O B-PER I-PER O B-PER I-PER O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O B-PER O B-PER O B-PER O B-PER O O O O: 1\n",
      "  Class O B-LOC I-LOC O O B-LOC O O O: 1\n",
      "  Class O O B-ORG I-ORG O B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O O B-LOC O O O O O O O O B-ORG I-ORG O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC O O O O O O O O: 1\n",
      "  Class O O B-PER O O O O B-PER O B-PER I-PER I-PER O O O B-PER O O O O O O O B-PER O O O O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O B-PER O: 1\n",
      "  Class O O O B-DATE O O O O O: 1\n",
      "  Class B-PER O B-PER I-PER I-PER O O O O O O B-ORG I-ORG I-ORG O B-LOC O: 1\n",
      "  Class O B-DATE O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O B-LOC O B-PER I-PER O B-DATE O B-LOC O O O O O O O O O O O O: 1\n",
      "  Class O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O O O O O: 1\n",
      "  Class B-ORG O O O O O O B-DATE O O O O O O O B-DATE I-DATE O: 1\n",
      "  Class B-PER O O O O O B-DATE O: 1\n",
      "  Class O O O O B-PER O B-LOC O O O O O O O O B-PER O O O O O O O B-ORG O: 1\n",
      "  Class O O O O: 1\n",
      "  Class O O O O O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O B-LOC O O O: 1\n",
      "  Class O O O O O B-LOC O O O O O O O O: 1\n",
      "  Class O O O O O O O O O O O O O O: 1\n",
      "  Class B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O: 1\n",
      "  Class O B-PER O O O O O O O O O B-LOC O B-LOC O O O O O: 1\n",
      "Class weights computed: {'O': 0.12834500106070248, 'B-PER': 3.9158576051779934, 'I-PER': 6.505376344086022, 'B-DATE': 9.020234291799786, 'B-ORG': 7.469135802469136, 'I-ORG': 8.504016064257028, 'B-LOC': 3.8622891016871863, 'I-DATE': 9.317931793179318, 'I-LOC': 14.116666666666667}\n"
     ]
    }
   ],
   "source": [
    "stratified_datasets = data_loader.prepare_stratified_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 1050\n",
      "eval dataset size: 225\n",
      "benchmark dataset size: 225\n",
      "class_weights dataset size: 9\n",
      "experimental dataset size: 2\n"
     ]
    }
   ],
   "source": [
    "for split, dataset in stratified_datasets.items():\n",
    "    print(f\"{split} dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying train dataset:\n",
      "Dataset type: <class 'pandas.core.frame.DataFrame'>\n",
      "Dataset shape: (1050, 4)\n",
      "Columns: ['text', 'label', 'language', 'split']\n",
      "Number of unique labels: 9\n",
      "Unique labels: {'I-ORG', 'O', 'I-DATE', 'B-DATE', 'B-LOC', 'I-LOC', 'I-PER', 'B-ORG', 'B-PER'}\n",
      "Text data check passed.\n",
      "Languages in train dataset: ['kin' 'lug' 'swh']\n",
      "Splits in train dataset: ['masakhane']\n",
      "Sample data:\n",
      "                                                   text  \\\n",
      "695   Perezida Paul Kagame aherutse kubikomozaho ati...   \n",
      "1022  Kati obulwadde obulala obulabika ngobunafu bwe...   \n",
      "51    Matayarisho ya uchaguzi Akiwa anajitayarisha k...   \n",
      "493     Hari icyo byafasha amakipe yitegura amarushanwa   \n",
      "390   Sendashonga wari umunyamuryango wa FPR Inkotan...   \n",
      "\n",
      "                                                  label language      split  \n",
      "695   O B-PER I-PER O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
      "1022                          O O O O O O O O O O O O O      lug  masakhane  \n",
      "51    O O O O O O O O O B-DATE O B-PER O O O O O O O...      swh  masakhane  \n",
      "493                                         O O O O O O      kin  masakhane  \n",
      "390   B-PER O O O B-ORG I-ORG O O O O O B-DATE O O O...      kin  masakhane  \n",
      "\n",
      "Verifying eval dataset:\n",
      "Dataset type: <class 'pandas.core.frame.DataFrame'>\n",
      "Dataset shape: (225, 4)\n",
      "Columns: ['text', 'label', 'language', 'split']\n",
      "Number of unique labels: 9\n",
      "Unique labels: {'I-ORG', 'O', 'I-DATE', 'B-DATE', 'B-LOC', 'I-LOC', 'I-PER', 'B-ORG', 'B-PER'}\n",
      "Text data check passed.\n",
      "Languages in eval dataset: ['lug' 'kin' 'swh']\n",
      "Splits in eval dataset: ['masakhane']\n",
      "Sample data:\n",
      "                                                  text  \\\n",
      "175  Abawagizi be baasigadde bamutenda nti musajja ...   \n",
      "107  Minisiteri y’Imari n’Igenamigambi yakoze gahun...   \n",
      "6    Utafiti huu pia umezingatia mauaji mikononi mw...   \n",
      "151  Ono yasabye poliisi okukozesa kamera ezaateeke...   \n",
      "94   3 zifite agaciro k’amafaranga asaga miliyari 2...   \n",
      "\n",
      "                                                 label language      split  \n",
      "175                                O O O O O O O O O O      lug  masakhane  \n",
      "107  B-ORG I-ORG I-ORG O O O O O O B-LOC O O O O O ...      kin  masakhane  \n",
      "6    O O O O O O O O O O O O O O O O O O O O O O O ...      swh  masakhane  \n",
      "151                        O O O O O O O O O O O O O O      lug  masakhane  \n",
      "94   O O O O O O O O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
      "\n",
      "Verifying benchmark dataset:\n",
      "Dataset type: <class 'pandas.core.frame.DataFrame'>\n",
      "Dataset shape: (225, 4)\n",
      "Columns: ['text', 'label', 'language', 'split']\n",
      "Number of unique labels: 9\n",
      "Unique labels: {'I-ORG', 'O', 'I-DATE', 'B-ORG', 'B-DATE', 'B-LOC', 'I-PER', 'I-LOC', 'B-PER'}\n",
      "Text data check passed.\n",
      "Languages in benchmark dataset: ['lug' 'kin' 'swh']\n",
      "Splits in benchmark dataset: ['masakhane']\n",
      "Sample data:\n",
      "                                                  text  \\\n",
      "175  Okwongera okugaziya akatale ka kaawa mu mmwaan...   \n",
      "107  Abantu benshi bakunze kwibaza ikibazo kijyanye...   \n",
      "6    Alikuwa mtu wa kwanza kuongoza benki huku akiw...   \n",
      "151  Ababiri bano balina endowooza zebyobufuzi ezen...   \n",
      "94   Buri nyubako ( apartment ) muri izo zari zigiz...   \n",
      "\n",
      "                                                 label language      split  \n",
      "175          O O O O O O O O B-LOC O O O O O O O O O O      lug  masakhane  \n",
      "107  O O O O O O O O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
      "6    O O O O O O O O O O O O O O O O O O O O O O O ...      swh  masakhane  \n",
      "151                                      O O O O O O O      lug  masakhane  \n",
      "94         O O O O O O O O O O B-LOC I-LOC O O O O O O      kin  masakhane  \n",
      "Class weights validated successfully.\n",
      "Experimental datasets structure validated.\n"
     ]
    }
   ],
   "source": [
    "# Verify data integrity\n",
    "if not data_loader.verify_data_integrity(stratified_datasets):\n",
    "    logger.error(\"Data integrity check failed. Please review the datasets.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRAIN Dataset ---\n",
      "Shape: (1050, 4)\n",
      "\n",
      "Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1050 entries, 695 to 912\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      1050 non-null   object\n",
      " 1   label     1050 non-null   object\n",
      " 2   language  1050 non-null   object\n",
      " 3   split     1050 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 41.0+ KB\n",
      "None\n",
      "\n",
      "Sample Data:\n",
      "                                                   text  \\\n",
      "695   Perezida Paul Kagame aherutse kubikomozaho ati...   \n",
      "1022  Kati obulwadde obulala obulabika ngobunafu bwe...   \n",
      "51    Matayarisho ya uchaguzi Akiwa anajitayarisha k...   \n",
      "493     Hari icyo byafasha amakipe yitegura amarushanwa   \n",
      "390   Sendashonga wari umunyamuryango wa FPR Inkotan...   \n",
      "\n",
      "                                                  label language      split  \n",
      "695   O B-PER I-PER O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
      "1022                          O O O O O O O O O O O O O      lug  masakhane  \n",
      "51    O O O O O O O O O B-DATE O B-PER O O O O O O O...      swh  masakhane  \n",
      "493                                         O O O O O O      kin  masakhane  \n",
      "390   B-PER O O O B-ORG I-ORG O O O O O B-DATE O O O...      kin  masakhane  \n",
      "\n",
      "Dataset Statistics:\n",
      "num_samples: 1050\n",
      "avg_text_length: 156.0647619047619\n",
      "num_classes: 801\n",
      "class_distribution: {'O O O O O O O O O O O O O': 16, 'O O O O O O O O O O': 16, 'O O O O O O O O O O O O O O O O O O': 14, 'O O O O O O O O O O O O O O O': 14, 'O O O O O O O O O O O O O O O O O O O O': 14, 'O O O O O O O O O O O': 14, 'O O O O O O O O O O O O O O O O O O O O O': 13, 'O O O O O O': 11, 'O O O O O O O O O O O O': 10, 'O O O O O O O O O': 10, 'O O O O O O O O O O O O O O': 10, 'O O O O O O O O O O O O O O O O O O O O O O O O': 9, 'O O O O O': 9, 'O O O O O O O': 9, 'O O O O O O O O O O O O O O O O O O O O O O O': 9, 'O O O O O O O O O O O O O O O O O O O O O O O O O': 8, 'O O O O O O O O O O O O O O O O': 7, 'O O O O O O O O O O O O O O O O O': 6, 'O O O O': 6, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O': 6, 'O O O O O O O O O O O O O O O O O O O': 5, 'O O O': 5, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 5, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 5, 'O O O O O O O O': 5, 'O O': 4, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 4, 'O O O O O O O O O O O O O O O O O O O O O O O O O O': 4, 'B-ORG O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O': 3, 'B-PER O O O O O O O O O O O O O O O O O': 3, 'O O O B-PER I-PER': 3, 'O O O O O B-LOC I-LOC I-LOC O': 3, 'B-PER O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O B-LOC O B-PER I-PER O O O O O O O O': 2, 'O O O O O O O O O O O O O O O B-ORG I-ORG O O': 2, 'B-LOC O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O B-ORG I-ORG O O O O O O O B-PER I-PER O O O O O O O O O O O': 2, 'O O O O O O B-PER O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O B-DATE O': 2, 'O O O O B-LOC I-LOC O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'B-PER O B-PER': 2, 'B-PER O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O B-DATE O O O O': 2, 'O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O B-DATE O': 2, 'O O O O O O B-LOC O O': 1, 'O O O O O O O O O O B-LOC O O O O O O O O B-LOC O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O': 1, 'O O O B-ORG I-ORG O B-LOC O': 1, 'O O O B-DATE O O O O O B-LOC O O O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O B-LOC O': 1, 'O O O O O B-LOC O O O O': 1, 'B-PER O B-PER O O O O O O O O O B-LOC O O O B-LOC O': 1, 'O O O O B-LOC B-PER I-PER O O O O B-PER I-PER O O O O O O O O O B-DATE O': 1, 'O O O B-ORG I-ORG O O O O O O O O B-PER I-PER O O B-LOC O B-PER I-PER O B-ORG I-ORG O B-ORG I-ORG O O O O B-PER I-PER O': 1, 'O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-ORG O O O O O O O B-LOC O O O O O O O O B-ORG I-ORG O O O O O B-LOC O': 1, 'O O O O O B-PER O O O O O O O B-DATE I-DATE O O O B-PER O O O O O O O B-DATE O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O B-PER I-PER O O O O O O O B-DATE I-DATE O O O O O O O B-DATE O O O O O O O B-PER I-PER O O O O O O O O O': 1, 'O O O O O O B-ORG I-ORG I-ORG O O O O O O O': 1, 'O O O O O O O O O B-ORG I-ORG O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O': 1, 'O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O O O O O': 1, 'O O O O B-ORG O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-LOC O O O O O O O O O O O O O O B-PER O O': 1, 'O B-DATE O O O O O O O O': 1, 'B-ORG I-ORG O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O': 1, 'B-PER O O O O O O O B-LOC I-LOC O B-LOC O O O O B-DATE O O O B-PER O O': 1, 'O O O B-PER O O O O O O O B-PER O O O O O O O O O': 1, 'B-PER O O O B-ORG O O O O O O O O': 1, 'O O O O O O B-LOC B-PER I-PER O O O O O O O O O': 1, 'B-PER I-PER I-PER O O O O O B-LOC I-LOC O O O O O O O B-ORG O O O B-LOC O O O B-LOC O O B-LOC O O O O B-LOC I-LOC O O O B-LOC O O O B-LOC O O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG I-ORG O': 1, 'O B-DATE O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE B-PER O O O O O O O B-ORG I-ORG I-ORG I-ORG O': 1, 'B-ORG O O O O O O O O O O O O O O B-ORG I-ORG O O O O O': 1, 'O O B-PER I-PER O O O O B-ORG O O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'B-PER O O O B-DATE B-ORG O O O O O O O O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O B-PER O O O': 1, 'O O O O O O O O O O O B-LOC O O O B-LOC I-LOC O O O B-LOC O O O O B-LOC O O O O B-LOC O O O O O O O O B-LOC O O B-LOC O O O O B-PER I-PER O O O B-DATE O O B-PER I-PER I-PER O O B-LOC O O O O': 1, 'B-ORG O B-ORG I-ORG O O B-DATE O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O B-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'O O O O B-ORG O B-LOC I-LOC I-LOC I-LOC I-LOC O B-PER I-PER O O O O B-PER I-PER O O O O O O B-LOC I-LOC O O B-LOC O O O O O B-LOC O': 1, 'O O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'B-PER O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-LOC I-LOC O O O O O B-LOC O O O O O O B-PER O O O B-PER O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O B-DATE I-DATE O O O O B-DATE I-DATE O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG I-ORG I-ORG O O O O O B-LOC O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O': 1, 'O O O O O O O O O O O O B-LOC O O B-LOC O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O B-ORG O O B-LOC O O B-LOC O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O B-LOC O B-PER I-PER O O O O O O O B-LOC I-LOC I-LOC O': 1, 'O B-PER O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O B-PER I-PER O O O O B-LOC O B-LOC I-LOC I-LOC O B-LOC O': 1, 'O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-LOC O': 1, 'O O O O O B-LOC O B-PER I-PER O O O O O O O B-DATE O O O O O O O O B-ORG O O O O O O O O': 1, 'O O B-DATE I-DATE O O B-LOC I-LOC I-LOC O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O B-LOC O O B-ORG O O O O B-LOC O B-LOC O B-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O O': 1, 'B-PER I-PER O O O O O O O B-ORG O O O O O O O O O O O O O O O O B-PER O O O O O B-ORG O O O O O O': 1, 'O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O B-DATE O O O O O O O O O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O O B-ORG O O B-LOC I-LOC O': 1, 'O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O B-DATE O O O O O O': 1, 'O O B-ORG O B-PER I-PER O O O O B-ORG O O O O O B-DATE I-DATE O': 1, 'O O O O O O O B-LOC O O B-LOC O O O B-LOC O B-LOC O O O O O': 1, 'O O O O B-PER I-PER I-PER I-PER I-PER I-PER O': 1, 'O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-DATE O O O O B-DATE O O O O O O O O': 1, 'O B-PER O O O O': 1, 'B-PER O O O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O': 1, 'O O B-PER O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O B-PER I-PER O B-LOC O': 1, 'O O B-PER O O O B-DATE O B-DATE O B-DATE O O O O O O O O O O B-LOC O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O': 1, 'O O O O O O O O O O B-LOC I-LOC O O O O O O B-LOC O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O': 1, 'B-DATE I-DATE I-DATE B-PER O O O O O B-LOC O': 1, 'B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O B-LOC O B-DATE I-DATE I-DATE I-DATE O O O O O O O': 1, 'O O B-PER I-PER O O O B-LOC O B-LOC O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-LOC O O O O B-ORG O O B-ORG O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O B-DATE O O O O O O O O O': 1, 'O B-LOC O O B-ORG I-ORG I-ORG O O O O': 1, 'O O O B-PER O O O O O O O O O O O': 1, 'O O O O B-LOC O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O B-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O O O B-DATE O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O B-DATE O B-DATE I-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O': 1, 'O O O B-PER I-PER O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O O O B-PER I-PER O B-LOC I-LOC I-LOC': 1, 'O O B-DATE I-DATE O O O B-PER O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-LOC O O O O O O B-LOC O': 1, 'B-ORG O O O B-ORG O O O B-ORG O O B-ORG O O B-ORG O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O B-PER I-PER I-PER O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O': 1, 'O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O O B-LOC B-DATE O O O O O O O O B-ORG O O O O O O O O O B-LOC O O B-DATE I-DATE O': 1, 'O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG O O O O O O B-ORG I-ORG O B-ORG I-ORG O B-LOC O O O B-PER I-PER O B-ORG O O O O O B-ORG I-ORG O O B-PER I-PER O O O O O O O O O B-PER I-PER O O O O B-PER I-PER O B-PER I-PER O O O O O O O O O B-PER I-PER O O O O B-PER I-PER O O O B-PER I-PER O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'B-PER O O O B-PER I-PER O O B-PER I-PER O O O O O O B-PER I-PER O O B-PER I-PER O': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-ORG O O O O': 1, 'B-LOC I-LOC': 1, 'O O O O B-LOC O O O O O O O O O O O O O O O O O B-DATE O B-DATE I-DATE O': 1, 'O O B-PER O O B-DATE O O O O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O': 1, 'O B-DATE O O B-LOC O O O O O O': 1, 'O O O B-PER I-PER O O O O O O O B-LOC O O O O O O O O B-DATE O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-ORG O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O': 1, 'O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-ORG O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O B-ORG O O B-LOC O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG O O O O B-ORG I-ORG O O B-ORG I-ORG O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-DATE I-DATE O B-ORG O O O O O O O O O B-DATE O B-LOC O B-ORG O O O O O': 1, 'O O B-ORG O O O O O O O B-DATE O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O B-LOC O O O O O O O B-LOC O': 1, 'O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-DATE I-DATE I-DATE I-DATE O O B-LOC O': 1, 'O O O O B-LOC O B-PER I-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O B-PER I-PER O B-DATE O O O O O O O O O': 1, 'O O O O O O O O O O O B-ORG O O O O O O': 1, 'O O O B-DATE O O O O B-DATE I-DATE I-DATE I-DATE O O O O': 1, 'O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O': 1, 'O O O O O O O B-ORG O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG O O O O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O O O O O O O B-LOC O': 1, 'O O O O B-LOC O O B-PER I-PER O O O O O O O O O O B-LOC O O O O O O O B-DATE I-DATE O O O O': 1, 'O O O O O O O O O B-DATE O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O O B-DATE I-DATE I-DATE I-DATE O O O O B-PER O O O': 1, 'O B-DATE I-DATE O O O O B-LOC I-LOC O O O O O O O O O': 1, 'B-PER O O O B-ORG O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O': 1, 'O O O O O B-PER I-PER O O B-LOC O B-PER O B-PER I-PER O B-PER O O B-LOC O B-PER O B-PER O O B-LOC O': 1, 'O O B-DATE I-DATE I-DATE O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O B-PER O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O B-DATE I-DATE O': 1, 'O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O B-DATE I-DATE O O O O': 1, 'O O B-PER I-PER O O O B-ORG B-DATE I-DATE I-DATE O O O O O O O O O B-LOC I-LOC I-LOC O': 1, 'O O O O O B-PER O O O O O': 1, 'O O O B-ORG O O O B-ORG O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O': 1, 'O O O O O O O O O B-LOC O O O O O O O B-DATE O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O B-LOC B-DATE O O O O O O O O O O O O O': 1, 'O O B-PER O O O B-PER O B-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-PER I-PER O': 1, 'O O O O O O O O O B-ORG I-ORG B-PER I-PER O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O': 1, 'O O O O B-PER O O O O B-LOC': 1, 'B-ORG I-ORG I-ORG O B-LOC O B-PER O O O O O O O O O O O O O O B-LOC I-LOC O O B-PER I-PER O O O O O B-PER I-PER O O B-PER O O O O O O O O O O': 1, 'O B-LOC O O O O O B-ORG': 1, 'O O B-PER O O O O O O O O O B-LOC O O O O B-DATE O O O O O O O O': 1, 'O O O O B-PER O O O O O O O O B-LOC O O O O O O O O O': 1, 'B-PER I-PER O B-PER I-PER O O B-ORG I-ORG I-ORG O B-ORG I-ORG': 1, 'O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O': 1, 'O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O': 1, 'B-PER O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG O B-ORG O B-ORG O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O': 1, 'O O B-PER I-PER O B-ORG O B-PER I-PER O O O O O O': 1, 'O O O O O B-LOC O B-PER O O O O O O O B-LOC O B-LOC O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O B-ORG O O O O O O B-LOC O B-ORG I-ORG O O O O O O O O O B-LOC O B-LOC I-LOC O': 1, 'O O O O O B-LOC I-LOC O O O': 1, 'O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O B-ORG O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'O O B-DATE O O O O O O O O O O': 1, 'B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O B-PER I-PER O O B-PER I-PER O O B-PER O O O B-PER O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O B-DATE I-DATE O O O O O O O B-LOC B-PER I-PER I-PER O O O O O O': 1, 'O O O B-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE I-DATE O O O O B-PER O O O B-ORG O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE O': 1, 'B-ORG O O O O O O O': 1, 'O O O O O O O O B-LOC': 1, 'O O O O O O B-PER O B-PER O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-LOC O O O O O O O O B-LOC I-LOC O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O': 1, 'O O O B-DATE O O O O O O O B-LOC O B-LOC': 1, 'O O O O O B-LOC I-LOC O O O O B-PER I-PER O O O O O O O O O': 1, 'O O O O B-LOC O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O B-PER O O O O O O O O O O B-LOC O O O O O': 1, 'B-PER O B-PER O O O O O B-LOC I-LOC B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O B-PER O O O O B-DATE I-DATE O O O O': 1, 'B-PER I-PER O O O O O B-LOC B-PER O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O B-PER I-PER O': 1, 'O O O O B-LOC O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-ORG O O O O O O O': 1, 'O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'B-PER I-PER O O O B-PER O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O O O B-LOC B-PER I-PER O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O B-LOC O O O O O O B-ORG O': 1, 'O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O': 1, 'O O O O O O O O O O B-PER O O O O O O O B-PER O O O O O O O O O O B-PER O O O O B-PER O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O B-LOC O O O B-PER O': 1, 'O O O O O O O O B-LOC I-LOC O B-LOC O O O O O O O O O O': 1, 'B-ORG I-ORG O B-ORG O O O O O O O O O O': 1, 'B-PER O O O B-LOC O B-PER I-PER O O O O B-LOC O O O O O O B-LOC I-LOC O B-PER I-PER O B-PER I-PER O B-LOC I-LOC O O O B-LOC O B-PER O O O O O B-LOC O O B-LOC O O O O O O O': 1, 'O O B-PER O O O O O O O O O O B-PER O O O O O O O O O': 1, 'O O O O O O O B-LOC O O O O O O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O B-PER O': 1, 'O O O B-ORG O O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O O O B-ORG I-ORG I-ORG O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O': 1, 'O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O': 1, 'O B-DATE I-DATE O B-LOC I-LOC O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O B-LOC B-PER I-PER O O O O B-LOC O': 1, 'O O O O O O O B-LOC O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O B-ORG O O O O O B-PER O O O O O O O': 1, 'B-PER O O O O O O O O O B-DATE O O O O O O O O': 1, 'B-PER O B-PER O O B-ORG I-ORG I-ORG O O O O O O O': 1, 'B-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O B-LOC O O B-DATE I-DATE O O O O': 1, 'B-PER O O O O B-ORG O O O O O O O B-ORG O O B-PER I-PER O B-PER I-PER O O O O O O O O B-ORG O O': 1, 'O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-ORG O O O': 1, 'O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O B-LOC O O B-LOC O O O O O O O': 1, 'O O O O O O O O B-PER O B-PER O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-DATE O O O O O O O O O': 1, 'B-PER I-PER O O O O B-DATE I-DATE O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O B-PER O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O O O O O O O O O O O O O B-DATE O': 1, 'B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O': 1, 'O O O O O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG I-ORG O': 1, 'B-PER O O O B-PER I-PER O B-PER O O O B-PER O': 1, 'B-PER O O O O O O O O O O O O O O O O B-LOC O O': 1, 'O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O B-PER O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O B-ORG I-ORG I-ORG O': 1, 'O O B-PER I-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC I-LOC O B-PER I-PER O O O B-PER O': 1, 'O O O O B-PER O O O O B-LOC O O B-LOC O O O B-LOC O O B-LOC O O': 1, 'O O O O B-DATE I-DATE O O B-ORG I-ORG I-ORG O O O': 1, 'O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-DATE B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O B-LOC O O O O O O': 1, 'B-PER I-PER O O O B-PER I-PER O O B-LOC I-LOC O O O O B-LOC O O O O O O B-DATE O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O O O O O O O O B-PER I-PER O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O B-LOC O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O B-PER O O O B-PER O O O O O O O O O O': 1, 'B-PER I-PER O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O': 1, 'O O O O O O O O O O O O O B-PER O O O O O O O O O O B-PER O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O O O O O O O B-DATE O B-LOC I-LOC O O O O O O O O O': 1, 'B-PER I-PER O O O O B-ORG I-ORG I-ORG O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-DATE I-DATE I-DATE I-DATE O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O O O O B-ORG O B-PER I-PER O': 1, 'O O O O O O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O': 1, 'O O B-PER I-PER O O O O O O O O O O O O': 1, 'B-PER I-PER O O O B-LOC O O O O O O O O O B-PER O O O O O B-DATE O O O O O O': 1, 'B-ORG I-ORG I-ORG I-ORG O B-ORG O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O B-PER O B-PER I-PER O O': 1, 'O O B-ORG O O O O B-LOC O O B-PER O O O O O B-ORG O B-DATE O': 1, 'B-PER O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O B-ORG I-ORG O O O O O O O O': 1, 'O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O': 1, 'O O B-ORG O O O O O O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG O O O O B-DATE I-DATE I-DATE I-DATE': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O O O O O O B-LOC O O B-DATE O O O O O O O': 1, 'B-PER O B-PER I-PER O O O O O': 1, 'O O O O O B-DATE O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O O': 1, 'O O O O O B-LOC O B-DATE O O O O B-LOC O': 1, 'O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O B-PER O O O O O O B-LOC I-LOC I-LOC I-LOC O O O O O O O B-PER O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG I-ORG O O O O O O O O O B-ORG O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-ORG O B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC I-LOC O B-LOC O O O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O': 1, 'B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC O B-LOC O B-LOC O B-LOC O B-DATE O O O O O': 1, 'B-ORG I-ORG O O B-PER I-PER O O O O O O O O O O O O B-LOC O O O O O O O B-ORG I-ORG O O O B-DATE I-DATE I-DATE O O O O B-LOC O': 1, 'O O O O O O O O O O B-PER O': 1, 'O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'B-ORG O O O O O O O O B-ORG O O O B-PER I-PER I-PER O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-ORG O B-DATE O O O O O O O O O O B-LOC O': 1, 'B-ORG O O O O O O O B-ORG I-ORG O B-LOC O': 1, 'B-PER O O O O O O O O O O O O O O O O B-DATE O': 1, 'O B-PER I-PER O O O O O O O O O O B-LOC O O O O': 1, 'O O B-PER O O O B-PER O O O O O O O B-DATE I-DATE O': 1, 'B-PER O O O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O': 1, 'B-ORG I-ORG I-ORG O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE O O O O B-DATE I-DATE O': 1, 'O O B-ORG I-ORG I-ORG O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O B-LOC O O O O O O O O O O O O B-DATE I-DATE O B-DATE I-DATE O': 1, 'O O O O B-LOC O O O B-PER I-PER O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O O B-LOC O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER O O O O B-PER O O O O B-LOC O O O O O O O': 1, 'B-PER O O O O B-ORG I-ORG O B-ORG O B-ORG O O O B-ORG O O': 1, 'O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-LOC I-LOC I-LOC O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O B-PER O O O O O O O': 1, 'O O O O B-DATE I-DATE O B-PER O O O O O O O O O': 1, 'O O O B-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O': 1, 'O O B-PER I-PER I-PER O O O O O O B-PER O O B-LOC O B-LOC O O O O O O O O O B-PER O O O O O B-PER I-PER I-PER I-PER I-PER O': 1, 'O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER I-PER O B-ORG O O O O B-PER I-PER O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O B-PER O O B-PER I-PER O B-PER I-PER O O': 1, 'B-ORG I-ORG O O O O O O O O B-DATE I-DATE I-DATE O O O B-PER I-PER I-PER O O O O O B-ORG I-ORG O O O O O O O O O O O O O B-ORG I-ORG O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O B-LOC O O O O O O B-PER I-PER O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC B-DATE I-DATE O O B-ORG O O B-ORG O': 1, 'O B-PER O O O': 1, 'B-PER I-PER O B-ORG I-ORG I-ORG O O B-PER O O O O O O O O O O': 1, 'B-ORG O O O O O O B-ORG I-ORG O O O O O O O B-PER O O O O O O O O O O O B-PER O O O O O O O O': 1, 'B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O': 1, 'O O O O O B-PER I-PER O O O O O O B-LOC I-LOC I-LOC O O O O B-DATE O O O O O O O B-DATE I-DATE O O O O O B-DATE I-DATE I-DATE O O': 1, 'O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O B-DATE I-DATE O O O': 1, 'O O O O O O O O O O O O B-LOC O': 1, 'B-PER O B-PER O O O B-DATE O': 1, 'B-PER I-PER O O O O O O B-PER I-PER O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O B-LOC O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-PER I-PER O O B-PER I-PER I-PER I-PER I-PER O O O O O O O O O B-LOC O O B-ORG I-ORG I-ORG O O O O O B-LOC O B-ORG O O O O O O O O O O B-LOC O O O B-LOC O O O O': 1, 'O B-PER I-PER I-PER I-PER O B-ORG I-ORG O O O O O B-LOC O': 1, 'O O O O O B-DATE O O O O O O O O O O B-DATE O O O O O B-LOC O': 1, 'O O O O B-PER I-PER O B-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O B-LOC O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O B-DATE I-DATE O O O B-ORG O O O O O O O O O B-LOC O': 1, 'B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG O O B-ORG I-ORG O O O O O O B-DATE I-DATE O O O O O B-LOC O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-DATE O O O O O O O O O': 1, 'B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-LOC O O O O O O O': 1, 'O O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE I-DATE O O B-ORG I-ORG I-ORG O O O O O O O O': 1, 'O O O O O O O O O O O B-LOC O O O': 1, 'O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O O B-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O B-PER O O O O O B-PER O O B-PER O O O O O O O O': 1, 'O O O B-PER O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O': 1, 'B-PER I-PER O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O B-PER I-PER O O O O B-ORG O O B-ORG O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O O O B-LOC O O O O O O O O O B-LOC O O O': 1, 'O O O O B-DATE I-DATE I-DATE O O O O O': 1, 'O O O O O O O O B-LOC O O O B-LOC O O O O O O': 1, 'O O O O O O O O O B-LOC I-LOC O O O O B-ORG O O O O B-ORG O O O O O O B-LOC O O O': 1, 'O O O O O O B-ORG O': 1, 'O O O O B-PER I-PER I-PER I-PER O': 1, 'O O O O O O O O O O O O O O O O O O B-ORG O': 1, 'O O O O O O O O O B-DATE O': 1, 'B-PER O O O O O O': 1, 'O O B-PER O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O B-LOC O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'O O O B-LOC O O O O O O O B-DATE O O O O O B-LOC I-LOC I-LOC O': 1, 'O O O O O O B-PER I-PER O B-LOC I-LOC O B-PER O O': 1, 'B-PER O O O O O O O O O O O O O O O B-PER O B-ORG I-ORG I-ORG O': 1, 'O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O': 1, 'O O O O O O O O O B-DATE': 1, 'O O O O O O O O O O O O B-PER O': 1, 'O O B-DATE I-DATE I-DATE O': 1, 'B-PER O O O O O': 1, 'B-ORG O B-PER O B-LOC I-LOC O O O O O O O O B-LOC O O O O O O': 1, 'O B-PER O B-LOC O O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O B-LOC O B-DATE O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O O B-ORG I-ORG I-ORG O O O O O O O O O O B-LOC I-LOC O O O B-PER O': 1, 'O O O O O B-LOC O B-LOC O O O B-PER I-PER O O O O O O O B-DATE O O O B-LOC O O O O O': 1, 'B-ORG O O O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O B-PER O B-DATE O O O O O O O O': 1, 'O O O O B-LOC O O O O B-PER I-PER O O O O O B-LOC O O B-LOC O B-DATE O': 1, 'O O B-LOC O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-DATE O O O O': 1, 'O O O O O O O B-LOC O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER I-PER I-PER I-PER O O O O O O O O': 1, 'O O O O O O B-ORG O B-PER I-PER O O O O O B-LOC O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O O O': 1, 'O O O O B-LOC B-PER I-PER I-PER O O O O O O O B-LOC I-LOC I-LOC O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O': 1, 'O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC B-PER I-PER O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O B-ORG O O O O B-DATE O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-ORG I-ORG I-ORG O B-PER I-PER I-PER O O O O O O O B-PER O O B-DATE O O O B-LOC O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O B-ORG I-ORG I-ORG O O O O B-LOC O O O O': 1, 'O O O O B-LOC O B-LOC B-DATE I-DATE I-DATE I-DATE I-DATE O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O B-LOC O O O B-PER I-PER O B-PER I-PER I-PER O O O O O B-LOC I-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'B-PER O O O O O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O B-LOC O O O O B-DATE O O O O B-DATE O': 1, 'O O B-ORG O O B-LOC O O O O B-LOC O O O O O O O O O B-PER O': 1, 'O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O B-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O': 1, 'O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O B-LOC O B-LOC O': 1, 'B-DATE I-DATE O O O O O O O O O O O O O O O O B-ORG O O O O B-LOC O O O O O O O B-LOC O O O O O O': 1, 'O O O B-LOC O B-ORG I-ORG I-ORG O B-ORG O O O O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O O O': 1, 'B-PER O O O O O B-LOC O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O O O O B-DATE O O O B-DATE O O': 1, 'O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC B-DATE O O O O O O': 1, 'O O O O O O O B-PER I-PER O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-DATE I-DATE O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O': 1, 'O B-PER O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O B-PER O B-PER O O O O O': 1, 'O O O B-PER O B-LOC O O O O O O O O O': 1, 'B-PER I-PER O O O O O B-LOC O O O O O B-PER O O O O B-DATE I-DATE O O B-DATE I-DATE O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O B-PER O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'B-PER I-PER O O O B-PER O O O O B-ORG O O O O O O O O O O O O': 1, 'B-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O B-ORG O O O O O O B-LOC O': 1, 'O O O O B-PER O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O': 1, 'O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O B-DATE O O B-LOC O O O B-LOC O O O O': 1, 'O O O O O B-LOC O O O O O O O O O O O': 1, 'O O O O O O O B-PER O O': 1, 'B-DATE I-DATE O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O B-ORG O O B-LOC I-LOC O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER I-PER O O O O O O O O O O O O O': 1, 'O O B-DATE O O B-PER O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O B-LOC B-DATE O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER O O O O O O O O O O O O O O O O': 1, 'O O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O O O B-PER O O O O O O O O O O O O': 1, 'O O B-PER O O O O B-LOC O O O B-LOC O O B-DATE O': 1, 'O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O B-LOC O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O B-PER O O O O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O B-LOC I-LOC I-LOC O O O B-LOC O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O': 1, 'B-PER I-PER I-PER O O O O B-DATE O O O O': 1, 'O B-DATE O O O O B-PER I-PER O O O O O O O O B-DATE O O O O O B-LOC I-LOC O': 1, 'B-PER I-PER O O O B-LOC O O O O O O O': 1, 'O O O O O B-PER I-PER': 1, 'O O B-PER O O O O O O': 1, 'O O O B-LOC O O O B-PER O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O O B-LOC O B-DATE O O O O O O O': 1, 'O O B-DATE O O O': 1, 'O O O O O O B-DATE I-DATE I-DATE O B-PER O O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O B-PER O B-LOC O O O O B-LOC O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-ORG I-ORG O O O O O O O O O O O': 1, 'O O O O O O B-LOC B-ORG O B-DATE O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O B-PER I-PER I-PER O O O O B-LOC O O O O O O O': 1, 'O B-ORG I-ORG I-ORG O B-PER I-PER O O B-PER O O O O O O O O O O O O O': 1, 'O O O O B-LOC I-LOC B-PER I-PER I-PER O O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O B-LOC O O O O O O O': 1, 'O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'O O O O B-LOC O B-LOC I-LOC O B-LOC I-LOC O B-LOC I-LOC O O O O O O O O O O': 1, 'O B-ORG I-ORG I-ORG O B-PER I-PER I-PER O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O': 1, 'O O O O O B-PER O B-PER I-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O O O O O O O O O O O O': 1, 'B-PER O O O O B-PER I-PER O O B-LOC O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O O O B-PER O B-PER I-PER O B-PER O B-PER O B-LOC O B-PER O B-LOC O O': 1, 'O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE O B-PER O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O': 1, 'B-PER I-PER B-LOC I-LOC O B-PER O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-PER O O O': 1, 'B-PER O O O B-PER O O O O O O O O O O O O B-PER O B-LOC I-LOC O B-LOC O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O B-ORG O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O B-PER O O O O O O O O O O O O B-PER O O O O O O O B-DATE O O O O O O O B-PER O O O O O O O O': 1, 'B-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER O O O O B-PER O O O O O B-ORG O O O O O O O B-PER O O O O O O O O O O O B-ORG I-ORG I-ORG B-PER I-PER O O B-DATE O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O B-PER O': 1, 'O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O': 1, 'B-LOC I-LOC O O O O O O O B-DATE I-DATE I-DATE O O O O': 1, 'O O O O O O O O O B-DATE O B-PER O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-ORG I-ORG O O O O O B-DATE O O O O O B-LOC O': 1, 'O O O O O B-LOC O B-DATE I-DATE O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'B-PER O B-PER I-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-DATE O B-DATE O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O B-PER I-PER O': 1, 'O O O B-LOC O B-LOC O O O O O O O O O O O O B-LOC O B-LOC O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O B-PER O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O B-ORG O O O O O B-DATE O': 1, 'B-PER I-PER O O O O B-LOC I-LOC I-LOC O': 1, 'O O O B-PER O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O B-LOC I-LOC O': 1, 'O O O O O B-PER O O O B-PER O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-LOC O O O O O O O O': 1, 'O B-DATE O O B-ORG O O O O O O O O O O': 1, 'O O O B-LOC O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-PER O O O O O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O': 1, 'O O O O O B-LOC I-LOC': 1, 'O O O O O O B-PER I-PER O O B-PER I-PER I-PER O': 1, 'O O O O B-ORG O O O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O': 1, 'O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O B-LOC O B-LOC O B-DATE O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O': 1, 'O O O O B-LOC O O O O B-PER I-PER O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O': 1, 'O O O B-PER O O O O O B-LOC O O O O O O O O B-PER I-PER O O O O O O B-LOC O': 1, 'B-DATE I-DATE O O O B-LOC O O O O O O O B-PER O O O O O O O O O O': 1, 'B-PER I-PER O O O B-LOC O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG I-ORG I-ORG O O O O B-LOC O O O O O O O O O O O B-LOC O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O B-DATE O O O O O O O': 1, 'O O O O B-LOC O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O B-DATE I-DATE O O O B-PER O O O O O': 1, 'O B-PER O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O B-ORG O O O O O O O O B-LOC O O O O O B-LOC O O O O O O O O O O O O O': 1, 'O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O B-PER I-PER O': 1, 'O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-DATE O O O O O O O O O O O O O O': 1, 'O O O O B-LOC I-LOC O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O O B-LOC O O O O O O O O O O O O O O B-LOC O O O O O B-LOC O': 1, 'O O O O B-LOC O O O O B-LOC O B-LOC O B-LOC O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O B-ORG I-ORG O': 1, 'O O O B-LOC O': 1, 'O O O O O O B-LOC I-LOC O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O B-DATE O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O B-PER I-PER O B-DATE I-DATE I-DATE I-DATE O O O O O O O O O O O B-LOC B-PER I-PER O': 1, 'O O O O B-LOC O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O B-DATE I-DATE O': 1, 'B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O B-PER I-PER I-PER O O B-PER I-PER O O O O O': 1, 'O O B-PER I-PER O O O B-PER O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'O O O B-LOC O B-LOC O O O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O': 1, 'O O O O B-ORG O O O O O O O B-DATE I-DATE O O O O O O O O O O': 1, 'O O O B-PER I-PER O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O B-DATE I-DATE O O O O O': 1, 'B-PER I-PER O O B-ORG O O O O O O O O O O B-DATE I-DATE O O O O O O': 1, 'O O O O O O O O O O O O B-PER I-PER O O O B-ORG O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O B-LOC O O O O O O O O B-PER O B-PER O': 1, 'O O B-LOC O O O O O O O O O B-LOC O B-ORG O B-ORG O O O O O O B-PER O O': 1, 'O O O O B-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O B-LOC O O O O O O': 1, 'O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-PER I-PER': 1, 'O O B-LOC O O B-LOC I-LOC I-LOC O B-DATE O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O B-LOC O B-PER I-PER O O O B-LOC O B-PER I-PER O O O B-LOC I-LOC O B-PER I-PER O': 1, 'O B-DATE O B-ORG O O O O O O O O B-ORG I-ORG O O B-LOC O': 1, 'O O B-PER I-PER O O O O O O B-ORG O O O O O O O O O': 1, 'O O O O O O O O O B-ORG O B-PER I-PER O O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG I-ORG I-ORG O B-DATE O B-DATE O B-DATE O B-PER I-PER O': 1, 'O O O O B-LOC O O O O B-DATE O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O B-LOC O': 1, 'O O O O O O O O O B-ORG O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-DATE O': 1, 'O O O O O O O O O O O O O B-ORG O O B-LOC I-LOC O B-LOC I-LOC I-LOC O B-LOC I-LOC O': 1, 'B-PER I-PER O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O B-LOC O O O O B-DATE I-DATE O O O O O O O O O O O O O B-ORG I-ORG O': 1, 'B-PER I-PER O O O O O O O O O O O O O B-PER I-PER O': 1, 'O O O O O B-DATE O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O B-PER O B-PER O O O O O B-LOC O': 1, 'O O O O B-PER I-PER O O O O O O O O B-LOC I-LOC O O O O O O O O O O O': 1, 'O B-PER I-PER O O B-PER I-PER O O O O O O O B-LOC B-DATE O': 1, 'O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O B-LOC O B-LOC O B-DATE O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER I-PER O O O B-PER O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O O B-PER I-PER O O O O O O O B-ORG O O O O O B-PER O O O O O O O O O O': 1, 'O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O B-LOC I-LOC O O O O O O O O O B-LOC O O O': 1, 'O O O B-LOC O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O': 1, 'O O B-PER O O O O O O O O O O O B-LOC O O B-PER O O O O O B-DATE I-DATE O O O': 1, 'B-PER O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O B-DATE I-DATE O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O O O': 1, 'B-PER O O O O B-PER I-PER O O O': 1, 'B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O B-DATE O O O O O O O B-PER I-PER O O O O O B-LOC O O O O O B-PER O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O B-LOC I-LOC O': 1, 'O O B-ORG I-ORG O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O': 1, 'O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-DATE O O B-ORG I-ORG I-ORG O B-ORG O O O O B-LOC O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O': 1, 'O O O O O B-LOC B-PER I-PER O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O': 1, 'B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER I-PER O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O': 1, 'B-PER O O B-LOC O O O B-LOC O O B-LOC I-LOC O B-DATE I-DATE I-DATE O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG I-ORG I-ORG O O O O O O O O O O O': 1, 'O O O O O O O O O O B-ORG O': 1, 'O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O': 1, 'O O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-ORG I-ORG O O B-DATE O O O O O O B-DATE O': 1, 'O O O O B-LOC O O O O B-LOC O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O B-ORG I-ORG O O O O O O O O O B-DATE I-DATE O': 1, 'O B-DATE O B-PER I-PER O O O O B-PER O': 1, 'O O O O B-PER O O O O O O O O O O O O B-LOC O O O O O O B-LOC O O O O O O O O O B-LOC O O O O O B-PER O': 1, 'O O B-PER O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O O': 1, 'O O O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O O O O O O B-LOC O O O O O O O O B-LOC O': 1, 'B-ORG I-ORG O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O': 1, 'B-PER O O O O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER': 1, 'O O O O O O B-LOC O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O B-LOC I-LOC O O B-LOC O O O B-LOC B-DATE O': 1, 'O B-DATE I-DATE I-DATE O B-ORG O O O O O B-LOC I-LOC O O O O O': 1, 'O O O O O O B-PER I-PER O': 1, 'B-ORG I-ORG O O O O': 1, 'O O O O O O O O O O O O O B-PER O B-ORG O O B-DATE O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O O O O B-PER O O O O O O O B-LOC O O O O O O': 1, 'B-PER I-PER O O O O B-PER O O O O O O O O B-PER O O O O O': 1, 'O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O O O B-ORG O': 1, 'B-PER O O O O O O O O O O B-LOC O O O B-DATE I-DATE O O O O O B-LOC O O O B-ORG O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O': 1, 'O B-DATE I-DATE O O O O O B-LOC O O O O B-PER O B-LOC O O O O O O': 1, 'O O O O O B-DATE I-DATE I-DATE O B-LOC I-LOC O O O B-LOC O': 1, 'B-ORG I-ORG I-ORG I-ORG O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O B-PER O O O O O O O O O O': 1, 'B-PER I-PER O B-PER I-PER O O O B-PER O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O B-PER I-PER O B-PER I-PER O O': 1, 'O B-DATE I-DATE I-DATE O O B-ORG O O B-ORG O O O O O O O O O B-ORG O': 1, 'O O O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O': 1, 'O O O B-PER I-PER O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O B-LOC O': 1, 'O O O O B-LOC O O O O B-LOC I-LOC O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O B-ORG O B-ORG O': 1, 'O O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG O': 1, 'O B-DATE I-DATE O O O O O O O O O': 1, 'B-PER I-PER': 1, 'O O O O B-PER I-PER I-PER I-PER O O O O O O O O O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O B-PER O': 1, 'B-LOC O O O O O O B-LOC O O B-DATE I-DATE O O O O O O O B-DATE I-DATE O': 1, 'O O O O O B-PER I-PER O O O O O O O O O B-LOC O B-LOC O O O O O O O O O O O': 1, 'O O O O O B-LOC O B-LOC O O O O B-PER O': 1, 'B-PER O O O O B-PER O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC O O O O B-LOC O O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O B-PER O O B-ORG I-ORG I-ORG O O O O O O O': 1, 'O O B-PER I-PER O O O O O O O O O O O O O O O O O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O B-PER O': 1, 'O O O O O O O B-ORG O O O O O O O B-DATE O B-PER O O O O O O B-PER I-PER O O B-LOC O O O O B-PER O O O O O': 1, 'O O O B-PER O O O B-LOC O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O B-PER I-PER O O O O': 1, 'O O O O O B-ORG O B-ORG I-ORG O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O O B-ORG O O O O O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG O O O O B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O B-LOC O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O O O B-PER I-PER O B-DATE I-DATE I-DATE O O O O O O B-LOC O': 1, 'O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-PER I-PER O': 1, 'B-PER I-PER O O O B-PER O O B-PER I-PER O O B-PER I-PER O O B-PER O O': 1, 'B-LOC I-LOC O O O O O B-LOC': 1, 'O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O': 1, 'O O O O O B-PER I-PER O B-LOC I-LOC I-LOC O': 1, 'O B-DATE I-DATE O O O O O O O O O O O O O O B-DATE O B-PER O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O B-LOC O O O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O': 1, 'O O O O O B-LOC O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O': 1, 'B-DATE I-DATE I-DATE O O B-DATE I-DATE O O O O O O O O O B-DATE I-DATE O B-DATE I-DATE I-DATE O': 1, 'O B-DATE I-DATE I-DATE O O O O O O B-ORG O B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O': 1, 'B-PER I-PER O B-PER I-PER O B-PER I-PER O O O O O O O O': 1, 'O O O B-LOC B-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O': 1, 'O O O O O O O O O O O O O O O O O O O O O B-ORG O B-LOC I-LOC O O O O O O O O O O B-LOC O': 1, 'O O O O O B-PER O O O O O B-LOC O B-DATE O O O O O O O B-LOC O O': 1, 'O O B-PER I-PER O O O O B-DATE I-DATE I-DATE O O O O O B-ORG I-ORG O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O': 1, 'O O O B-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE I-DATE I-DATE O': 1, 'B-PER I-PER O O O B-LOC O': 1, 'B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-DATE I-DATE O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O B-LOC O B-LOC O O B-LOC O': 1, 'O O O O O O O O O O B-DATE O O O B-PER I-PER I-PER I-PER I-PER O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O': 1, 'O B-LOC O O O O O O O O O O O O': 1, 'O O O O O B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O': 1, 'O O O B-PER I-PER B-ORG I-ORG O B-LOC I-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC I-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-PER I-PER B-ORG I-ORG O': 1, 'O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O B-DATE I-DATE I-DATE O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O': 1, 'O B-DATE O O O O O O O O O O O O O O': 1, 'O B-DATE I-DATE O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-ORG I-ORG I-ORG O O O B-LOC O O O O O B-LOC O O O O O B-ORG I-ORG O O O B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O B-PER O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O B-ORG O B-LOC I-LOC O': 1, 'B-PER O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O O O O O B-DATE O': 1, 'O O B-PER O O O O O O O O O B-LOC I-LOC O O': 1, 'O B-LOC O O B-PER I-PER O O O O O O O O B-DATE I-DATE O O': 1, 'B-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC O B-LOC O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC O B-LOC B-DATE O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O': 1, 'O O O O O O B-PER I-PER O O O O B-LOC': 1, 'O O B-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O': 1, 'O O B-LOC O O B-LOC O O O O O O O B-PER I-PER I-PER O O O': 1, 'O O O B-ORG O O B-PER I-PER O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O': 1, 'O O B-LOC I-LOC B-PER I-PER O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O': 1, 'O B-DATE I-DATE O B-ORG O O O O O O B-PER I-PER O O O O O B-DATE I-DATE O B-PER I-PER O O O O': 1, 'O O O O O O O O O O O O B-ORG O B-PER I-PER O O O O B-ORG O B-PER I-PER O O O B-PER I-PER O B-PER I-PER O O O O B-PER O O O': 1, 'B-ORG O B-ORG O O O O O O O O O O': 1, 'O O O O O O O O O O O B-PER I-PER O B-ORG I-ORG O O O O O O O O O B-PER O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O B-LOC O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O': 1, 'B-DATE I-DATE I-DATE B-ORG O O O O O B-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC O O O': 1, 'O O O O O O O O O O O O O O O B-ORG O B-ORG O': 1, 'O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O': 1, 'O O B-DATE I-DATE I-DATE O O O O O O O O O O O B-DATE O O O O B-DATE I-DATE O O O O O O O O O O': 1, 'B-PER O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O B-LOC I-LOC O B-ORG O O B-DATE I-DATE O O B-DATE O O O O O O O B-DATE O O O B-LOC O O O O B-LOC O B-LOC O O O B-LOC O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O O O B-DATE O O O O': 1, 'O O O O O O O O O O O B-PER O': 1, 'O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O': 1, 'O O B-ORG I-ORG O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O B-LOC O O O O O O O O O O O O O': 1, 'O O O O B-LOC O B-LOC O O O B-LOC O O O O O O O O O B-LOC O O O O B-LOC O O O O O O O B-ORG O': 1, 'O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O B-PER I-PER I-PER O B-LOC O O B-PER O O O O O O B-LOC I-LOC I-LOC O O O O O O B-LOC O O': 1}\n",
      "language_distribution: {'kin': 350, 'lug': 350, 'swh': 350}\n",
      "split_distribution: {'masakhane': 1050}\n",
      "\n",
      "--- EVAL Dataset ---\n",
      "Shape: (225, 4)\n",
      "\n",
      "Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 225 entries, 175 to 109\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      225 non-null    object\n",
      " 1   label     225 non-null    object\n",
      " 2   language  225 non-null    object\n",
      " 3   split     225 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 8.8+ KB\n",
      "None\n",
      "\n",
      "Sample Data:\n",
      "                                                  text  \\\n",
      "175  Abawagizi be baasigadde bamutenda nti musajja ...   \n",
      "107  Minisiteri y’Imari n’Igenamigambi yakoze gahun...   \n",
      "6    Utafiti huu pia umezingatia mauaji mikononi mw...   \n",
      "151  Ono yasabye poliisi okukozesa kamera ezaateeke...   \n",
      "94   3 zifite agaciro k’amafaranga asaga miliyari 2...   \n",
      "\n",
      "                                                 label language      split  \n",
      "175                                O O O O O O O O O O      lug  masakhane  \n",
      "107  B-ORG I-ORG I-ORG O O O O O O B-LOC O O O O O ...      kin  masakhane  \n",
      "6    O O O O O O O O O O O O O O O O O O O O O O O ...      swh  masakhane  \n",
      "151                        O O O O O O O O O O O O O O      lug  masakhane  \n",
      "94   O O O O O O O O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
      "\n",
      "Dataset Statistics:\n",
      "num_samples: 225\n",
      "avg_text_length: 160.05777777777777\n",
      "num_classes: 187\n",
      "class_distribution: {'O O O O O O O O O O': 6, 'O O O O O O O O O O O': 5, 'O O O': 4, 'O O O O O O O O O O O O O O O O O': 4, 'O O O O O O O O O O O O': 4, 'O O O O O O O O O O O O O O': 4, 'O O O O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O': 2, 'O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O': 2, 'O B-PER O O O O O O O O O O O': 2, 'O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O': 2, 'O O O O O O O O O': 2, 'B-PER O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O B-LOC O O O O': 1, 'O O O O O O O B-LOC O O B-PER I-PER I-PER I-PER O O O O O O B-ORG O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG O O O O O O O O': 1, 'O O O B-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER O B-PER O O B-PER O B-PER O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O B-DATE I-DATE I-DATE O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-PER I-PER O O O O O O O O O O O O O B-DATE O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O O B-DATE O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG O O O O B-PER I-PER O O O B-PER O O O O O O O O O': 1, 'O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG O O B-ORG I-ORG I-ORG O O O B-LOC I-LOC I-LOC O O O B-LOC I-LOC O O B-ORG I-ORG O O B-ORG I-ORG O': 1, 'O O B-DATE I-DATE I-DATE O B-PER O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC B-PER I-PER O O O O B-PER I-PER O B-LOC O O O B-LOC O O O O O O O O O O O O O': 1, 'O O B-LOC O B-PER I-PER I-PER I-PER O B-DATE O O O O O O B-LOC I-LOC O O O O O O O O O': 1, 'O O B-ORG O O O O B-ORG O O B-PER O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O': 1, 'O O O O O B-PER I-PER O O B-PER O O O O O O O B-LOC I-LOC O O O O O B-PER O O O O': 1, 'O O O O O O O B-LOC O O B-PER': 1, 'O O O O O O B-LOC O O O O O B-PER O': 1, 'B-ORG I-ORG O O O O O O O B-LOC O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O B-LOC I-LOC O B-LOC O O B-LOC I-LOC O O O O O B-ORG I-ORG I-ORG I-ORG O O O B-DATE O': 1, 'B-PER O O O O O O O O O O O O O O O': 1, 'B-PER O O B-LOC O O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O O B-DATE I-DATE O O O O O O O O O O O O O': 1, 'O O O O O O O B-DATE O O O O O O O O O O B-LOC O O O O O O O O O O O O O': 1, 'O B-PER I-PER O B-PER I-PER I-PER I-PER I-PER O O O O O B-LOC O B-LOC O O O O O O O O O': 1, 'O O O O O O B-DATE I-DATE O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O O O O O B-LOC I-LOC I-LOC O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O B-LOC O O': 1, 'O O O O O O O B-LOC B-PER I-PER O O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-LOC B-PER I-PER O O O O O O O O O B-LOC O B-LOC O B-LOC O': 1, 'B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O B-DATE I-DATE O O O O O O': 1, 'O O B-DATE I-DATE O B-DATE I-DATE O O B-DATE I-DATE O B-DATE I-DATE O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O B-PER O O O O O O O O B-LOC O': 1, 'O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O': 1, 'O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O': 1, 'B-ORG I-ORG I-ORG O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O O B-PER I-PER O O O O B-PER I-PER O O B-ORG I-ORG O B-PER I-PER O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG O O O O O O O O O O O O B-ORG I-ORG O': 1, 'O B-ORG I-ORG I-ORG O B-PER O O O O O O O O O O O O O O O O': 1, 'O O B-ORG O O O O O O O O O B-ORG O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O B-PER I-PER O O B-ORG O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O B-ORG O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-ORG O B-PER O O O O O O O O O O O O O O O O': 1, 'O O O O B-DATE I-DATE I-DATE O O O O O O O': 1, 'O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'B-PER O O O O O O O O O O B-ORG O O O O O O O O O': 1, 'O B-PER O O O O O O O B-PER O O O O O O O O B-LOC O B-LOC O': 1, 'O O O O O O O O O O O O O O O B-LOC O O': 1, 'O O B-PER O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG O O O O O O O O B-DATE O O O B-LOC O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O': 1, 'O O O O O B-DATE O O O O O O O O B-LOC O O O O O B-LOC O B-LOC O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O B-PER I-PER O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC B-DATE O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O': 1, 'O O B-PER O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O B-LOC O O B-DATE O O B-PER I-PER O O O O O B-ORG I-ORG O O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O B-PER I-PER O': 1, 'B-PER O O B-PER O O O O O O O O O O O B-PER O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-DATE I-DATE I-DATE O': 1, 'O O O O O O O B-ORG I-ORG O O O O O O B-LOC I-LOC O O O O O B-PER I-PER O O O B-LOC O O O O O O B-ORG I-ORG O O O O O O O O B-PER I-PER O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER I-PER O B-LOC I-LOC I-LOC O': 1, 'O B-ORG I-ORG I-ORG B-PER I-PER I-PER O O O O O O O O O O O O O': 1, 'O O O B-ORG O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O': 1, 'B-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O B-DATE I-DATE O B-LOC I-LOC I-LOC O O O B-LOC O': 1, 'O O O B-LOC O O O O O': 1, 'O B-PER O O O O O O O O O B-PER O O O O B-PER O O O O O O O O O O B-PER O': 1, 'O B-PER I-PER O O O O O B-LOC O O O O O O O O O O B-PER O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE I-DATE I-DATE I-DATE O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG I-ORG O B-ORG I-ORG O O B-LOC O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O B-LOC O O O O O O O B-LOC O B-LOC O B-LOC O O O O O O O B-ORG I-ORG O': 1, 'O O B-ORG O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O B-LOC O B-LOC O': 1, 'O O O B-ORG O O O O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O O O': 1, 'O O B-DATE O O O O B-LOC O O O O B-LOC O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-ORG O': 1, 'O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O O O O O O O O O O O O O B-DATE I-DATE O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC O': 1, 'O O B-ORG O O O O O O O O O B-LOC O B-LOC O O B-LOC O B-LOC O': 1, 'O O O B-DATE O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O O O': 1, 'O O B-LOC O O O O O O O O O O O O B-LOC O B-LOC O O O O O O O O B-LOC O': 1, 'O O O B-DATE I-DATE I-DATE I-DATE O': 1, 'B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O O O O B-LOC O O O O O O O O O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O O O B-ORG I-ORG O B-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O B-LOC I-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O O O O O O O': 1, 'O O O O O O O O O B-PER I-PER O O O O O O O O O': 1, 'B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'O O B-ORG I-ORG I-ORG I-ORG O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-LOC O O O O O B-LOC O O O O O O O B-DATE I-DATE O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O B-LOC O B-LOC O B-LOC O O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O B-DATE I-DATE I-DATE O O O O B-LOC O O B-DATE I-DATE I-DATE I-DATE O O O O O O O B-DATE O O O O O O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O B-LOC O O B-ORG I-ORG I-ORG O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O B-LOC O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O B-LOC O B-LOC I-LOC O B-LOC O': 1, 'O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-LOC O O O O O O O B-LOC I-LOC I-LOC O O O O B-LOC O B-LOC O O O O B-LOC O B-LOC O O O O B-LOC O O': 1, 'O O O O O O O O O O O B-DATE O O O O O O B-LOC O O O B-LOC O': 1, 'B-DATE I-DATE I-DATE O O O O O O O O O B-LOC O O O O O O': 1, 'O B-LOC O O O': 1, 'O O O O B-LOC B-PER I-PER O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O': 1, 'O O O O O O O O O O O O O B-LOC O O O O O O B-DATE I-DATE O B-DATE O': 1, 'O O O O B-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER O B-PER O O O B-ORG O O O O O O O O O O B-ORG O B-PER O O O O': 1, 'O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O O B-ORG O': 1, 'O O O O O O B-ORG B-PER I-PER I-PER O O B-DATE O O O O O O O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O O O O O B-PER O B-PER O B-PER O O O O O O B-PER O O O O O O O O': 1, 'B-ORG I-ORG I-ORG I-ORG I-ORG O B-DATE O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O B-PER I-PER O O O B-PER I-PER O O O B-ORG I-ORG O O O': 1, 'O O O B-ORG O O O O O O B-ORG O O O O': 1, 'O O O O O O O O O B-LOC O O B-ORG I-ORG O B-ORG O B-ORG O': 1, 'O O O O O O O B-DATE I-DATE I-DATE I-DATE O O O O O O O': 1, 'O O B-ORG O B-PER I-PER O O B-DATE I-DATE O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O': 1, 'O O O O O O O B-LOC O O O O O': 1, 'O O O O O O O O O O B-DATE O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O': 1, 'O O O B-PER B-DATE I-DATE I-DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O B-PER O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O O O O B-LOC O O O O O B-PER I-PER O O O B-PER I-PER I-PER O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O B-LOC O B-LOC O': 1, 'B-PER O O O O O B-LOC O O O O B-PER O O O B-ORG O O O O O B-PER O O O B-PER O O O O': 1, 'O O O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O B-LOC O O B-LOC O': 1, 'O O B-PER O O B-DATE I-DATE O O O O O O': 1, 'O O B-PER O B-LOC O O O O O O O O O': 1}\n",
      "language_distribution: {'lug': 75, 'kin': 75, 'swh': 75}\n",
      "split_distribution: {'masakhane': 225}\n",
      "\n",
      "--- BENCHMARK Dataset ---\n",
      "Shape: (225, 4)\n",
      "\n",
      "Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 225 entries, 175 to 109\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      225 non-null    object\n",
      " 1   label     225 non-null    object\n",
      " 2   language  225 non-null    object\n",
      " 3   split     225 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 8.8+ KB\n",
      "None\n",
      "\n",
      "Sample Data:\n",
      "                                                  text  \\\n",
      "175  Okwongera okugaziya akatale ka kaawa mu mmwaan...   \n",
      "107  Abantu benshi bakunze kwibaza ikibazo kijyanye...   \n",
      "6    Alikuwa mtu wa kwanza kuongoza benki huku akiw...   \n",
      "151  Ababiri bano balina endowooza zebyobufuzi ezen...   \n",
      "94   Buri nyubako ( apartment ) muri izo zari zigiz...   \n",
      "\n",
      "                                                 label language      split  \n",
      "175          O O O O O O O O B-LOC O O O O O O O O O O      lug  masakhane  \n",
      "107  O O O O O O O O O O O O O O O O O O O O O O O ...      kin  masakhane  \n",
      "6    O O O O O O O O O O O O O O O O O O O O O O O ...      swh  masakhane  \n",
      "151                                      O O O O O O O      lug  masakhane  \n",
      "94         O O O O O O O O O O B-LOC I-LOC O O O O O O      kin  masakhane  \n",
      "\n",
      "Dataset Statistics:\n",
      "num_samples: 225\n",
      "avg_text_length: 149.91555555555556\n",
      "num_classes: 196\n",
      "class_distribution: {'O O O O O O O O O': 8, 'O O O O O O O O O O O O O O O O O O O O': 4, 'O O O O O O O': 4, 'O O O O O O O O O O': 4, 'O O O O O O O O': 3, 'O O O O O O O O O O O': 3, 'O O O O O O O O O O O O O O O O O O O O O O': 3, 'O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 2, 'B-PER O O O O O O O O O O O O O O': 2, 'O O O O O O O O O O O O': 2, 'B-DATE B-LOC I-LOC O O O O O O O B-PER O O O O O O': 1, 'O B-PER O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-ORG O O O O O B-ORG I-ORG O O B-LOC O O B-LOC I-LOC O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O': 1, 'O O B-DATE I-DATE I-DATE O B-ORG I-ORG I-ORG O B-ORG O O O O O O B-ORG O B-ORG O O O O O O O': 1, 'O O O O O O B-PER O O O O O O B-DATE O O O B-LOC B-PER I-PER O O O O O B-LOC B-PER I-PER O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O B-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O': 1, 'O O O O B-DATE I-DATE I-DATE I-DATE I-DATE I-DATE I-DATE O': 1, 'B-PER O B-PER O O O B-DATE O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O B-DATE I-DATE O B-DATE I-DATE O O B-DATE I-DATE O B-DATE I-DATE O O O O O O O O O O O O O O O': 1, 'B-LOC O O O O O O O O O O': 1, 'O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O B-LOC O O O B-LOC O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-LOC O': 1, 'O O B-ORG I-ORG O B-PER I-PER O O O O O O O O O B-LOC O O B-LOC O O O B-LOC O': 1, 'O O O O B-PER I-PER O B-PER I-PER B-ORG I-ORG O O O B-DATE I-DATE I-DATE O': 1, 'B-PER O O O O O O O B-PER O O O O O O': 1, 'B-PER O O O O O O': 1, 'O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O B-LOC I-LOC O': 1, 'O O O O O O O O O B-DATE I-DATE O O O B-DATE I-DATE O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-DATE O O O': 1, 'O O B-LOC O B-DATE I-DATE I-DATE I-DATE O O O O B-PER I-PER O B-PER I-PER O O O O O O O O O O B-PER O O O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-DATE I-DATE I-DATE I-DATE O O O O B-PER I-PER O B-PER I-PER O O O O O O O B-PER O O O O O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O': 1, 'B-PER I-PER O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-DATE O O O O O O B-ORG O O O O B-DATE I-DATE O B-ORG O B-ORG O O O O O O B-ORG O B-ORG O B-ORG O O O O O O O O': 1, 'O O O B-PER I-PER O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O': 1, 'B-PER O B-PER O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-LOC B-DATE O O O O O O O B-LOC I-LOC O B-PER O O O B-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O B-PER I-PER O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O': 1, 'O O B-ORG O O O B-PER O O O O O O O O O O O O O O O': 1, 'O O O B-DATE O O O O O O O O O O O O': 1, 'O O O B-DATE O B-ORG I-ORG O B-ORG I-ORG O B-LOC O B-ORG I-ORG O B-ORG I-ORG O B-LOC I-LOC I-LOC I-LOC O': 1, 'B-PER O O O O O B-DATE O O O O B-LOC O O B-LOC O O O O': 1, 'O O O O O O B-LOC O B-LOC O B-LOC O': 1, 'B-PER I-PER O O O B-LOC I-LOC I-LOC O O O O B-PER I-PER O B-PER I-PER O O O O O': 1, 'O O O O O O O O O O O O O O O O O B-PER O B-PER O B-PER O B-PER O O O O': 1, 'O B-LOC I-LOC O O B-LOC O O O': 1, 'O O B-ORG I-ORG O B-PER I-PER O O O B-ORG I-ORG O O O O O O O O O O B-LOC O O O O O O O O B-ORG I-ORG O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O': 1, 'O O B-PER O O O O B-PER O B-PER I-PER I-PER O O O B-PER O O O O O O O B-PER O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-PER O': 1, 'O O O O O O O O O O B-LOC I-LOC O O O O O O': 1, 'O O O O B-ORG O O O O O O B-PER I-PER O O O B-ORG O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O B-PER O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O B-DATE O B-DATE O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-DATE I-DATE O': 1, 'B-DATE I-DATE O O O O O O O O B-PER O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-DATE O O O O B-ORG O O O O O O O O O B-ORG O O O O O O O O O O O': 1, 'B-PER O O O O O B-DATE O': 1, 'O O O B-DATE O O O O O': 1, 'B-PER O B-PER I-PER I-PER O O O O O O B-ORG I-ORG I-ORG O B-LOC O': 1, 'O B-DATE O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O B-PER I-PER O B-DATE O B-LOC O O O O O O O O O O O O': 1, 'O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O': 1, 'B-ORG O O O O O O B-DATE O O O O O O O B-DATE I-DATE O': 1, 'O O O O O O B-LOC O O O O O O O O O O O B-LOC O O O O O O O O O B-PER O O O O O O O': 1, 'O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O O O O B-PER O O O': 1, 'O O O O O O O O O B-LOC O O O O B-PER I-PER I-PER I-PER O O O O O O O O O O O O O B-LOC O O O O O O O O O O O': 1, 'B-LOC O O O O B-LOC O O O O B-LOC O O O B-LOC O O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER I-PER B-ORG I-ORG O B-LOC O B-PER I-PER B-ORG I-ORG I-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG O B-LOC O B-PER I-PER B-ORG I-ORG O B-LOC O': 1, 'O O O B-PER O O O O O O B-LOC O O O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O B-LOC O': 1, 'O O O O B-LOC O O O B-PER I-PER I-PER O B-PER I-PER O O O O B-LOC O O O O O O B-LOC O O O B-LOC O O O O': 1, 'O B-ORG I-ORG O B-ORG I-ORG O B-ORG I-ORG O B-ORG I-ORG O O O O O O': 1, 'O O O O O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O': 1, 'O O O O O O O O O O O B-ORG I-ORG O': 1, 'O O O B-PER O O O B-DATE O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG I-ORG I-ORG O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-ORG I-ORG O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O': 1, 'O O O B-LOC I-LOC O O O O O O B-LOC': 1, 'B-PER O O O O O O O O O O O O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O B-PER I-PER O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'B-DATE B-LOC O O O O B-LOC B-DATE I-DATE I-DATE I-DATE I-DATE O O B-DATE I-DATE O O B-LOC O': 1, 'O O O O O B-ORG O O O O O O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O B-PER I-PER O O O O O': 1, 'O O O O O B-LOC O O O O O O O O O B-ORG O O O O O O O O O O': 1, 'O O O B-DATE I-DATE O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-DATE I-DATE O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O O': 1, 'O O O O B-LOC O O O O O B-PER I-PER I-PER O O O B-ORG O B-LOC O B-PER I-PER I-PER O O': 1, 'O O O O O O O O O O O O O B-DATE I-DATE O O O': 1, 'O O O O O O O O O O O O O O O O O B-PER O O O O O O O O O B-LOC O O B-ORG I-ORG I-ORG O B-DATE I-DATE O': 1, 'B-PER I-PER O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O': 1, 'O B-PER O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-LOC O O B-ORG I-ORG O B-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG O B-ORG O B-ORG I-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-LOC O B-LOC O B-LOC O': 1, 'O O B-ORG I-ORG O O O O O O O O O O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O B-DATE I-DATE I-DATE O O O B-PER O O O O O O O O O O O': 1, 'O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O B-ORG O B-PER I-PER O B-DATE O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O B-LOC I-LOC O O O O O O B-ORG O O O O B-PER O O O O': 1, 'O O B-LOC B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O': 1, 'B-PER O O O B-DATE O O B-PER I-PER I-PER O O O O O O O O O O O O O B-PER I-PER I-PER O': 1, 'O O O O O B-DATE I-DATE O': 1, 'O O O O O O O O B-LOC O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O B-LOC O O O O O O O O B-PER O O O O O O O B-ORG O': 1, 'O O O O': 1, 'O O O O O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O O B-LOC O O O B-LOC O O O': 1, 'O O O O O B-LOC O O O O O O O O': 1, 'O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-PER O O O O O O O': 1, 'O O O B-DATE I-DATE I-DATE O O O': 1, 'O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O B-PER O B-LOC O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O B-LOC O B-LOC O B-LOC O O O O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O B-LOC O O O O O O': 1, 'O O O O O O O O B-PER I-PER I-PER O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O B-LOC O O O O O O O B-LOC O B-LOC O O O O B-LOC O O O B-LOC O O O O B-LOC O O O': 1, 'O O O O O B-PER O O O O O O O O O O': 1, 'O O O O O O B-DATE O B-PER I-PER O O O O O O B-LOC O B-LOC O O B-DATE I-DATE I-DATE O O O O O O B-LOC O B-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O': 1, 'B-PER O B-LOC O B-LOC I-LOC O B-DATE I-DATE I-DATE I-DATE O O O O O O B-PER O O O O O O O O O O B-LOC O O O B-LOC O': 1, 'O O O O O B-PER I-PER': 1, 'O O O O O O O O O O O O O O O O O': 1, 'B-PER I-PER O O B-LOC O O B-ORG I-ORG I-ORG O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O': 1, 'B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O B-LOC I-LOC O B-PER I-PER I-PER I-PER O O O O O O B-PER O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O': 1, 'O O O O B-PER O O O O O O O O O B-PER I-PER O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-PER O O O O O O B-ORG O O O O O O O O': 1, 'B-LOC I-LOC O O O O O O O O O': 1, 'O O O O O B-ORG O O O O O O O O O O O': 1, 'B-LOC O O O O O O B-PER I-PER O O O': 1, 'O O O B-LOC O O B-PER I-PER O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O': 1, 'B-LOC I-LOC I-LOC': 1, 'O O O O O O O O O O O B-ORG I-ORG I-ORG O O O B-ORG I-ORG O O O O O O O O': 1, 'B-LOC I-LOC O O O O B-LOC I-LOC O B-LOC O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O B-LOC I-LOC I-LOC I-LOC I-LOC O B-LOC O B-LOC I-LOC O B-LOC I-LOC O B-LOC I-LOC O': 1, 'O O B-ORG O B-LOC O B-PER I-PER O O O O O O O O O O O O O O': 1, 'O O O B-PER O O O O O O O O B-LOC O O B-PER O O O O O O O O O O B-LOC O': 1, 'O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O B-LOC B-ORG I-ORG I-ORG O O O O O': 1, 'O B-PER O O O O O O O O O B-LOC O B-LOC O O O O O': 1, 'B-PER O B-PER O O O O O O O O O O O O O B-DATE I-DATE O': 1, 'B-DATE I-DATE O O B-PER O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O B-ORG O O O O O O O O O O O B-DATE I-DATE I-DATE O O O O O O O O': 1, 'B-PER O O O O O O O O O O B-LOC I-LOC B-ORG O O O O O O O O O O O O O O O O O O O O O O B-DATE O': 1, 'O O O O O O O O O O O B-LOC O O O O O O O O O O O O O O O O B-DATE O O O': 1, 'B-PER O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'O O O O O O O O B-LOC O O O O': 1, 'O O O O O B-PER O O O O O O': 1, 'O O O O B-PER I-PER O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O B-DATE O': 1, 'O B-ORG I-ORG O O O O O O O O': 1, 'O O O O O O O B-DATE O O O O O O O O': 1, 'O O O B-LOC I-LOC O O O B-PER O O O O O O B-LOC O O O O B-ORG I-ORG I-ORG O O B-DATE O O': 1, 'O B-LOC I-LOC O B-DATE I-DATE O O O O O O B-PER I-PER O B-PER I-PER I-PER O': 1, 'O O O O O B-PER O O B-ORG O O O O O O O O O O O B-PER O O O O O O': 1, 'O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O B-LOC O O O': 1, 'O O O O O O O O O B-ORG O': 1, 'O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O B-LOC O O O O O O O O O': 1, 'B-PER O O O O O B-LOC O O O O O O O O O O O O O O O B-PER I-PER O O O B-LOC I-LOC I-LOC O': 1, 'O O O O O O B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O B-ORG O O O O O O O O O O O O O O O O O O O': 1, 'O O O O O B-PER I-PER O O O O O O O O O O': 1, 'B-PER O O O O O O O O B-LOC O B-PER O O O B-PER O O O O O O O O O O O O': 1, 'O O B-LOC O B-PER I-PER O O O O B-ORG I-ORG I-ORG O B-LOC O O O O O': 1, 'B-ORG O O O O O B-LOC O O B-LOC': 1, 'O O O O O O O O O O O O O O B-PER O O O O O': 1, 'O B-PER O O O O B-LOC O O O O O O O O O O O O O O O O O': 1, 'O O O O O O O O O O O O O O O O O O O O O O O O O O': 1, 'O O B-DATE O O': 1}\n",
      "language_distribution: {'lug': 75, 'kin': 75, 'swh': 75}\n",
      "split_distribution: {'masakhane': 225}\n",
      "\n",
      "--- CLASS_WEIGHTS Dataset ---\n",
      "O: <class 'float'>\n",
      "B-PER: <class 'float'>\n",
      "I-PER: <class 'float'>\n",
      "B-DATE: <class 'float'>\n",
      "B-ORG: <class 'float'>\n",
      "I-ORG: <class 'float'>\n",
      "B-LOC: <class 'float'>\n",
      "I-DATE: <class 'float'>\n",
      "I-LOC: <class 'float'>\n",
      "\n",
      "--- EXPERIMENTAL Dataset ---\n",
      "\n",
      "--- EXPERIMENTAL - ZERO_SHOT ---\n",
      "Shape: (5, 4)\n",
      "\n",
      "Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      5 non-null      object\n",
      " 1   label     5 non-null      object\n",
      " 2   language  5 non-null      object\n",
      " 3   split     5 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 288.0+ bytes\n",
      "None\n",
      "\n",
      "Sample Data:\n",
      "                                                text  \\\n",
      "0            John Smith visited New York last summer   \n",
      "1       Apple Inc. announced a new product yesterday   \n",
      "2  The conference will be held in London on May 15th   \n",
      "3       Maria Garcia joined Microsoft as CEO in 2022   \n",
      "4  The Eiffel Tower in Paris attracts millions of...   \n",
      "\n",
      "                                    label language      split  \n",
      "0  B-PER I-PER O O B-LOC I-LOC O B-DATE O      eng  zero_shot  \n",
      "1            B-ORG I-ORG O O O O O B-DATE      eng  zero_shot  \n",
      "2       O O O O O O B-LOC O B-DATE I-DATE      eng  zero_shot  \n",
      "3        B-PER I-PER O B-ORG O O O B-DATE      eng  zero_shot  \n",
      "4           O B-LOC I-LOC O B-LOC O O O O      eng  zero_shot  \n",
      "\n",
      "Dataset Statistics:\n",
      "num_samples: 5\n",
      "avg_text_length: 46.2\n",
      "num_classes: 5\n",
      "class_distribution: {'B-PER I-PER O O B-LOC I-LOC O B-DATE O': 1, 'B-ORG I-ORG O O O O O B-DATE': 1, 'O O O O O O B-LOC O B-DATE I-DATE': 1, 'B-PER I-PER O B-ORG O O O B-DATE': 1, 'O B-LOC I-LOC O B-LOC O O O O': 1}\n",
      "language_distribution: {'eng': 5}\n",
      "split_distribution: {'zero_shot': 5}\n",
      "\n",
      "--- EXPERIMENTAL - CODE_SWITCH ---\n",
      "Shape: (5, 4)\n",
      "\n",
      "Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      5 non-null      object\n",
      " 1   label     5 non-null      object\n",
      " 2   language  5 non-null      object\n",
      " 3   split     5 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 288.0+ bytes\n",
      "None\n",
      "\n",
      "Sample Data:\n",
      "                                                text  \\\n",
      "0    I want to fly from Nairobi to Nueva York mañana   \n",
      "1  Reserva un vuelo from Nairobi to London por favor   \n",
      "2      Book a flight desde Nairobi a París next week   \n",
      "3             Quiero un hotel en Tokyo para mi viaje   \n",
      "4              Find me a restaurante in Berlin bitte   \n",
      "\n",
      "                                  label language      split  \n",
      "0  O O O O O B-LOC O B-LOC I-LOC B-DATE      eng  zero_shot  \n",
      "1             O O O O B-LOC O B-LOC O O      eng  zero_shot  \n",
      "2   O O O O B-LOC O B-LOC B-DATE I-DATE      eng  zero_shot  \n",
      "3                   O O O O B-LOC O O O      eng  zero_shot  \n",
      "4                     O O O O O B-LOC O      eng  zero_shot  \n",
      "\n",
      "Dataset Statistics:\n",
      "num_samples: 5\n",
      "avg_text_length: 43.2\n",
      "num_classes: 5\n",
      "class_distribution: {'O O O O O B-LOC O B-LOC I-LOC B-DATE': 1, 'O O O O B-LOC O B-LOC O O': 1, 'O O O O B-LOC O B-LOC B-DATE I-DATE': 1, 'O O O O B-LOC O O O': 1, 'O O O O O B-LOC O': 1}\n",
      "language_distribution: {'eng': 5}\n",
      "split_distribution: {'zero_shot': 5}\n"
     ]
    }
   ],
   "source": [
    "# Print dataset information\n",
    "data_loader.print_dataset_info(stratified_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of stratified_datasets:\n",
      "train: DataFrame(shape=(1050, 4))\n",
      "eval: DataFrame(shape=(225, 4))\n",
      "benchmark: DataFrame(shape=(225, 4))\n",
      "class_weights:   O: float(0.12834500106070248)\n",
      "  B-PER: float(3.9158576051779934)\n",
      "  I-PER: float(6.505376344086022)\n",
      "  B-DATE: float(9.020234291799786)\n",
      "  B-ORG: float(7.469135802469136)\n",
      "  I-ORG: float(8.504016064257028)\n",
      "  B-LOC: float(3.8622891016871863)\n",
      "  I-DATE: float(9.317931793179318)\n",
      "  I-LOC: float(14.116666666666667)\n",
      "experimental:   zero_shot: DataFrame(shape=(5, 4))\n",
      "  code_switch: DataFrame(shape=(5, 4))\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of stratified_datasets\n",
    "print(\"Structure of stratified_datasets:\")\n",
    "print(data_loader.inspect_dataset_structure(stratified_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess datasets\n",
    "preprocessed_datasets = {\n",
    "    key: data_loader.preprocess_dataset(dataset)\n",
    "    for key, dataset in stratified_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure of preprocessed_datasets:\n",
      "train: DataFrame(shape=(1050, 4))\n",
      "eval: DataFrame(shape=(225, 4))\n",
      "benchmark: DataFrame(shape=(225, 4))\n",
      "class_weights:   O: str(0.12835)\n",
      "  B-PER: str(3.91586)\n",
      "  I-PER: str(6.50538)\n",
      "  B-DATE: str(9.02023)\n",
      "  B-ORG: str(7.46914)\n",
      "  I-ORG: str(8.50402)\n",
      "  B-LOC: str(3.86229)\n",
      "  I-DATE: str(9.31793)\n",
      "  I-LOC: str(14.11667)\n",
      "experimental:   zero_shot: DataFrame(shape=(5, 4))\n",
      "  code_switch: DataFrame(shape=(5, 4))\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of preprocessed_datasets\n",
    "print(\"\\nStructure of preprocessed_datasets:\")\n",
    "print(data_loader.inspect_dataset_structure(preprocessed_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /workspace/Msc-FYP/py/models to Python path\n"
     ]
    }
   ],
   "source": [
    "# Check if the models directory is in the Python path\n",
    "models_dir = os.path.abspath(os.path.join('..', 'py', 'models'))\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.append(models_dir)\n",
    "    print(f\"Added {models_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models directory contents: ['__pycache__', 'afro_xlmr_large.py', 'ernie_m.py', '.ipynb_checkpoints', 'llama2_decoder.py']\n"
     ]
    }
   ],
   "source": [
    "# Print contents of the models directory\n",
    "print(f\"Models directory contents: {os.listdir(models_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models, tokenizers = {}, {}\n",
    "num_labels = len(preprocessed_datasets['train']['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: afro-xlmr-large\n",
      "Cache directory: /workspace/Msc-FYP/ipynb/model_cache\n",
      "Auth token: hf_EF...Ulmqr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized afro-xlmr-large\n",
      "Initializing model: meta-llama/Llama-2-7b-hf\n",
      "Cache directory: /workspace/Msc-FYP/ipynb/model_cache\n",
      "Auth token: hf_EF...Ulmqr\n",
      "Initializing Llama2Decoder with:\n",
      "  model_name: meta-llama/Llama-2-7b-hf\n",
      "  auth_token: hf_EF...Ulmqr\n",
      "  cache_dir: /workspace/Msc-FYP/ipynb/model_cache\n",
      "Set cache directory to: /workspace/Msc-FYP/ipynb/model_cache\n",
      "Model meta-llama/Llama-2-7b-hf not found in cache. Will attempt to download.\n",
      "Initializing tokenizer from meta-llama/Llama-2-7b-hf...\n",
      "Tokenizer parameters:\n",
      "  pretrained_model_name_or_path: meta-llama/Llama-2-7b-hf\n",
      "  use_auth_token: True\n",
      "  cache_dir: /workspace/Msc-FYP/ipynb/model_cache\n",
      "  local_files_only: False\n",
      "Initializing model from meta-llama/Llama-2-7b-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e144f9b8f1524ebbabc5e155e81706e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized meta-llama/Llama-2-7b-hf\n",
      "Model device: cuda:0\n",
      "Successfully initialized meta-llama/Llama-2-7b-hf\n"
     ]
    }
   ],
   "source": [
    "# Initialize models with the configured parameters and authentication token\n",
    "for model_name in config['model']['names']:\n",
    "    print(f\"Initializing model: {model_name}\")\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    print(f\"Auth token: {auth_token[:5]}...{auth_token[-5:] if auth_token else None}\")\n",
    "    try:\n",
    "        if model_name == \"meta-llama/Llama-2-7b-hf\":\n",
    "            llama_model = Llama2Decoder(model_name, auth_token=auth_token, cache_dir=cache_dir)\n",
    "            models[model_name] = llama_model.get_model()   # The model is already on the appropriate device\n",
    "            tokenizers[model_name] = llama_model.get_tokenizer()\n",
    "        else:\n",
    "            model, tokenizer = get_model(model_name, num_labels=num_labels, auth_token=auth_token, cache_dir=cache_dir)\n",
    "            models[model_name] = model.to('cuda') # Ensure Afro XLMR is on GPU\n",
    "            tokenizers[model_name] = tokenizer\n",
    "\n",
    "        if models[model_name] is not None and tokenizers[model_name] is not None:\n",
    "            print(f\"Successfully initialized {model_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to initialize {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing {model_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['afro-xlmr-large', 'meta-llama/Llama-2-7b-hf']\n",
      "Available tokenizers: ['afro-xlmr-large', 'meta-llama/Llama-2-7b-hf']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available models:\", list(models.keys()))\n",
    "print(\"Available tokenizers:\", list(tokenizers.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating datasets for combined Afro-XLMR and LLaMA model with model type: encoder_decoder\n",
      "Initialized CustomDataset with 1050 samples\n",
      "Number of unique labels: 9\n",
      "Model type: encoder_decoder\n",
      "Initialized CustomDataset with 225 samples\n",
      "Number of unique labels: 9\n",
      "Model type: encoder_decoder\n",
      "Initialized CustomDataset with 225 samples\n",
      "Number of unique labels: 9\n",
      "Model type: encoder_decoder\n"
     ]
    }
   ],
   "source": [
    "# Create custom datasets for PyTorch\n",
    "datasets = {}\n",
    "\n",
    "model_type = 'encoder_decoder'  # New model type for the combined model\n",
    "\n",
    "# For the combined Afro-XLMR and LLaMA model\n",
    "combined_tokenizer = tokenizers['afro-xlmr-large']  # Assuming you're using the Afro-XLMR tokenizer for the combined model\n",
    "\n",
    "logging.info(f\"Creating datasets for combined Afro-XLMR and LLaMA model with model type: {model_type}\")\n",
    "\n",
    "# Use the key 'combined_afro_xlmr_llama'\n",
    "datasets['combined_afro_xlmr_llama'] = {\n",
    "    'train': CustomDataset(preprocessed_datasets['train'], combined_tokenizer, model_type=model_type),\n",
    "    'eval': CustomDataset(preprocessed_datasets['eval'], combined_tokenizer, model_type=model_type),\n",
    "    'benchmark': CustomDataset(preprocessed_datasets['benchmark'], combined_tokenizer, model_type=model_type)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "zero_shot_classifier = ZeroShotClassifier(\n",
    "    encoder=models['afro-xlmr-large'],\n",
    "    decoder=models['meta-llama/Llama-2-7b-hf'],\n",
    "    tokenizer=combined_tokenizer\n",
    ")\n",
    "\n",
    "code_switch_classifier = CodeSwitchClassifier(\n",
    "    encoder=models['afro-xlmr-large'],\n",
    "    decoder=models['meta-llama/Llama-2-7b-hf'],\n",
    "    tokenizer=combined_tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combined_afro_xlmr_llama': {'train': <utils.CustomDataset object at 0x7f93689e5120>, 'eval': <utils.CustomDataset object at 0x7f93689e4d00>, 'benchmark': <utils.CustomDataset object at 0x7f8f272f3e20>}}\n"
     ]
    }
   ],
   "source": [
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: dict_keys(['combined_afro_xlmr_llama'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Available datasets:\", datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in 'combined_afro_xlmr_llama': dict_keys(['train', 'eval', 'benchmark'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys in 'combined_afro_xlmr_llama':\", datasets['combined_afro_xlmr_llama'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: <utils.CustomDataset object at 0x7f93689e5120>\n",
      "Eval dataset: <utils.CustomDataset object at 0x7f93689e4d00>\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset:\", datasets['combined_afro_xlmr_llama']['train'])\n",
    "print(\"Eval dataset:\", datasets['combined_afro_xlmr_llama']['eval'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df37e869929a41a49ce2133c31eddfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.9.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../root/.cache/huggingface/hub/models--masakhane--africomet-mtl/snapshots/91a7e56061446598665d569d89b762387399e3ac/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluators\n",
    "evaluators = {\n",
    "    'combined_afro_xlmr_llama': AfriCOMETEvaluator(\n",
    "        model=None,  # AfriCOMETEvaluator doesn't use the model directly\n",
    "        tokenizer=tokenizers['afro-xlmr-large']\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization for combined Afro-XLMR and LLaMA (including intent and slot tasks)\n",
      "Before optimization - GPU Memory (GB): Total: 85.06, Allocated: 17.98, Reserved: 17.99, Available: 49.09\n",
      "PyTorch version: 2.1.0+cu118\n",
      "[I 2024-09-23 01:34:34,996] A new study created in memory with name: no-name-bf6249f9-1340-487c-b480-42da0f853b41\n",
      "Cleared GPU memory. Current allocated: 17.98 GB\n",
      "Trial 0: Starting with hyperparameters: {'encoder_lr': 1.584334793720517e-06, 'decoder_lr': 3.609547702666522e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'weight_decay': 0.03126343079509253, 'warmup_steps': 475, 'gradient_accumulation_steps': 128, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.4915261425253563, 'slot_loss_weight': 0.8792820384180697, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 1.584334793720517e-06, 'decoder_lr': 3.609547702666522e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'weight_decay': 0.03126343079509253, 'warmup_steps': 475, 'gradient_accumulation_steps': 128, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.4915261425253563, 'slot_loss_weight': 0.8792820384180697, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 16.90 GB\n",
      "GPU memory cached: 18.50 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 15.74GB (Max: 17.22GB), Reserved: 17.23GB (Max: 17.23GB)\n",
      "Total training steps: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating projection layer: 5120 -> 4897\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 1 - Average train loss: 0.0005\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 2 - Average train loss: 0.0005\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 29.55GB (Max: 34.48GB), Reserved: 32.01GB (Max: 37.04GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 31.73 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 31.74 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 31.73 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 31.74 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 14.531547\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 2:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58 58 58]\n",
      "  Predicted Slots: [58 58 58]\n",
      "Example 3:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58 58 58 58]\n",
      "  Predicted Slots: [58 58 58 58]\n",
      "Example 4:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 5:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Unique true intents: {47}\n",
      "Unique predicted intents: {47}\n",
      "Unique true slots: {58}\n",
      "Unique predicted slots: {58}\n",
      "Cleared GPU memory. Current allocated: 31.74 GB\n",
      "Generating batches: 100%|██████████| 4/4 [09:21<00:00, 140.46s/it]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n",
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:04<00:00,  6.93it/s]\n",
      "Epoch 1/2 Evaluation results: {'translation_score': 0.11248937243863316, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 14.531546592712402}\n",
      "Starting epoch 2/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.48 GB\n",
      "GPU memory cached: 33.10 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 221\n",
      "Score statistics: Min: 0.0005, Max: 0.5359, Median: 0.1109, Average: 0.1125\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.46GB (Max: 34.48GB), Reserved: 30.83GB (Max: 37.04GB)\n",
      "Total training steps: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating projection layer: 5120 -> 4897\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 1 - Average train loss: 0.0005\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 2 - Average train loss: 0.0005\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 27.67GB (Max: 34.48GB), Reserved: 30.83GB (Max: 37.04GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 29.72 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 29.72 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 29.72 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 29.72 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 15.160246\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 2:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58 58 58]\n",
      "  Predicted Slots: [58 58 58]\n",
      "Example 3:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58 58 58 58]\n",
      "  Predicted Slots: [58 58 58 58]\n",
      "Example 4:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 5:\n",
      "  True Intent: 47, Predicted Intent: 47\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Unique true intents: {47}\n",
      "Unique predicted intents: {47}\n",
      "Unique true slots: {58}\n",
      "Unique predicted slots: {58}\n",
      "Cleared GPU memory. Current allocated: 29.72 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:59<00:00, 134.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:04<00:00,  7.21it/s]\n",
      "Epoch 2/2 Evaluation results: {'translation_score': 0.11505203403962347, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 15.160245895385742}\n",
      "Early stopping patience: 1/3\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 221\n",
      "Score statistics: Min: 0.0004, Max: 0.2976, Median: 0.1144, Average: 0.1151\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "[I 2024-09-23 01:56:24,867] Trial 0 finished with value: -0.8849479659603765 and parameters: {'encoder_lr': 1.584334793720517e-06, 'decoder_lr': 3.609547702666522e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 32, 'weight_decay': 0.03126343079509253, 'warmup_steps': 475, 'gradient_accumulation_steps': 128, 'intent_loss_weight': 0.4915261425253563, 'slot_loss_weight': 0.8792820384180697}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "Trial 1: Starting with hyperparameters: {'encoder_lr': 2.7540994896652652e-05, 'decoder_lr': 8.396947822021402e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'weight_decay': 0.012576227491772597, 'warmup_steps': 117, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.7740213672953477, 'slot_loss_weight': 0.832290200442238, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 2.7540994896652652e-05, 'decoder_lr': 8.396947822021402e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'weight_decay': 0.012576227491772597, 'warmup_steps': 117, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.7740213672953477, 'slot_loss_weight': 0.832290200442238, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.24 GB\n",
      "GPU memory cached: 33.01 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.23GB (Max: 34.48GB), Reserved: 30.74GB (Max: 37.04GB)\n",
      "Total training steps: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/66\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Epoch 1 - Average train loss: 0.1370\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/66\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Epoch 2 - Average train loss: 0.1369\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.64GB (Max: 68.52GB), Reserved: 59.24GB (Max: 69.34GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 15.182608\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55]\n",
      "  Predicted Slots: [55]\n",
      "Example 2:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55 55 55]\n",
      "  Predicted Slots: [55 55 55]\n",
      "Example 3:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55 55 55 55]\n",
      "  Predicted Slots: [55 55 55 55]\n",
      "Example 4:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55]\n",
      "  Predicted Slots: [55]\n",
      "Example 5:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55]\n",
      "  Predicted Slots: [55]\n",
      "Unique true intents: {36, 47}\n",
      "Unique predicted intents: {36, 47}\n",
      "Unique true slots: {3, 55}\n",
      "Unique predicted slots: {3, 55}\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Generating batches: 100%|██████████| 4/4 [09:20<00:00, 140.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.04it/s]\n",
      "Epoch 1/2 Evaluation results: {'translation_score': 0.10912466122520185, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 15.182607650756836}\n",
      "Starting epoch 2/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.68 GB\n",
      "GPU memory cached: 63.61 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 221\n",
      "Score statistics: Min: 0.0050, Max: 0.4240, Median: 0.1111, Average: 0.1091\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.65GB (Max: 68.52GB), Reserved: 59.24GB (Max: 69.34GB)\n",
      "Total training steps: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating projection layer: 5120 -> 4897\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/66\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Epoch 1 - Average train loss: 0.1393\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/66\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Epoch 2 - Average train loss: 0.1407\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.86GB (Max: 68.75GB), Reserved: 59.12GB (Max: 69.34GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.91 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.91 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 14.975476\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55]\n",
      "  Predicted Slots: [55]\n",
      "Example 2:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55 55 55]\n",
      "  Predicted Slots: [55 55 55]\n",
      "Example 3:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55 55 55 55]\n",
      "  Predicted Slots: [55 55 55 55]\n",
      "Example 4:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55]\n",
      "  Predicted Slots: [55]\n",
      "Example 5:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [55]\n",
      "  Predicted Slots: [55]\n",
      "Unique true intents: {36, 47}\n",
      "Unique predicted intents: {36, 47}\n",
      "Unique true slots: {55}\n",
      "Unique predicted slots: {55}\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:58<00:00, 134.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.31it/s]\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2/2 Evaluation results: {'translation_score': 0.1179577163280488, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 14.975476264953613}\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 216\n",
      "Score statistics: Min: 0.0021, Max: 0.4339, Median: 0.1119, Average: 0.1180\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "[I 2024-09-23 02:19:23,841] Trial 1 finished with value: -0.8820422836719513 and parameters: {'encoder_lr': 2.7540994896652652e-05, 'decoder_lr': 8.396947822021402e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'weight_decay': 0.012576227491772597, 'warmup_steps': 117, 'gradient_accumulation_steps': 8, 'intent_loss_weight': 0.7740213672953477, 'slot_loss_weight': 0.832290200442238}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "Trial 2: Starting with hyperparameters: {'encoder_lr': 1.3220780379824521e-05, 'decoder_lr': 7.804805185706499e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.08951796334203876, 'warmup_steps': 430, 'gradient_accumulation_steps': 4, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.9716137636933955, 'slot_loss_weight': 0.43379227621507477, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 1.3220780379824521e-05, 'decoder_lr': 7.804805185706499e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.08951796334203876, 'warmup_steps': 430, 'gradient_accumulation_steps': 4, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.9716137636933955, 'slot_loss_weight': 0.43379227621507477, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.24 GB\n",
      "GPU memory cached: 32.01 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.23GB (Max: 68.75GB), Reserved: 29.81GB (Max: 69.34GB)\n",
      "Total training steps: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 0.5527\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 0.5510\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "NaN or Inf values detected in logits\n",
      "Skipping batch 33 due to invalid loss.\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 3 - Average train loss: 0.5371\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 41.04GB (Max: 68.75GB), Reserved: 42.55GB (Max: 69.62GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 44.08 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 44.08 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 44.08 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 44.08 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 14.810886\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Example 2:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35 35 35]\n",
      "  Predicted Slots: [35 35 35]\n",
      "Example 3:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35 35 35 35]\n",
      "  Predicted Slots: [35 35 35 35]\n",
      "Example 4:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Example 5:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Unique true intents: {43}\n",
      "Unique predicted intents: {43}\n",
      "Unique true slots: {35}\n",
      "Unique predicted slots: {35}\n",
      "Cleared GPU memory. Current allocated: 44.08 GB\n",
      "Generating batches: 100%|██████████| 4/4 [09:27<00:00, 141.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  7.34it/s]\n",
      "Epoch 1/3 Evaluation results: {'translation_score': 0.1099138421891898, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 14.81088638305664}\n",
      "Starting epoch 2/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 44.09 GB\n",
      "GPU memory cached: 45.69 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 218\n",
      "Score statistics: Min: 0.0007, Max: 0.4967, Median: 0.1096, Average: 0.1099\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 41.06GB (Max: 68.75GB), Reserved: 42.55GB (Max: 69.62GB)\n",
      "Total training steps: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 0.5271\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 0.5282\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 3 - Average train loss: 0.5200\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 41.27GB (Max: 68.75GB), Reserved: 42.63GB (Max: 69.93GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 44.32 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 44.32 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 44.32 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 44.32 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 14.793429\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Example 2:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35 35 35]\n",
      "  Predicted Slots: [35 35 35]\n",
      "Example 3:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35 35 35 35]\n",
      "  Predicted Slots: [35 35 35 35]\n",
      "Example 4:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Example 5:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Unique true intents: {43}\n",
      "Unique predicted intents: {43}\n",
      "Unique true slots: {35}\n",
      "Unique predicted slots: {35}\n",
      "Cleared GPU memory. Current allocated: 44.33 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:52<00:00, 133.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.16it/s]\n",
      "Epoch 2/3 Evaluation results: {'translation_score': 0.11561169816376683, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 14.793429374694824}\n",
      "Starting epoch 3/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 44.33 GB\n",
      "GPU memory cached: 45.77 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 219\n",
      "Score statistics: Min: 0.0039, Max: 0.4742, Median: 0.1148, Average: 0.1156\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 41.29GB (Max: 68.75GB), Reserved: 42.63GB (Max: 69.93GB)\n",
      "Total training steps: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 0.5182\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 0.5161\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 3 - Average train loss: 0.5151\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 41.50GB (Max: 68.75GB), Reserved: 42.73GB (Max: 70.19GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 44.56 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 44.56 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 44.56 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 44.56 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 14.471477\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Example 2:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35 35 35]\n",
      "  Predicted Slots: [35 35 35]\n",
      "Example 3:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35 35 35 35]\n",
      "  Predicted Slots: [35 35 35 35]\n",
      "Example 4:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Example 5:\n",
      "  True Intent: 43, Predicted Intent: 43\n",
      "  True Slots: [35]\n",
      "  Predicted Slots: [35]\n",
      "Unique true intents: {43}\n",
      "Unique predicted intents: {43}\n",
      "Unique true slots: {35}\n",
      "Unique predicted slots: {35}\n",
      "Cleared GPU memory. Current allocated: 44.57 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:41<00:00, 130.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.11it/s]\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3/3 Evaluation results: {'translation_score': 0.11515461621865024, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 14.471476554870605}\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 221\n",
      "Score statistics: Min: 0.0088, Max: 0.2896, Median: 0.1160, Average: 0.1152\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 14.60 GB\n",
      "[I 2024-09-23 03:01:42,368] Trial 2 finished with value: -0.8848453837813498 and parameters: {'encoder_lr': 1.3220780379824521e-05, 'decoder_lr': 7.804805185706499e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.08951796334203876, 'warmup_steps': 430, 'gradient_accumulation_steps': 4, 'intent_loss_weight': 0.9716137636933955, 'slot_loss_weight': 0.43379227621507477}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 14.60 GB\n",
      "Delay after trial 1: 0.09 seconds\n",
      "Trial 1 completed in 5227.37 seconds\n",
      "After trial 1 - GPU Memory (GB): Total: 85.06, Allocated: 14.60, Reserved: 16.38, Available: 54.08\n",
      "Trial 1 results:\n",
      "  Translation score: -0.8848\n",
      "  Intent accuracy: 1.0000\n",
      "  Slot F1 score: 1.0000\n",
      "Cleared GPU memory. Current allocated: 14.60 GB\n",
      "Trial 3: Starting with hyperparameters: {'encoder_lr': 1.2271216456820661e-05, 'decoder_lr': 6.572739527520792e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2, 'weight_decay': 0.06350094246672124, 'warmup_steps': 364, 'gradient_accumulation_steps': 4, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.34073433526683883, 'slot_loss_weight': 0.1923919090222901, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 1.2271216456820661e-05, 'decoder_lr': 6.572739527520792e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2, 'weight_decay': 0.06350094246672124, 'warmup_steps': 364, 'gradient_accumulation_steps': 4, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.34073433526683883, 'slot_loss_weight': 0.1923919090222901, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 14.64 GB\n",
      "GPU memory cached: 16.38 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 13.64GB (Max: 68.75GB), Reserved: 15.25GB (Max: 70.19GB)\n",
      "Total training steps: 1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 1, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 1, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 1, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 1, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 1, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 1 - Average train loss: 0.5175\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 2, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 2, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 2, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 2, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 2, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 2 - Average train loss: 0.4269\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 3, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 3, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 3, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 3, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 3, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 3, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 3, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 3, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 3 - Average train loss: 0.2709\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.64GB (Max: 68.75GB), Reserved: 56.58GB (Max: 70.19GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 13.004158\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 2:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58 58 58]\n",
      "  Predicted Slots: [58 58 58]\n",
      "Example 3:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58 58 58 58]\n",
      "  Predicted Slots: [58 58 58 58]\n",
      "Example 4:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 5:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Unique true intents: {38}\n",
      "Unique predicted intents: {38}\n",
      "Unique true slots: {58}\n",
      "Unique predicted slots: {58}\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Generating batches: 100%|██████████| 4/4 [07:40<00:00, 115.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.18it/s]\n",
      "Epoch 1/3 Evaluation results: {'translation_score': 0.11056665698733097, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 13.004158020019531}\n",
      "Starting epoch 2/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.68 GB\n",
      "GPU memory cached: 60.75 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 220\n",
      "Score statistics: Min: 0.0018, Max: 0.2937, Median: 0.1134, Average: 0.1106\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.65GB (Max: 68.75GB), Reserved: 56.58GB (Max: 70.19GB)\n",
      "Total training steps: 1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 1, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 1, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 1, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 1, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 1, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 1 - Average train loss: 0.3650\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 2, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 2, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 2, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 2, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 2, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 2 - Average train loss: 0.3065\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 3, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 3, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 3, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 3, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 3, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 3, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 3, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 3, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 3 - Average train loss: 0.2317\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.86GB (Max: 68.75GB), Reserved: 56.57GB (Max: 70.19GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.91 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.91 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.135643\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 2:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58 58 58]\n",
      "  Predicted Slots: [58 58 58]\n",
      "Example 3:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58 58 58 58]\n",
      "  Predicted Slots: [58 58 58 58]\n",
      "Example 4:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Example 5:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [58]\n",
      "  Predicted Slots: [58]\n",
      "Unique true intents: {38}\n",
      "Unique predicted intents: {38}\n",
      "Unique true slots: {58}\n",
      "Unique predicted slots: {58}\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:16<00:00, 124.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.08it/s]\n",
      "Epoch 2/3 Evaluation results: {'translation_score': 0.12095317937895109, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.135643005371094}\n",
      "Starting epoch 3/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.93 GB\n",
      "GPU memory cached: 60.74 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 218\n",
      "Score statistics: Min: 0.0023, Max: 0.4942, Median: 0.1178, Average: 0.1210\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.88GB (Max: 68.75GB), Reserved: 56.57GB (Max: 70.19GB)\n",
      "Total training steps: 1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 1, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 1, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 1, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 1, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 1, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 1 - Average train loss: 0.2918\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 2, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 2, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 2, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 2, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 2, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 2 - Average train loss: 0.2540\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 3, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 3, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 3, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 3, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 3, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 3, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 3, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 3, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 3 - Average train loss: 0.2129\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 55.09GB (Max: 68.75GB), Reserved: 56.57GB (Max: 70.19GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 59.16 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 59.16 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 59.16 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 59.16 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.344810\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [57]\n",
      "  Predicted Slots: [57]\n",
      "Example 2:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [57 57 57]\n",
      "  Predicted Slots: [57 57 57]\n",
      "Example 3:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [57 57 57 57]\n",
      "  Predicted Slots: [57 57 57 57]\n",
      "Example 4:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [57]\n",
      "  Predicted Slots: [57]\n",
      "Example 5:\n",
      "  True Intent: 38, Predicted Intent: 38\n",
      "  True Slots: [57]\n",
      "  Predicted Slots: [57]\n",
      "Unique true intents: {38}\n",
      "Unique predicted intents: {38}\n",
      "Unique true slots: {57}\n",
      "Unique predicted slots: {57}\n",
      "Cleared GPU memory. Current allocated: 59.16 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:26<00:00, 126.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.19it/s]\n",
      "Epoch 3/3 Evaluation results: {'translation_score': 0.12430255515628481, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.344809532165527}\n",
      "Early stopping patience: 1/3\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 219\n",
      "Score statistics: Min: 0.0048, Max: 0.3294, Median: 0.1201, Average: 0.1243\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "[I 2024-09-23 04:13:07,909] Trial 3 finished with value: -0.8756974448437151 and parameters: {'encoder_lr': 1.2271216456820661e-05, 'decoder_lr': 6.572739527520792e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 2, 'weight_decay': 0.06350094246672124, 'warmup_steps': 364, 'gradient_accumulation_steps': 4, 'intent_loss_weight': 0.34073433526683883, 'slot_loss_weight': 0.1923919090222901}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "Delay after trial 2: 0.11 seconds\n",
      "Trial 2 completed in 4285.44 seconds\n",
      "After trial 2 - GPU Memory (GB): Total: 85.06, Allocated: 29.20, Reserved: 31.30, Available: 24.55\n",
      "Trial 2 results:\n",
      "  Translation score: -0.8757\n",
      "  Intent accuracy: 1.0000\n",
      "  Slot F1 score: 1.0000\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "Trial 4: Starting with hyperparameters: {'encoder_lr': 5.340901774718344e-05, 'decoder_lr': 9.720656845589094e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'weight_decay': 0.013849567247167247, 'warmup_steps': 175, 'gradient_accumulation_steps': 32, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.5289296755578761, 'slot_loss_weight': 0.6355026326449162, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 5.340901774718344e-05, 'decoder_lr': 9.720656845589094e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'weight_decay': 0.013849567247167247, 'warmup_steps': 175, 'gradient_accumulation_steps': 32, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.5289296755578761, 'slot_loss_weight': 0.6355026326449162, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.24 GB\n",
      "GPU memory cached: 31.30 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.23GB (Max: 68.75GB), Reserved: 29.15GB (Max: 70.19GB)\n",
      "Total training steps: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 1 - Average train loss: 0.0051\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 2 - Average train loss: 0.0050\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 3 - Average train loss: 0.0051\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.65GB (Max: 69.85GB), Reserved: 57.09GB (Max: 70.97GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.882577\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Example 2:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82 82 82]\n",
      "  Predicted Slots: [82 82 82]\n",
      "Example 3:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82 82 82 82]\n",
      "  Predicted Slots: [82 82 82 82]\n",
      "Example 4:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Example 5:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Unique true intents: {34}\n",
      "Unique predicted intents: {34}\n",
      "Unique true slots: {82}\n",
      "Unique predicted slots: {82}\n",
      "Cleared GPU memory. Current allocated: 58.69 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:08<00:00, 122.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.09it/s]\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/3 Evaluation results: {'translation_score': 0.12250518514487688, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.882576942443848}\n",
      "Starting epoch 2/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.69 GB\n",
      "GPU memory cached: 61.31 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 219\n",
      "Score statistics: Min: 0.0015, Max: 0.3852, Median: 0.1182, Average: 0.1225\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.66GB (Max: 69.85GB), Reserved: 57.10GB (Max: 70.97GB)\n",
      "Total training steps: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating projection layer: 5120 -> 4897\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 1 - Average train loss: 0.0045\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 2 - Average train loss: 0.0046\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 3 - Average train loss: 0.0046\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.88GB (Max: 70.09GB), Reserved: 57.10GB (Max: 70.97GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.93 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.93 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.430446\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Example 2:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82 82 82]\n",
      "  Predicted Slots: [82 82 82]\n",
      "Example 3:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82 82 82 82]\n",
      "  Predicted Slots: [82 82 82 82]\n",
      "Example 4:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Example 5:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Unique true intents: {34}\n",
      "Unique predicted intents: {34}\n",
      "Unique true slots: {82, 84}\n",
      "Unique predicted slots: {82, 84}\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:49<00:00, 132.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  7.38it/s]\n",
      "Epoch 2/3 Evaluation results: {'translation_score': 0.11602186480730907, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.430445671081543}\n",
      "Starting epoch 3/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.95 GB\n",
      "GPU memory cached: 61.31 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 221\n",
      "Score statistics: Min: 0.0082, Max: 0.3916, Median: 0.1126, Average: 0.1160\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.90GB (Max: 70.09GB), Reserved: 57.10GB (Max: 70.97GB)\n",
      "Total training steps: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating projection layer: 5120 -> 4897\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 1 - Average train loss: 0.0051\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 2 - Average train loss: 0.0051\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 3 - Average train loss: 0.0051\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 55.11GB (Max: 70.33GB), Reserved: 57.04GB (Max: 71.24GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 59.18 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 59.18 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 59.18 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 59.18 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.927847\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Example 2:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82 82 82]\n",
      "  Predicted Slots: [82 82 82]\n",
      "Example 3:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82 82 82 82]\n",
      "  Predicted Slots: [82 82 82 82]\n",
      "Example 4:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Example 5:\n",
      "  True Intent: 34, Predicted Intent: 34\n",
      "  True Slots: [82]\n",
      "  Predicted Slots: [82]\n",
      "Unique true intents: {34}\n",
      "Unique predicted intents: {34}\n",
      "Unique true slots: {82}\n",
      "Unique predicted slots: {82}\n",
      "Cleared GPU memory. Current allocated: 59.19 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:35<00:00, 128.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.14it/s]\n",
      "Epoch 3/3 Evaluation results: {'translation_score': 0.12003916883799015, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.927846908569336}\n",
      "Early stopping patience: 1/3\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 221\n",
      "Score statistics: Min: 0.0102, Max: 0.2541, Median: 0.1161, Average: 0.1200\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.22 GB\n",
      "[I 2024-09-23 04:45:16,160] Trial 4 finished with value: -0.8799608311620098 and parameters: {'encoder_lr': 5.340901774718344e-05, 'decoder_lr': 9.720656845589094e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.013849567247167247, 'warmup_steps': 175, 'gradient_accumulation_steps': 32, 'intent_loss_weight': 0.5289296755578761, 'slot_loss_weight': 0.6355026326449162}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.22 GB\n",
      "Trial 5: Starting with hyperparameters: {'encoder_lr': 5.5751951468132376e-06, 'decoder_lr': 1.0655500347034755e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.03857847809160532, 'warmup_steps': 297, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.8542567056492404, 'slot_loss_weight': 0.8183935438359984, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 5.5751951468132376e-06, 'decoder_lr': 1.0655500347034755e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.03857847809160532, 'warmup_steps': 297, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.8542567056492404, 'slot_loss_weight': 0.8183935438359984, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.26 GB\n",
      "GPU memory cached: 32.42 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.25GB (Max: 70.33GB), Reserved: 30.19GB (Max: 71.24GB)\n",
      "Total training steps: 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 0.0763\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 0.0763\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.66GB (Max: 70.33GB), Reserved: 55.69GB (Max: 71.24GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.70 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.70 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.70 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.70 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 12.402215\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [67]\n",
      "  Predicted Slots: [67]\n",
      "Example 2:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [67 67 67]\n",
      "  Predicted Slots: [67 67 67]\n",
      "Example 3:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [67 67 67 67]\n",
      "  Predicted Slots: [67 67 67 67]\n",
      "Example 4:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [67]\n",
      "  Predicted Slots: [67]\n",
      "Example 5:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [67]\n",
      "  Predicted Slots: [67]\n",
      "Unique true intents: {0, 17}\n",
      "Unique predicted intents: {0, 17}\n",
      "Unique true slots: {67}\n",
      "Unique predicted slots: {67}\n",
      "Cleared GPU memory. Current allocated: 58.70 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:07<00:00, 121.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  7.65it/s]\n",
      "Epoch 1/2 Evaluation results: {'translation_score': 0.12681887674128312, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 12.402215003967285}\n",
      "Starting epoch 2/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.71 GB\n",
      "GPU memory cached: 59.80 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 218\n",
      "Score statistics: Min: 0.0030, Max: 0.5103, Median: 0.1246, Average: 0.1268\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.67GB (Max: 70.33GB), Reserved: 55.69GB (Max: 71.24GB)\n",
      "Total training steps: 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 0.0797\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 0.0797\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.88GB (Max: 70.33GB), Reserved: 55.76GB (Max: 71.24GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 12.482211\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [8]\n",
      "  Predicted Slots: [8]\n",
      "Example 2:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [8 8 8]\n",
      "  Predicted Slots: [8 8 8]\n",
      "Example 3:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [ 8 12  8  8]\n",
      "  Predicted Slots: [ 8 12  8  8]\n",
      "Example 4:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [8]\n",
      "  Predicted Slots: [8]\n",
      "Example 5:\n",
      "  True Intent: 17, Predicted Intent: 17\n",
      "  True Slots: [8]\n",
      "  Predicted Slots: [8]\n",
      "Unique true intents: {0, 17}\n",
      "Unique predicted intents: {0, 17}\n",
      "Unique true slots: {8, 12}\n",
      "Unique predicted slots: {8, 12}\n",
      "Cleared GPU memory. Current allocated: 58.94 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:36<00:00, 129.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.13it/s]\n",
      "Epoch 2/2 Evaluation results: {'translation_score': 0.12312586307227952, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 12.482211112976074}\n",
      "Early stopping patience: 1/3\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 219\n",
      "Score statistics: Min: 0.0009, Max: 0.5144, Median: 0.1225, Average: 0.1231\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.21 GB\n",
      "[I 2024-09-23 05:08:36,318] Trial 5 finished with value: -0.8768741369277204 and parameters: {'encoder_lr': 5.5751951468132376e-06, 'decoder_lr': 1.0655500347034755e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8, 'weight_decay': 0.03857847809160532, 'warmup_steps': 297, 'gradient_accumulation_steps': 8, 'intent_loss_weight': 0.8542567056492404, 'slot_loss_weight': 0.8183935438359984}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.21 GB\n",
      "Trial 6: Starting with hyperparameters: {'encoder_lr': 1.0620039101098075e-05, 'decoder_lr': 6.409771301580595e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'weight_decay': 0.026185644643783743, 'warmup_steps': 379, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.8371677057410659, 'slot_loss_weight': 0.6774568835061096, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 1.0620039101098075e-05, 'decoder_lr': 6.409771301580595e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'weight_decay': 0.026185644643783743, 'warmup_steps': 379, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.8371677057410659, 'slot_loss_weight': 0.6774568835061096, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/1\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.25 GB\n",
      "GPU memory cached: 33.13 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([32, 128])\n",
      "  attention_mask: torch.Size([32, 128])\n",
      "  labels: torch.Size([32, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.24GB (Max: 70.33GB), Reserved: 30.85GB (Max: 71.24GB)\n",
      "Total training steps: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Epoch 1 - Average train loss: 0.0789\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.68GB (Max: 70.33GB), Reserved: 57.16GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.72 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.72 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.72 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.72 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.762144\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [34]\n",
      "  Predicted Slots: [34]\n",
      "Example 2:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [34 34 34]\n",
      "  Predicted Slots: [34 34 34]\n",
      "Example 3:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [34 34 34 34]\n",
      "  Predicted Slots: [34 34 34 34]\n",
      "Example 4:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [34]\n",
      "  Predicted Slots: [34]\n",
      "Example 5:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [34]\n",
      "  Predicted Slots: [34]\n",
      "Unique true intents: {27}\n",
      "Unique predicted intents: {27}\n",
      "Unique true slots: {34}\n",
      "Unique predicted slots: {34}\n",
      "Cleared GPU memory. Current allocated: 58.72 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:03<00:00, 120.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.09it/s]\n",
      "Epoch 1/1 Evaluation results: {'translation_score': 0.12088168234572755, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.762144088745117}\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 222\n",
      "Score statistics: Min: 0.0045, Max: 0.6466, Median: 0.1206, Average: 0.1209\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.23 GB\n",
      "[I 2024-09-23 05:17:36,439] Trial 6 finished with value: -0.8791183176542725 and parameters: {'encoder_lr': 1.0620039101098075e-05, 'decoder_lr': 6.409771301580595e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 32, 'weight_decay': 0.026185644643783743, 'warmup_steps': 379, 'gradient_accumulation_steps': 8, 'intent_loss_weight': 0.8371677057410659, 'slot_loss_weight': 0.6774568835061096}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.23 GB\n",
      "Delay after trial 3: 0.12 seconds\n",
      "Trial 3 completed in 3868.42 seconds\n",
      "After trial 3 - GPU Memory (GB): Total: 85.06, Allocated: 29.23, Reserved: 34.31, Available: 21.52\n",
      "Trial 3 results:\n",
      "  Translation score: -0.8791\n",
      "  Intent accuracy: 1.0000\n",
      "  Slot F1 score: 1.0000\n",
      "Cleared GPU memory. Current allocated: 29.23 GB\n",
      "Trial 7: Starting with hyperparameters: {'encoder_lr': 5.95928012590099e-05, 'decoder_lr': 2.890273847476356e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'weight_decay': 0.036932736284978056, 'warmup_steps': 269, 'gradient_accumulation_steps': 64, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.6938748227654997, 'slot_loss_weight': 0.9377115967447781, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 5.95928012590099e-05, 'decoder_lr': 2.890273847476356e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'weight_decay': 0.036932736284978056, 'warmup_steps': 269, 'gradient_accumulation_steps': 64, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.6938748227654997, 'slot_loss_weight': 0.9377115967447781, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/1\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.27 GB\n",
      "GPU memory cached: 34.31 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.26GB (Max: 70.33GB), Reserved: 31.95GB (Max: 71.27GB)\n",
      "Total training steps: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating projection layer: 5120 -> 4897\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/66\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Epoch 1 - Average train loss: 0.0012\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.67GB (Max: 70.33GB), Reserved: 56.20GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.71 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.71 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.71 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.71 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.112671\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [50]\n",
      "  Predicted Slots: [50]\n",
      "Example 2:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [50 50 50]\n",
      "  Predicted Slots: [50 50 50]\n",
      "Example 3:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [50 50 50 50]\n",
      "  Predicted Slots: [50 50 50 50]\n",
      "Example 4:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [50]\n",
      "  Predicted Slots: [50]\n",
      "Example 5:\n",
      "  True Intent: 36, Predicted Intent: 36\n",
      "  True Slots: [50]\n",
      "  Predicted Slots: [50]\n",
      "Unique true intents: {36}\n",
      "Unique predicted intents: {36}\n",
      "Unique true slots: {50}\n",
      "Unique predicted slots: {50}\n",
      "Cleared GPU memory. Current allocated: 58.72 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:57<00:00, 134.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.13it/s]\n",
      "Epoch 1/1 Evaluation results: {'translation_score': 0.11840678740788353, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.1126708984375}\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 218\n",
      "Score statistics: Min: 0.0039, Max: 0.2487, Median: 0.1175, Average: 0.1184\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.22 GB\n",
      "[I 2024-09-23 05:27:48,796] Trial 7 finished with value: -0.8815932125921164 and parameters: {'encoder_lr': 5.95928012590099e-05, 'decoder_lr': 2.890273847476356e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'weight_decay': 0.036932736284978056, 'warmup_steps': 269, 'gradient_accumulation_steps': 64, 'intent_loss_weight': 0.6938748227654997, 'slot_loss_weight': 0.9377115967447781}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.22 GB\n",
      "Trial 8: Starting with hyperparameters: {'encoder_lr': 3.481496259559779e-05, 'decoder_lr': 1.122780597878627e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.011692717320829804, 'warmup_steps': 350, 'gradient_accumulation_steps': 2, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.21037072396085482, 'slot_loss_weight': 0.3650443741137871, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 3.481496259559779e-05, 'decoder_lr': 1.122780597878627e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.011692717320829804, 'warmup_steps': 350, 'gradient_accumulation_steps': 2, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.21037072396085482, 'slot_loss_weight': 0.3650443741137871, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.26 GB\n",
      "GPU memory cached: 34.27 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.25GB (Max: 70.33GB), Reserved: 31.92GB (Max: 71.27GB)\n",
      "Total training steps: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 1.2593\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 1.1781\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 3 - Average train loss: 0.9736\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 41.05GB (Max: 70.33GB), Reserved: 42.42GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 44.08 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 44.09 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 44.08 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 44.09 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 10.040504\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Example 2:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91 91 91]\n",
      "  Predicted Slots: [91 91 91]\n",
      "Example 3:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91 91 91 91]\n",
      "  Predicted Slots: [91 91 91 91]\n",
      "Example 4:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Example 5:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Unique true intents: {29}\n",
      "Unique predicted intents: {29}\n",
      "Unique true slots: {91}\n",
      "Unique predicted slots: {91}\n",
      "Cleared GPU memory. Current allocated: 44.09 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:51<00:00, 132.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.08it/s]\n",
      "Epoch 1/3 Evaluation results: {'translation_score': 0.11894268495855709, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 10.04050350189209}\n",
      "Starting epoch 2/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 44.09 GB\n",
      "GPU memory cached: 45.55 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 219\n",
      "Score statistics: Min: 0.0023, Max: 0.3311, Median: 0.1161, Average: 0.1189\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 41.07GB (Max: 70.33GB), Reserved: 42.43GB (Max: 71.27GB)\n",
      "Total training steps: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 1.1260\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 1.0492\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 3 - Average train loss: 0.9248\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 41.28GB (Max: 70.33GB), Reserved: 42.59GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 44.33 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 44.33 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 44.33 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 44.33 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 11.314167\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Example 2:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91 91 91]\n",
      "  Predicted Slots: [91 91 91]\n",
      "Example 3:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91 91 91 91]\n",
      "  Predicted Slots: [91 91 91 91]\n",
      "Example 4:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Example 5:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Unique true intents: {29}\n",
      "Unique predicted intents: {29}\n",
      "Unique true slots: {91}\n",
      "Unique predicted slots: {91}\n",
      "Cleared GPU memory. Current allocated: 44.33 GB\n",
      "Generating batches: 100%|██████████| 4/4 [07:35<00:00, 114.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.16it/s]\n",
      "Epoch 2/3 Evaluation results: {'translation_score': 0.11792881634001705, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 11.314167022705078}\n",
      "Early stopping patience: 1/3\n",
      "Starting epoch 3/3\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 44.34 GB\n",
      "GPU memory cached: 45.64 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([8, 128])\n",
      "  attention_mask: torch.Size([8, 128])\n",
      "  labels: torch.Size([8, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 220\n",
      "Score statistics: Min: 0.0032, Max: 0.4501, Median: 0.1231, Average: 0.1179\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 41.29GB (Max: 70.33GB), Reserved: 42.50GB (Max: 71.27GB)\n",
      "Total training steps: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 1 - Average train loss: 1.0592\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 2 - Average train loss: 0.9879\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 3, Step 50/132\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 3, Step 100/132\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Epoch 3 - Average train loss: 0.8583\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 41.50GB (Max: 70.33GB), Reserved: 42.80GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 44.57 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 44.57 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 44.57 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 44.57 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 9.495889\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Example 2:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91 91 91]\n",
      "  Predicted Slots: [91 91 91]\n",
      "Example 3:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91 91 91 91]\n",
      "  Predicted Slots: [91 91 91 91]\n",
      "Example 4:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Example 5:\n",
      "  True Intent: 29, Predicted Intent: 29\n",
      "  True Slots: [91]\n",
      "  Predicted Slots: [91]\n",
      "Unique true intents: {29}\n",
      "Unique predicted intents: {29}\n",
      "Unique true slots: {91}\n",
      "Unique predicted slots: {91}\n",
      "Cleared GPU memory. Current allocated: 44.58 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:23<00:00, 125.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.21it/s]\n",
      "Epoch 3/3 Evaluation results: {'translation_score': 0.1220100999679752, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 9.495888710021973}\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 222\n",
      "Score statistics: Min: 0.0069, Max: 0.4359, Median: 0.1267, Average: 0.1220\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 14.60 GB\n",
      "[I 2024-09-23 06:07:55,212] Trial 8 finished with value: -0.8779899000320248 and parameters: {'encoder_lr': 3.481496259559779e-05, 'decoder_lr': 1.122780597878627e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8, 'weight_decay': 0.011692717320829804, 'warmup_steps': 350, 'gradient_accumulation_steps': 2, 'intent_loss_weight': 0.21037072396085482, 'slot_loss_weight': 0.3650443741137871}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 14.60 GB\n",
      "Trial 9: Starting with hyperparameters: {'encoder_lr': 2.713226672082772e-06, 'decoder_lr': 7.029251555873959e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2, 'weight_decay': 0.05314240785855675, 'warmup_steps': 256, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.880005622356883, 'slot_loss_weight': 0.13543329629075776, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 2.713226672082772e-06, 'decoder_lr': 7.029251555873959e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2, 'weight_decay': 0.05314240785855675, 'warmup_steps': 256, 'gradient_accumulation_steps': 8, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.880005622356883, 'slot_loss_weight': 0.13543329629075776, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/1\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 14.64 GB\n",
      "GPU memory cached: 16.38 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([2, 128])\n",
      "  attention_mask: torch.Size([2, 128])\n",
      "  labels: torch.Size([2, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 13.64GB (Max: 70.33GB), Reserved: 15.25GB (Max: 71.27GB)\n",
      "Total training steps: 525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/525\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/525\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/525\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/525\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/525\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 1, Step 300/525\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 1, Step 350/525\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 1, Step 400/525\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 1, Step 450/525\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 1, Step 500/525\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Epoch 1 - Average train loss: 0.0433\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.64GB (Max: 70.33GB), Reserved: 56.31GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.67 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 9.999743\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 49, Predicted Intent: 49\n",
      "  True Slots: [63]\n",
      "  Predicted Slots: [63]\n",
      "Example 2:\n",
      "  True Intent: 49, Predicted Intent: 49\n",
      "  True Slots: [63 63 63]\n",
      "  Predicted Slots: [63 63 63]\n",
      "Example 3:\n",
      "  True Intent: 49, Predicted Intent: 49\n",
      "  True Slots: [63 63 63 63]\n",
      "  Predicted Slots: [63 63 63 63]\n",
      "Example 4:\n",
      "  True Intent: 49, Predicted Intent: 49\n",
      "  True Slots: [63]\n",
      "  Predicted Slots: [63]\n",
      "Example 5:\n",
      "  True Intent: 49, Predicted Intent: 49\n",
      "  True Slots: [63]\n",
      "  Predicted Slots: [63]\n",
      "Unique true intents: {49}\n",
      "Unique predicted intents: {49}\n",
      "Unique true slots: {21, 63}\n",
      "Unique predicted slots: {21, 63}\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:32<00:00, 128.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  7.91it/s]\n",
      "Epoch 1/1 Evaluation results: {'translation_score': 0.11835029269967161, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 9.99974250793457}\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 222\n",
      "Score statistics: Min: 0.0025, Max: 0.5097, Median: 0.1167, Average: 0.1184\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "[I 2024-09-23 06:21:45,346] Trial 9 finished with value: -0.8816497073003284 and parameters: {'encoder_lr': 2.713226672082772e-06, 'decoder_lr': 7.029251555873959e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 2, 'weight_decay': 0.05314240785855675, 'warmup_steps': 256, 'gradient_accumulation_steps': 8, 'intent_loss_weight': 0.880005622356883, 'slot_loss_weight': 0.13543329629075776}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "Delay after trial 4: 0.10 seconds\n",
      "Trial 4 completed in 3848.78 seconds\n",
      "After trial 4 - GPU Memory (GB): Total: 85.06, Allocated: 29.20, Reserved: 31.67, Available: 24.19\n",
      "Trial 4 results:\n",
      "  Translation score: -0.8816\n",
      "  Intent accuracy: 1.0000\n",
      "  Slot F1 score: 1.0000\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "Trial 10: Starting with hyperparameters: {'encoder_lr': 1.0385188433766581e-06, 'decoder_lr': 1.4344219528553843e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4, 'weight_decay': 0.02053691187630526, 'warmup_steps': 486, 'gradient_accumulation_steps': 128, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.5084874460068664, 'slot_loss_weight': 0.9904333083270163, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 1.0385188433766581e-06, 'decoder_lr': 1.4344219528553843e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4, 'weight_decay': 0.02053691187630526, 'warmup_steps': 486, 'gradient_accumulation_steps': 128, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.5084874460068664, 'slot_loss_weight': 0.9904333083270163, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.24 GB\n",
      "GPU memory cached: 31.67 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.23GB (Max: 70.33GB), Reserved: 29.49GB (Max: 71.27GB)\n",
      "Total training steps: 526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/263\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/263\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/263\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/263\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/263\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Epoch 1 - Average train loss: 0.0002\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/263\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/263\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/263\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/263\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/263\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Epoch 2 - Average train loss: 0.0002\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.64GB (Max: 70.33GB), Reserved: 55.13GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 9.783488\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56]\n",
      "  Predicted Slots: [56]\n",
      "Example 2:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56 56 56]\n",
      "  Predicted Slots: [56 56 56]\n",
      "Example 3:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56 56 56 56]\n",
      "  Predicted Slots: [56 56 56 56]\n",
      "Example 4:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56]\n",
      "  Predicted Slots: [56]\n",
      "Example 5:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56]\n",
      "  Predicted Slots: [56]\n",
      "Unique true intents: {42, 23}\n",
      "Unique predicted intents: {42, 23}\n",
      "Unique true slots: {56}\n",
      "Unique predicted slots: {56}\n",
      "Cleared GPU memory. Current allocated: 58.69 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:13<00:00, 123.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  7.92it/s]\n",
      "Epoch 1/2 Evaluation results: {'translation_score': 0.11734718817172411, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 9.783488273620605}\n",
      "Starting epoch 2/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.69 GB\n",
      "GPU memory cached: 59.20 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([4, 128])\n",
      "  attention_mask: torch.Size([4, 128])\n",
      "  labels: torch.Size([4, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 221\n",
      "Score statistics: Min: 0.0081, Max: 0.2867, Median: 0.1179, Average: 0.1173\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.66GB (Max: 70.33GB), Reserved: 55.14GB (Max: 71.27GB)\n",
      "Total training steps: 526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/263\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/263\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/263\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/263\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/263\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Epoch 1 - Average train loss: 0.0002\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/263\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/263\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/263\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/263\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/263\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Epoch 2 - Average train loss: 0.0002\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.87GB (Max: 70.33GB), Reserved: 55.25GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 10.140888\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56]\n",
      "  Predicted Slots: [56]\n",
      "Example 2:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56 56 56]\n",
      "  Predicted Slots: [56 56 56]\n",
      "Example 3:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56 56 56 56]\n",
      "  Predicted Slots: [56 56 56 56]\n",
      "Example 4:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56]\n",
      "  Predicted Slots: [56]\n",
      "Example 5:\n",
      "  True Intent: 42, Predicted Intent: 42\n",
      "  True Slots: [56]\n",
      "  Predicted Slots: [56]\n",
      "Unique true intents: {42, 23}\n",
      "Unique predicted intents: {42, 23}\n",
      "Unique true slots: {56}\n",
      "Unique predicted slots: {56}\n",
      "Cleared GPU memory. Current allocated: 58.93 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:28<00:00, 127.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.08it/s]\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2/2 Evaluation results: {'translation_score': 0.1188260601286631, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 10.140888214111328}\n",
      "Early stopping patience: 1/3\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 218\n",
      "Score statistics: Min: 0.0094, Max: 0.3331, Median: 0.1174, Average: 0.1188\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "[I 2024-09-23 06:49:32,084] Trial 10 finished with value: -0.8811739398713369 and parameters: {'encoder_lr': 1.0385188433766581e-06, 'decoder_lr': 1.4344219528553843e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 4, 'weight_decay': 0.02053691187630526, 'warmup_steps': 486, 'gradient_accumulation_steps': 128, 'intent_loss_weight': 0.5084874460068664, 'slot_loss_weight': 0.9904333083270163}. Best is trial 0 with value: -0.8849479659603765.\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "Trial 11: Starting with hyperparameters: {'encoder_lr': 2.994471116556308e-06, 'decoder_lr': 3.3477388089161037e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'weight_decay': 0.09304895986444242, 'warmup_steps': 487, 'gradient_accumulation_steps': 128, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.9761991671313265, 'slot_loss_weight': 0.43081143618503664, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Initialized CombinedEncoderDecoderTrainer with config: {'encoder_lr': 2.994471116556308e-06, 'decoder_lr': 3.3477388089161037e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'weight_decay': 0.09304895986444242, 'warmup_steps': 487, 'gradient_accumulation_steps': 128, 'fp16': False, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'save_steps': 200, 'logging_steps': 50, 'max_grad_norm': 1.0, 'output_dir': './model_outputs', 'seed': 42, 'device': 'cuda', 'cache_dir': './model_cache', 'gradient_checkpointing': True, 'intent_loss_weight': 0.9761991671313265, 'slot_loss_weight': 0.43081143618503664, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n",
      "Train dataset size: 1050\n",
      "Eval dataset size: 225\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting epoch 1/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 29.24 GB\n",
      "GPU memory cached: 31.86 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 27.23GB (Max: 70.33GB), Reserved: 29.67GB (Max: 71.27GB)\n",
      "Total training steps: 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/1050\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/1050\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/1050\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/1050\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/1050\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 1, Step 300/1050\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 1, Step 350/1050\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 1, Step 400/1050\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 1, Step 450/1050\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 1, Step 500/1050\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Starting train_step for batch 526\n",
      "Starting train_step for batch 527\n",
      "Starting train_step for batch 528\n",
      "Starting train_step for batch 529\n",
      "Starting train_step for batch 530\n",
      "Starting train_step for batch 531\n",
      "Starting train_step for batch 532\n",
      "Starting train_step for batch 533\n",
      "Starting train_step for batch 534\n",
      "Starting train_step for batch 535\n",
      "Starting train_step for batch 536\n",
      "Starting train_step for batch 537\n",
      "Starting train_step for batch 538\n",
      "Starting train_step for batch 539\n",
      "Starting train_step for batch 540\n",
      "Starting train_step for batch 541\n",
      "Starting train_step for batch 542\n",
      "Starting train_step for batch 543\n",
      "Starting train_step for batch 544\n",
      "Starting train_step for batch 545\n",
      "Starting train_step for batch 546\n",
      "Starting train_step for batch 547\n",
      "Starting train_step for batch 548\n",
      "Starting train_step for batch 549\n",
      "Starting train_step for batch 550\n",
      "Epoch 1, Step 550/1050\n",
      "Starting train_step for batch 551\n",
      "Starting train_step for batch 552\n",
      "Starting train_step for batch 553\n",
      "Starting train_step for batch 554\n",
      "Starting train_step for batch 555\n",
      "Starting train_step for batch 556\n",
      "Starting train_step for batch 557\n",
      "Starting train_step for batch 558\n",
      "Starting train_step for batch 559\n",
      "Starting train_step for batch 560\n",
      "Starting train_step for batch 561\n",
      "Starting train_step for batch 562\n",
      "Starting train_step for batch 563\n",
      "Starting train_step for batch 564\n",
      "Starting train_step for batch 565\n",
      "Starting train_step for batch 566\n",
      "Starting train_step for batch 567\n",
      "Starting train_step for batch 568\n",
      "Starting train_step for batch 569\n",
      "Starting train_step for batch 570\n",
      "Starting train_step for batch 571\n",
      "Starting train_step for batch 572\n",
      "Starting train_step for batch 573\n",
      "Starting train_step for batch 574\n",
      "Starting train_step for batch 575\n",
      "Starting train_step for batch 576\n",
      "Starting train_step for batch 577\n",
      "Starting train_step for batch 578\n",
      "Starting train_step for batch 579\n",
      "Starting train_step for batch 580\n",
      "Starting train_step for batch 581\n",
      "Starting train_step for batch 582\n",
      "Starting train_step for batch 583\n",
      "Starting train_step for batch 584\n",
      "Starting train_step for batch 585\n",
      "Starting train_step for batch 586\n",
      "Starting train_step for batch 587\n",
      "Starting train_step for batch 588\n",
      "Starting train_step for batch 589\n",
      "Starting train_step for batch 590\n",
      "Starting train_step for batch 591\n",
      "Starting train_step for batch 592\n",
      "Starting train_step for batch 593\n",
      "Starting train_step for batch 594\n",
      "Starting train_step for batch 595\n",
      "Starting train_step for batch 596\n",
      "Starting train_step for batch 597\n",
      "Starting train_step for batch 598\n",
      "Starting train_step for batch 599\n",
      "Starting train_step for batch 600\n",
      "Epoch 1, Step 600/1050\n",
      "Starting train_step for batch 601\n",
      "Starting train_step for batch 602\n",
      "Starting train_step for batch 603\n",
      "Starting train_step for batch 604\n",
      "Starting train_step for batch 605\n",
      "Starting train_step for batch 606\n",
      "Starting train_step for batch 607\n",
      "Starting train_step for batch 608\n",
      "Starting train_step for batch 609\n",
      "Starting train_step for batch 610\n",
      "Starting train_step for batch 611\n",
      "Starting train_step for batch 612\n",
      "Starting train_step for batch 613\n",
      "Starting train_step for batch 614\n",
      "Starting train_step for batch 615\n",
      "Starting train_step for batch 616\n",
      "Starting train_step for batch 617\n",
      "Starting train_step for batch 618\n",
      "Starting train_step for batch 619\n",
      "Starting train_step for batch 620\n",
      "Starting train_step for batch 621\n",
      "Starting train_step for batch 622\n",
      "Starting train_step for batch 623\n",
      "Starting train_step for batch 624\n",
      "Starting train_step for batch 625\n",
      "Starting train_step for batch 626\n",
      "Starting train_step for batch 627\n",
      "Starting train_step for batch 628\n",
      "Starting train_step for batch 629\n",
      "Starting train_step for batch 630\n",
      "Starting train_step for batch 631\n",
      "Starting train_step for batch 632\n",
      "Starting train_step for batch 633\n",
      "Starting train_step for batch 634\n",
      "Starting train_step for batch 635\n",
      "Starting train_step for batch 636\n",
      "Starting train_step for batch 637\n",
      "Starting train_step for batch 638\n",
      "Starting train_step for batch 639\n",
      "Starting train_step for batch 640\n",
      "Starting train_step for batch 641\n",
      "Starting train_step for batch 642\n",
      "Starting train_step for batch 643\n",
      "Starting train_step for batch 644\n",
      "Starting train_step for batch 645\n",
      "Starting train_step for batch 646\n",
      "Starting train_step for batch 647\n",
      "Starting train_step for batch 648\n",
      "Starting train_step for batch 649\n",
      "Starting train_step for batch 650\n",
      "Epoch 1, Step 650/1050\n",
      "Starting train_step for batch 651\n",
      "Starting train_step for batch 652\n",
      "Starting train_step for batch 653\n",
      "Starting train_step for batch 654\n",
      "Starting train_step for batch 655\n",
      "Starting train_step for batch 656\n",
      "Starting train_step for batch 657\n",
      "Starting train_step for batch 658\n",
      "Starting train_step for batch 659\n",
      "Starting train_step for batch 660\n",
      "Starting train_step for batch 661\n",
      "Starting train_step for batch 662\n",
      "Starting train_step for batch 663\n",
      "Starting train_step for batch 664\n",
      "Starting train_step for batch 665\n",
      "Starting train_step for batch 666\n",
      "Starting train_step for batch 667\n",
      "Starting train_step for batch 668\n",
      "Starting train_step for batch 669\n",
      "Starting train_step for batch 670\n",
      "Starting train_step for batch 671\n",
      "Starting train_step for batch 672\n",
      "Starting train_step for batch 673\n",
      "Starting train_step for batch 674\n",
      "Starting train_step for batch 675\n",
      "Starting train_step for batch 676\n",
      "Starting train_step for batch 677\n",
      "Starting train_step for batch 678\n",
      "Starting train_step for batch 679\n",
      "Starting train_step for batch 680\n",
      "Starting train_step for batch 681\n",
      "Starting train_step for batch 682\n",
      "Starting train_step for batch 683\n",
      "Starting train_step for batch 684\n",
      "Starting train_step for batch 685\n",
      "Starting train_step for batch 686\n",
      "Starting train_step for batch 687\n",
      "Starting train_step for batch 688\n",
      "Starting train_step for batch 689\n",
      "Starting train_step for batch 690\n",
      "Starting train_step for batch 691\n",
      "Starting train_step for batch 692\n",
      "Starting train_step for batch 693\n",
      "Starting train_step for batch 694\n",
      "Starting train_step for batch 695\n",
      "Starting train_step for batch 696\n",
      "Starting train_step for batch 697\n",
      "Starting train_step for batch 698\n",
      "Starting train_step for batch 699\n",
      "Starting train_step for batch 700\n",
      "Epoch 1, Step 700/1050\n",
      "Starting train_step for batch 701\n",
      "Starting train_step for batch 702\n",
      "Starting train_step for batch 703\n",
      "Starting train_step for batch 704\n",
      "Starting train_step for batch 705\n",
      "Starting train_step for batch 706\n",
      "Starting train_step for batch 707\n",
      "Starting train_step for batch 708\n",
      "Starting train_step for batch 709\n",
      "Starting train_step for batch 710\n",
      "Starting train_step for batch 711\n",
      "Starting train_step for batch 712\n",
      "Starting train_step for batch 713\n",
      "Starting train_step for batch 714\n",
      "Starting train_step for batch 715\n",
      "Starting train_step for batch 716\n",
      "Starting train_step for batch 717\n",
      "Starting train_step for batch 718\n",
      "Starting train_step for batch 719\n",
      "Starting train_step for batch 720\n",
      "Starting train_step for batch 721\n",
      "Starting train_step for batch 722\n",
      "Starting train_step for batch 723\n",
      "Starting train_step for batch 724\n",
      "Starting train_step for batch 725\n",
      "Starting train_step for batch 726\n",
      "Starting train_step for batch 727\n",
      "Starting train_step for batch 728\n",
      "Starting train_step for batch 729\n",
      "Starting train_step for batch 730\n",
      "Starting train_step for batch 731\n",
      "Starting train_step for batch 732\n",
      "Starting train_step for batch 733\n",
      "Starting train_step for batch 734\n",
      "Starting train_step for batch 735\n",
      "Starting train_step for batch 736\n",
      "Starting train_step for batch 737\n",
      "Starting train_step for batch 738\n",
      "Starting train_step for batch 739\n",
      "Starting train_step for batch 740\n",
      "Starting train_step for batch 741\n",
      "Starting train_step for batch 742\n",
      "Starting train_step for batch 743\n",
      "Starting train_step for batch 744\n",
      "Starting train_step for batch 745\n",
      "Starting train_step for batch 746\n",
      "Starting train_step for batch 747\n",
      "Starting train_step for batch 748\n",
      "Starting train_step for batch 749\n",
      "Starting train_step for batch 750\n",
      "Epoch 1, Step 750/1050\n",
      "Starting train_step for batch 751\n",
      "Starting train_step for batch 752\n",
      "Starting train_step for batch 753\n",
      "Starting train_step for batch 754\n",
      "Starting train_step for batch 755\n",
      "Starting train_step for batch 756\n",
      "Starting train_step for batch 757\n",
      "Starting train_step for batch 758\n",
      "Starting train_step for batch 759\n",
      "Starting train_step for batch 760\n",
      "Starting train_step for batch 761\n",
      "Starting train_step for batch 762\n",
      "Starting train_step for batch 763\n",
      "Starting train_step for batch 764\n",
      "Starting train_step for batch 765\n",
      "Starting train_step for batch 766\n",
      "Starting train_step for batch 767\n",
      "Starting train_step for batch 768\n",
      "Starting train_step for batch 769\n",
      "Starting train_step for batch 770\n",
      "Starting train_step for batch 771\n",
      "Starting train_step for batch 772\n",
      "Starting train_step for batch 773\n",
      "Starting train_step for batch 774\n",
      "Starting train_step for batch 775\n",
      "Starting train_step for batch 776\n",
      "Starting train_step for batch 777\n",
      "Starting train_step for batch 778\n",
      "Starting train_step for batch 779\n",
      "Starting train_step for batch 780\n",
      "Starting train_step for batch 781\n",
      "Starting train_step for batch 782\n",
      "Starting train_step for batch 783\n",
      "Starting train_step for batch 784\n",
      "Starting train_step for batch 785\n",
      "Starting train_step for batch 786\n",
      "Starting train_step for batch 787\n",
      "Starting train_step for batch 788\n",
      "Starting train_step for batch 789\n",
      "Starting train_step for batch 790\n",
      "Starting train_step for batch 791\n",
      "Starting train_step for batch 792\n",
      "Starting train_step for batch 793\n",
      "Starting train_step for batch 794\n",
      "Starting train_step for batch 795\n",
      "Starting train_step for batch 796\n",
      "Starting train_step for batch 797\n",
      "Starting train_step for batch 798\n",
      "Starting train_step for batch 799\n",
      "Starting train_step for batch 800\n",
      "Epoch 1, Step 800/1050\n",
      "Starting train_step for batch 801\n",
      "Starting train_step for batch 802\n",
      "Starting train_step for batch 803\n",
      "Starting train_step for batch 804\n",
      "Starting train_step for batch 805\n",
      "Starting train_step for batch 806\n",
      "Starting train_step for batch 807\n",
      "Starting train_step for batch 808\n",
      "Starting train_step for batch 809\n",
      "Starting train_step for batch 810\n",
      "Starting train_step for batch 811\n",
      "Starting train_step for batch 812\n",
      "Starting train_step for batch 813\n",
      "Starting train_step for batch 814\n",
      "Starting train_step for batch 815\n",
      "Starting train_step for batch 816\n",
      "Starting train_step for batch 817\n",
      "Starting train_step for batch 818\n",
      "Starting train_step for batch 819\n",
      "Starting train_step for batch 820\n",
      "Starting train_step for batch 821\n",
      "Starting train_step for batch 822\n",
      "Starting train_step for batch 823\n",
      "Starting train_step for batch 824\n",
      "Starting train_step for batch 825\n",
      "Starting train_step for batch 826\n",
      "Starting train_step for batch 827\n",
      "Starting train_step for batch 828\n",
      "Starting train_step for batch 829\n",
      "Starting train_step for batch 830\n",
      "Starting train_step for batch 831\n",
      "Starting train_step for batch 832\n",
      "Starting train_step for batch 833\n",
      "Starting train_step for batch 834\n",
      "Starting train_step for batch 835\n",
      "Starting train_step for batch 836\n",
      "Starting train_step for batch 837\n",
      "Starting train_step for batch 838\n",
      "Starting train_step for batch 839\n",
      "Starting train_step for batch 840\n",
      "Starting train_step for batch 841\n",
      "Starting train_step for batch 842\n",
      "Starting train_step for batch 843\n",
      "Starting train_step for batch 844\n",
      "Starting train_step for batch 845\n",
      "Starting train_step for batch 846\n",
      "Starting train_step for batch 847\n",
      "Starting train_step for batch 848\n",
      "Starting train_step for batch 849\n",
      "Starting train_step for batch 850\n",
      "Epoch 1, Step 850/1050\n",
      "Starting train_step for batch 851\n",
      "Starting train_step for batch 852\n",
      "Starting train_step for batch 853\n",
      "Starting train_step for batch 854\n",
      "Starting train_step for batch 855\n",
      "Starting train_step for batch 856\n",
      "Starting train_step for batch 857\n",
      "Starting train_step for batch 858\n",
      "Starting train_step for batch 859\n",
      "Starting train_step for batch 860\n",
      "Starting train_step for batch 861\n",
      "Starting train_step for batch 862\n",
      "Starting train_step for batch 863\n",
      "Starting train_step for batch 864\n",
      "Starting train_step for batch 865\n",
      "Starting train_step for batch 866\n",
      "Starting train_step for batch 867\n",
      "Starting train_step for batch 868\n",
      "Starting train_step for batch 869\n",
      "Starting train_step for batch 870\n",
      "Starting train_step for batch 871\n",
      "Starting train_step for batch 872\n",
      "Starting train_step for batch 873\n",
      "Starting train_step for batch 874\n",
      "Starting train_step for batch 875\n",
      "Starting train_step for batch 876\n",
      "Starting train_step for batch 877\n",
      "Starting train_step for batch 878\n",
      "Starting train_step for batch 879\n",
      "Starting train_step for batch 880\n",
      "Starting train_step for batch 881\n",
      "Starting train_step for batch 882\n",
      "Starting train_step for batch 883\n",
      "Starting train_step for batch 884\n",
      "Starting train_step for batch 885\n",
      "Starting train_step for batch 886\n",
      "Starting train_step for batch 887\n",
      "Starting train_step for batch 888\n",
      "Starting train_step for batch 889\n",
      "Starting train_step for batch 890\n",
      "Starting train_step for batch 891\n",
      "Starting train_step for batch 892\n",
      "Starting train_step for batch 893\n",
      "Starting train_step for batch 894\n",
      "Starting train_step for batch 895\n",
      "Starting train_step for batch 896\n",
      "Starting train_step for batch 897\n",
      "Starting train_step for batch 898\n",
      "Starting train_step for batch 899\n",
      "Starting train_step for batch 900\n",
      "Epoch 1, Step 900/1050\n",
      "Starting train_step for batch 901\n",
      "Starting train_step for batch 902\n",
      "Starting train_step for batch 903\n",
      "Starting train_step for batch 904\n",
      "Starting train_step for batch 905\n",
      "Starting train_step for batch 906\n",
      "Starting train_step for batch 907\n",
      "Starting train_step for batch 908\n",
      "Starting train_step for batch 909\n",
      "Starting train_step for batch 910\n",
      "Starting train_step for batch 911\n",
      "Starting train_step for batch 912\n",
      "Starting train_step for batch 913\n",
      "Starting train_step for batch 914\n",
      "Starting train_step for batch 915\n",
      "Starting train_step for batch 916\n",
      "Starting train_step for batch 917\n",
      "Starting train_step for batch 918\n",
      "Starting train_step for batch 919\n",
      "Starting train_step for batch 920\n",
      "Starting train_step for batch 921\n",
      "Starting train_step for batch 922\n",
      "Starting train_step for batch 923\n",
      "Starting train_step for batch 924\n",
      "Starting train_step for batch 925\n",
      "Starting train_step for batch 926\n",
      "Starting train_step for batch 927\n",
      "Starting train_step for batch 928\n",
      "Starting train_step for batch 929\n",
      "Starting train_step for batch 930\n",
      "Starting train_step for batch 931\n",
      "Starting train_step for batch 932\n",
      "Starting train_step for batch 933\n",
      "Starting train_step for batch 934\n",
      "Starting train_step for batch 935\n",
      "Starting train_step for batch 936\n",
      "Starting train_step for batch 937\n",
      "Starting train_step for batch 938\n",
      "Starting train_step for batch 939\n",
      "Starting train_step for batch 940\n",
      "Starting train_step for batch 941\n",
      "Starting train_step for batch 942\n",
      "Starting train_step for batch 943\n",
      "Starting train_step for batch 944\n",
      "Starting train_step for batch 945\n",
      "Starting train_step for batch 946\n",
      "Starting train_step for batch 947\n",
      "Starting train_step for batch 948\n",
      "Starting train_step for batch 949\n",
      "Starting train_step for batch 950\n",
      "Epoch 1, Step 950/1050\n",
      "Starting train_step for batch 951\n",
      "Starting train_step for batch 952\n",
      "Starting train_step for batch 953\n",
      "Starting train_step for batch 954\n",
      "Starting train_step for batch 955\n",
      "Starting train_step for batch 956\n",
      "Starting train_step for batch 957\n",
      "Starting train_step for batch 958\n",
      "Starting train_step for batch 959\n",
      "Starting train_step for batch 960\n",
      "Starting train_step for batch 961\n",
      "Starting train_step for batch 962\n",
      "Starting train_step for batch 963\n",
      "Starting train_step for batch 964\n",
      "Starting train_step for batch 965\n",
      "Starting train_step for batch 966\n",
      "Starting train_step for batch 967\n",
      "Starting train_step for batch 968\n",
      "Starting train_step for batch 969\n",
      "Starting train_step for batch 970\n",
      "Starting train_step for batch 971\n",
      "Starting train_step for batch 972\n",
      "Starting train_step for batch 973\n",
      "Starting train_step for batch 974\n",
      "Starting train_step for batch 975\n",
      "Starting train_step for batch 976\n",
      "Starting train_step for batch 977\n",
      "Starting train_step for batch 978\n",
      "Starting train_step for batch 979\n",
      "Starting train_step for batch 980\n",
      "Starting train_step for batch 981\n",
      "Starting train_step for batch 982\n",
      "Starting train_step for batch 983\n",
      "Starting train_step for batch 984\n",
      "Starting train_step for batch 985\n",
      "Starting train_step for batch 986\n",
      "Starting train_step for batch 987\n",
      "Starting train_step for batch 988\n",
      "Starting train_step for batch 989\n",
      "Starting train_step for batch 990\n",
      "Starting train_step for batch 991\n",
      "Starting train_step for batch 992\n",
      "Starting train_step for batch 993\n",
      "Starting train_step for batch 994\n",
      "Starting train_step for batch 995\n",
      "Starting train_step for batch 996\n",
      "Starting train_step for batch 997\n",
      "Starting train_step for batch 998\n",
      "Starting train_step for batch 999\n",
      "Starting train_step for batch 1000\n",
      "Epoch 1, Step 1000/1050\n",
      "Starting train_step for batch 1001\n",
      "Starting train_step for batch 1002\n",
      "Starting train_step for batch 1003\n",
      "Starting train_step for batch 1004\n",
      "Starting train_step for batch 1005\n",
      "Starting train_step for batch 1006\n",
      "Starting train_step for batch 1007\n",
      "Starting train_step for batch 1008\n",
      "Starting train_step for batch 1009\n",
      "Starting train_step for batch 1010\n",
      "Starting train_step for batch 1011\n",
      "Starting train_step for batch 1012\n",
      "Starting train_step for batch 1013\n",
      "Starting train_step for batch 1014\n",
      "Starting train_step for batch 1015\n",
      "Starting train_step for batch 1016\n",
      "Starting train_step for batch 1017\n",
      "Starting train_step for batch 1018\n",
      "Starting train_step for batch 1019\n",
      "Starting train_step for batch 1020\n",
      "Starting train_step for batch 1021\n",
      "Starting train_step for batch 1022\n",
      "Starting train_step for batch 1023\n",
      "Starting train_step for batch 1024\n",
      "Starting train_step for batch 1025\n",
      "Starting train_step for batch 1026\n",
      "Starting train_step for batch 1027\n",
      "Starting train_step for batch 1028\n",
      "Starting train_step for batch 1029\n",
      "Starting train_step for batch 1030\n",
      "Starting train_step for batch 1031\n",
      "Starting train_step for batch 1032\n",
      "Starting train_step for batch 1033\n",
      "Starting train_step for batch 1034\n",
      "Starting train_step for batch 1035\n",
      "Starting train_step for batch 1036\n",
      "Starting train_step for batch 1037\n",
      "Starting train_step for batch 1038\n",
      "Starting train_step for batch 1039\n",
      "Starting train_step for batch 1040\n",
      "Starting train_step for batch 1041\n",
      "Starting train_step for batch 1042\n",
      "Starting train_step for batch 1043\n",
      "Starting train_step for batch 1044\n",
      "Starting train_step for batch 1045\n",
      "Starting train_step for batch 1046\n",
      "Starting train_step for batch 1047\n",
      "Starting train_step for batch 1048\n",
      "Starting train_step for batch 1049\n",
      "Starting train_step for batch 1050\n",
      "Epoch 1, Step 1050/1050\n",
      "Epoch 1 - Average train loss: 0.0001\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/1050\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/1050\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/1050\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/1050\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/1050\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 2, Step 300/1050\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 2, Step 350/1050\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 2, Step 400/1050\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 2, Step 450/1050\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 2, Step 500/1050\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Starting train_step for batch 526\n",
      "Starting train_step for batch 527\n",
      "Starting train_step for batch 528\n",
      "Starting train_step for batch 529\n",
      "Starting train_step for batch 530\n",
      "Starting train_step for batch 531\n",
      "Starting train_step for batch 532\n",
      "Starting train_step for batch 533\n",
      "Starting train_step for batch 534\n",
      "Starting train_step for batch 535\n",
      "Starting train_step for batch 536\n",
      "Starting train_step for batch 537\n",
      "Starting train_step for batch 538\n",
      "Starting train_step for batch 539\n",
      "Starting train_step for batch 540\n",
      "Starting train_step for batch 541\n",
      "Starting train_step for batch 542\n",
      "Starting train_step for batch 543\n",
      "Starting train_step for batch 544\n",
      "Starting train_step for batch 545\n",
      "Starting train_step for batch 546\n",
      "Starting train_step for batch 547\n",
      "Starting train_step for batch 548\n",
      "Starting train_step for batch 549\n",
      "Starting train_step for batch 550\n",
      "Epoch 2, Step 550/1050\n",
      "Starting train_step for batch 551\n",
      "Starting train_step for batch 552\n",
      "Starting train_step for batch 553\n",
      "Starting train_step for batch 554\n",
      "Starting train_step for batch 555\n",
      "Starting train_step for batch 556\n",
      "Starting train_step for batch 557\n",
      "Starting train_step for batch 558\n",
      "Starting train_step for batch 559\n",
      "Starting train_step for batch 560\n",
      "Starting train_step for batch 561\n",
      "Starting train_step for batch 562\n",
      "Starting train_step for batch 563\n",
      "Starting train_step for batch 564\n",
      "Starting train_step for batch 565\n",
      "Starting train_step for batch 566\n",
      "Starting train_step for batch 567\n",
      "Starting train_step for batch 568\n",
      "Starting train_step for batch 569\n",
      "Starting train_step for batch 570\n",
      "Starting train_step for batch 571\n",
      "Starting train_step for batch 572\n",
      "Starting train_step for batch 573\n",
      "Starting train_step for batch 574\n",
      "Starting train_step for batch 575\n",
      "Starting train_step for batch 576\n",
      "Starting train_step for batch 577\n",
      "Starting train_step for batch 578\n",
      "Starting train_step for batch 579\n",
      "Starting train_step for batch 580\n",
      "Starting train_step for batch 581\n",
      "Starting train_step for batch 582\n",
      "Starting train_step for batch 583\n",
      "Starting train_step for batch 584\n",
      "Starting train_step for batch 585\n",
      "Starting train_step for batch 586\n",
      "Starting train_step for batch 587\n",
      "Starting train_step for batch 588\n",
      "Starting train_step for batch 589\n",
      "Starting train_step for batch 590\n",
      "Starting train_step for batch 591\n",
      "Starting train_step for batch 592\n",
      "Starting train_step for batch 593\n",
      "Starting train_step for batch 594\n",
      "Starting train_step for batch 595\n",
      "Starting train_step for batch 596\n",
      "Starting train_step for batch 597\n",
      "Starting train_step for batch 598\n",
      "Starting train_step for batch 599\n",
      "Starting train_step for batch 600\n",
      "Epoch 2, Step 600/1050\n",
      "Starting train_step for batch 601\n",
      "Starting train_step for batch 602\n",
      "Starting train_step for batch 603\n",
      "Starting train_step for batch 604\n",
      "Starting train_step for batch 605\n",
      "Starting train_step for batch 606\n",
      "Starting train_step for batch 607\n",
      "Starting train_step for batch 608\n",
      "Starting train_step for batch 609\n",
      "Starting train_step for batch 610\n",
      "Starting train_step for batch 611\n",
      "Starting train_step for batch 612\n",
      "Starting train_step for batch 613\n",
      "Starting train_step for batch 614\n",
      "Starting train_step for batch 615\n",
      "Starting train_step for batch 616\n",
      "Starting train_step for batch 617\n",
      "Starting train_step for batch 618\n",
      "Starting train_step for batch 619\n",
      "Starting train_step for batch 620\n",
      "Starting train_step for batch 621\n",
      "Starting train_step for batch 622\n",
      "Starting train_step for batch 623\n",
      "Starting train_step for batch 624\n",
      "Starting train_step for batch 625\n",
      "Starting train_step for batch 626\n",
      "Starting train_step for batch 627\n",
      "Starting train_step for batch 628\n",
      "Starting train_step for batch 629\n",
      "Starting train_step for batch 630\n",
      "Starting train_step for batch 631\n",
      "Starting train_step for batch 632\n",
      "Starting train_step for batch 633\n",
      "Starting train_step for batch 634\n",
      "Starting train_step for batch 635\n",
      "Starting train_step for batch 636\n",
      "Starting train_step for batch 637\n",
      "Starting train_step for batch 638\n",
      "Starting train_step for batch 639\n",
      "Starting train_step for batch 640\n",
      "Starting train_step for batch 641\n",
      "Starting train_step for batch 642\n",
      "Starting train_step for batch 643\n",
      "Starting train_step for batch 644\n",
      "Starting train_step for batch 645\n",
      "Starting train_step for batch 646\n",
      "Starting train_step for batch 647\n",
      "Starting train_step for batch 648\n",
      "Starting train_step for batch 649\n",
      "Starting train_step for batch 650\n",
      "Epoch 2, Step 650/1050\n",
      "Starting train_step for batch 651\n",
      "Starting train_step for batch 652\n",
      "Starting train_step for batch 653\n",
      "Starting train_step for batch 654\n",
      "Starting train_step for batch 655\n",
      "Starting train_step for batch 656\n",
      "Starting train_step for batch 657\n",
      "Starting train_step for batch 658\n",
      "Starting train_step for batch 659\n",
      "Starting train_step for batch 660\n",
      "Starting train_step for batch 661\n",
      "Starting train_step for batch 662\n",
      "Starting train_step for batch 663\n",
      "Starting train_step for batch 664\n",
      "Starting train_step for batch 665\n",
      "Starting train_step for batch 666\n",
      "Starting train_step for batch 667\n",
      "Starting train_step for batch 668\n",
      "Starting train_step for batch 669\n",
      "Starting train_step for batch 670\n",
      "Starting train_step for batch 671\n",
      "Starting train_step for batch 672\n",
      "Starting train_step for batch 673\n",
      "Starting train_step for batch 674\n",
      "Starting train_step for batch 675\n",
      "Starting train_step for batch 676\n",
      "Starting train_step for batch 677\n",
      "Starting train_step for batch 678\n",
      "Starting train_step for batch 679\n",
      "Starting train_step for batch 680\n",
      "Starting train_step for batch 681\n",
      "Starting train_step for batch 682\n",
      "Starting train_step for batch 683\n",
      "Starting train_step for batch 684\n",
      "Starting train_step for batch 685\n",
      "Starting train_step for batch 686\n",
      "Starting train_step for batch 687\n",
      "Starting train_step for batch 688\n",
      "Starting train_step for batch 689\n",
      "Starting train_step for batch 690\n",
      "Starting train_step for batch 691\n",
      "Starting train_step for batch 692\n",
      "Starting train_step for batch 693\n",
      "Starting train_step for batch 694\n",
      "Starting train_step for batch 695\n",
      "Starting train_step for batch 696\n",
      "Starting train_step for batch 697\n",
      "Starting train_step for batch 698\n",
      "Starting train_step for batch 699\n",
      "Starting train_step for batch 700\n",
      "Epoch 2, Step 700/1050\n",
      "Starting train_step for batch 701\n",
      "Starting train_step for batch 702\n",
      "Starting train_step for batch 703\n",
      "Starting train_step for batch 704\n",
      "Starting train_step for batch 705\n",
      "Starting train_step for batch 706\n",
      "Starting train_step for batch 707\n",
      "Starting train_step for batch 708\n",
      "Starting train_step for batch 709\n",
      "Starting train_step for batch 710\n",
      "Starting train_step for batch 711\n",
      "Starting train_step for batch 712\n",
      "Starting train_step for batch 713\n",
      "Starting train_step for batch 714\n",
      "Starting train_step for batch 715\n",
      "Starting train_step for batch 716\n",
      "Starting train_step for batch 717\n",
      "Starting train_step for batch 718\n",
      "Starting train_step for batch 719\n",
      "Starting train_step for batch 720\n",
      "Starting train_step for batch 721\n",
      "Starting train_step for batch 722\n",
      "Starting train_step for batch 723\n",
      "Starting train_step for batch 724\n",
      "Starting train_step for batch 725\n",
      "Starting train_step for batch 726\n",
      "Starting train_step for batch 727\n",
      "Starting train_step for batch 728\n",
      "Starting train_step for batch 729\n",
      "Starting train_step for batch 730\n",
      "Starting train_step for batch 731\n",
      "Starting train_step for batch 732\n",
      "Starting train_step for batch 733\n",
      "Starting train_step for batch 734\n",
      "Starting train_step for batch 735\n",
      "Starting train_step for batch 736\n",
      "Starting train_step for batch 737\n",
      "Starting train_step for batch 738\n",
      "Starting train_step for batch 739\n",
      "Starting train_step for batch 740\n",
      "Starting train_step for batch 741\n",
      "Starting train_step for batch 742\n",
      "Starting train_step for batch 743\n",
      "Starting train_step for batch 744\n",
      "Starting train_step for batch 745\n",
      "Starting train_step for batch 746\n",
      "Starting train_step for batch 747\n",
      "Starting train_step for batch 748\n",
      "Starting train_step for batch 749\n",
      "Starting train_step for batch 750\n",
      "Epoch 2, Step 750/1050\n",
      "Starting train_step for batch 751\n",
      "Starting train_step for batch 752\n",
      "Starting train_step for batch 753\n",
      "Starting train_step for batch 754\n",
      "Starting train_step for batch 755\n",
      "Starting train_step for batch 756\n",
      "Starting train_step for batch 757\n",
      "Starting train_step for batch 758\n",
      "Starting train_step for batch 759\n",
      "Starting train_step for batch 760\n",
      "Starting train_step for batch 761\n",
      "Starting train_step for batch 762\n",
      "Starting train_step for batch 763\n",
      "Starting train_step for batch 764\n",
      "Starting train_step for batch 765\n",
      "Starting train_step for batch 766\n",
      "Starting train_step for batch 767\n",
      "Starting train_step for batch 768\n",
      "Starting train_step for batch 769\n",
      "Starting train_step for batch 770\n",
      "Starting train_step for batch 771\n",
      "Starting train_step for batch 772\n",
      "Starting train_step for batch 773\n",
      "Starting train_step for batch 774\n",
      "Starting train_step for batch 775\n",
      "Starting train_step for batch 776\n",
      "Starting train_step for batch 777\n",
      "Starting train_step for batch 778\n",
      "Starting train_step for batch 779\n",
      "Starting train_step for batch 780\n",
      "Starting train_step for batch 781\n",
      "Starting train_step for batch 782\n",
      "Starting train_step for batch 783\n",
      "Starting train_step for batch 784\n",
      "Starting train_step for batch 785\n",
      "Starting train_step for batch 786\n",
      "Starting train_step for batch 787\n",
      "Starting train_step for batch 788\n",
      "Starting train_step for batch 789\n",
      "Starting train_step for batch 790\n",
      "Starting train_step for batch 791\n",
      "Starting train_step for batch 792\n",
      "Starting train_step for batch 793\n",
      "Starting train_step for batch 794\n",
      "Starting train_step for batch 795\n",
      "Starting train_step for batch 796\n",
      "Starting train_step for batch 797\n",
      "Starting train_step for batch 798\n",
      "Starting train_step for batch 799\n",
      "Starting train_step for batch 800\n",
      "Epoch 2, Step 800/1050\n",
      "Starting train_step for batch 801\n",
      "Starting train_step for batch 802\n",
      "Starting train_step for batch 803\n",
      "Starting train_step for batch 804\n",
      "Starting train_step for batch 805\n",
      "Starting train_step for batch 806\n",
      "Starting train_step for batch 807\n",
      "Starting train_step for batch 808\n",
      "Starting train_step for batch 809\n",
      "Starting train_step for batch 810\n",
      "Starting train_step for batch 811\n",
      "Starting train_step for batch 812\n",
      "Starting train_step for batch 813\n",
      "Starting train_step for batch 814\n",
      "Starting train_step for batch 815\n",
      "Starting train_step for batch 816\n",
      "Starting train_step for batch 817\n",
      "Starting train_step for batch 818\n",
      "Starting train_step for batch 819\n",
      "Starting train_step for batch 820\n",
      "Starting train_step for batch 821\n",
      "Starting train_step for batch 822\n",
      "Starting train_step for batch 823\n",
      "Starting train_step for batch 824\n",
      "Starting train_step for batch 825\n",
      "Starting train_step for batch 826\n",
      "Starting train_step for batch 827\n",
      "Starting train_step for batch 828\n",
      "Starting train_step for batch 829\n",
      "Starting train_step for batch 830\n",
      "Starting train_step for batch 831\n",
      "Starting train_step for batch 832\n",
      "Starting train_step for batch 833\n",
      "Starting train_step for batch 834\n",
      "Starting train_step for batch 835\n",
      "Starting train_step for batch 836\n",
      "Starting train_step for batch 837\n",
      "Starting train_step for batch 838\n",
      "Starting train_step for batch 839\n",
      "Starting train_step for batch 840\n",
      "Starting train_step for batch 841\n",
      "Starting train_step for batch 842\n",
      "Starting train_step for batch 843\n",
      "Starting train_step for batch 844\n",
      "Starting train_step for batch 845\n",
      "Starting train_step for batch 846\n",
      "Starting train_step for batch 847\n",
      "Starting train_step for batch 848\n",
      "Starting train_step for batch 849\n",
      "Starting train_step for batch 850\n",
      "Epoch 2, Step 850/1050\n",
      "Starting train_step for batch 851\n",
      "Starting train_step for batch 852\n",
      "Starting train_step for batch 853\n",
      "Starting train_step for batch 854\n",
      "Starting train_step for batch 855\n",
      "Starting train_step for batch 856\n",
      "Starting train_step for batch 857\n",
      "Starting train_step for batch 858\n",
      "Starting train_step for batch 859\n",
      "Starting train_step for batch 860\n",
      "Starting train_step for batch 861\n",
      "Starting train_step for batch 862\n",
      "Starting train_step for batch 863\n",
      "Starting train_step for batch 864\n",
      "Starting train_step for batch 865\n",
      "Starting train_step for batch 866\n",
      "Starting train_step for batch 867\n",
      "Starting train_step for batch 868\n",
      "Starting train_step for batch 869\n",
      "Starting train_step for batch 870\n",
      "Starting train_step for batch 871\n",
      "Starting train_step for batch 872\n",
      "Starting train_step for batch 873\n",
      "Starting train_step for batch 874\n",
      "Starting train_step for batch 875\n",
      "Starting train_step for batch 876\n",
      "Starting train_step for batch 877\n",
      "Starting train_step for batch 878\n",
      "Starting train_step for batch 879\n",
      "Starting train_step for batch 880\n",
      "Starting train_step for batch 881\n",
      "Starting train_step for batch 882\n",
      "Starting train_step for batch 883\n",
      "Starting train_step for batch 884\n",
      "Starting train_step for batch 885\n",
      "Starting train_step for batch 886\n",
      "Starting train_step for batch 887\n",
      "Starting train_step for batch 888\n",
      "Starting train_step for batch 889\n",
      "Starting train_step for batch 890\n",
      "Starting train_step for batch 891\n",
      "Starting train_step for batch 892\n",
      "Starting train_step for batch 893\n",
      "Starting train_step for batch 894\n",
      "Starting train_step for batch 895\n",
      "Starting train_step for batch 896\n",
      "Starting train_step for batch 897\n",
      "Starting train_step for batch 898\n",
      "Starting train_step for batch 899\n",
      "Starting train_step for batch 900\n",
      "Epoch 2, Step 900/1050\n",
      "Starting train_step for batch 901\n",
      "Starting train_step for batch 902\n",
      "Starting train_step for batch 903\n",
      "Starting train_step for batch 904\n",
      "Starting train_step for batch 905\n",
      "Starting train_step for batch 906\n",
      "Starting train_step for batch 907\n",
      "Starting train_step for batch 908\n",
      "Starting train_step for batch 909\n",
      "Starting train_step for batch 910\n",
      "Starting train_step for batch 911\n",
      "Starting train_step for batch 912\n",
      "Starting train_step for batch 913\n",
      "Starting train_step for batch 914\n",
      "Starting train_step for batch 915\n",
      "Starting train_step for batch 916\n",
      "Starting train_step for batch 917\n",
      "Starting train_step for batch 918\n",
      "Starting train_step for batch 919\n",
      "Starting train_step for batch 920\n",
      "Starting train_step for batch 921\n",
      "Starting train_step for batch 922\n",
      "Starting train_step for batch 923\n",
      "Starting train_step for batch 924\n",
      "Starting train_step for batch 925\n",
      "Starting train_step for batch 926\n",
      "Starting train_step for batch 927\n",
      "Starting train_step for batch 928\n",
      "Starting train_step for batch 929\n",
      "Starting train_step for batch 930\n",
      "Starting train_step for batch 931\n",
      "Starting train_step for batch 932\n",
      "Starting train_step for batch 933\n",
      "Starting train_step for batch 934\n",
      "Starting train_step for batch 935\n",
      "Starting train_step for batch 936\n",
      "Starting train_step for batch 937\n",
      "Starting train_step for batch 938\n",
      "Starting train_step for batch 939\n",
      "Starting train_step for batch 940\n",
      "Starting train_step for batch 941\n",
      "Starting train_step for batch 942\n",
      "Starting train_step for batch 943\n",
      "Starting train_step for batch 944\n",
      "Starting train_step for batch 945\n",
      "Starting train_step for batch 946\n",
      "Starting train_step for batch 947\n",
      "Starting train_step for batch 948\n",
      "Starting train_step for batch 949\n",
      "Starting train_step for batch 950\n",
      "Epoch 2, Step 950/1050\n",
      "Starting train_step for batch 951\n",
      "Starting train_step for batch 952\n",
      "Starting train_step for batch 953\n",
      "Starting train_step for batch 954\n",
      "Starting train_step for batch 955\n",
      "Starting train_step for batch 956\n",
      "Starting train_step for batch 957\n",
      "Starting train_step for batch 958\n",
      "Starting train_step for batch 959\n",
      "Starting train_step for batch 960\n",
      "Starting train_step for batch 961\n",
      "Starting train_step for batch 962\n",
      "Starting train_step for batch 963\n",
      "Starting train_step for batch 964\n",
      "Starting train_step for batch 965\n",
      "Starting train_step for batch 966\n",
      "Starting train_step for batch 967\n",
      "Starting train_step for batch 968\n",
      "Starting train_step for batch 969\n",
      "Starting train_step for batch 970\n",
      "Starting train_step for batch 971\n",
      "Starting train_step for batch 972\n",
      "Starting train_step for batch 973\n",
      "Starting train_step for batch 974\n",
      "Starting train_step for batch 975\n",
      "Starting train_step for batch 976\n",
      "Starting train_step for batch 977\n",
      "Starting train_step for batch 978\n",
      "Starting train_step for batch 979\n",
      "Starting train_step for batch 980\n",
      "Starting train_step for batch 981\n",
      "Starting train_step for batch 982\n",
      "Starting train_step for batch 983\n",
      "Starting train_step for batch 984\n",
      "Starting train_step for batch 985\n",
      "Starting train_step for batch 986\n",
      "Starting train_step for batch 987\n",
      "Starting train_step for batch 988\n",
      "Starting train_step for batch 989\n",
      "Starting train_step for batch 990\n",
      "Starting train_step for batch 991\n",
      "Starting train_step for batch 992\n",
      "Starting train_step for batch 993\n",
      "Starting train_step for batch 994\n",
      "Starting train_step for batch 995\n",
      "Starting train_step for batch 996\n",
      "Starting train_step for batch 997\n",
      "Starting train_step for batch 998\n",
      "Starting train_step for batch 999\n",
      "Starting train_step for batch 1000\n",
      "Epoch 2, Step 1000/1050\n",
      "Starting train_step for batch 1001\n",
      "Starting train_step for batch 1002\n",
      "Starting train_step for batch 1003\n",
      "Starting train_step for batch 1004\n",
      "Starting train_step for batch 1005\n",
      "Starting train_step for batch 1006\n",
      "Starting train_step for batch 1007\n",
      "Starting train_step for batch 1008\n",
      "Starting train_step for batch 1009\n",
      "Starting train_step for batch 1010\n",
      "Starting train_step for batch 1011\n",
      "Starting train_step for batch 1012\n",
      "Starting train_step for batch 1013\n",
      "Starting train_step for batch 1014\n",
      "Starting train_step for batch 1015\n",
      "Starting train_step for batch 1016\n",
      "Starting train_step for batch 1017\n",
      "Starting train_step for batch 1018\n",
      "Starting train_step for batch 1019\n",
      "Starting train_step for batch 1020\n",
      "Starting train_step for batch 1021\n",
      "Starting train_step for batch 1022\n",
      "Starting train_step for batch 1023\n",
      "Starting train_step for batch 1024\n",
      "Starting train_step for batch 1025\n",
      "Starting train_step for batch 1026\n",
      "Starting train_step for batch 1027\n",
      "Starting train_step for batch 1028\n",
      "Starting train_step for batch 1029\n",
      "Starting train_step for batch 1030\n",
      "Starting train_step for batch 1031\n",
      "Starting train_step for batch 1032\n",
      "Starting train_step for batch 1033\n",
      "Starting train_step for batch 1034\n",
      "Starting train_step for batch 1035\n",
      "Starting train_step for batch 1036\n",
      "Starting train_step for batch 1037\n",
      "Starting train_step for batch 1038\n",
      "Starting train_step for batch 1039\n",
      "Starting train_step for batch 1040\n",
      "Starting train_step for batch 1041\n",
      "Starting train_step for batch 1042\n",
      "Starting train_step for batch 1043\n",
      "Starting train_step for batch 1044\n",
      "Starting train_step for batch 1045\n",
      "Starting train_step for batch 1046\n",
      "Starting train_step for batch 1047\n",
      "Starting train_step for batch 1048\n",
      "Starting train_step for batch 1049\n",
      "Starting train_step for batch 1050\n",
      "Epoch 2, Step 1050/1050\n",
      "Epoch 2 - Average train loss: 0.0001\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.64GB (Max: 70.33GB), Reserved: 54.72GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 10.706799\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [76]\n",
      "  Predicted Slots: [76]\n",
      "Example 2:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [76 76 76]\n",
      "  Predicted Slots: [76 76 76]\n",
      "Example 3:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [76 76 76 76]\n",
      "  Predicted Slots: [76 76 76 76]\n",
      "Example 4:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [76]\n",
      "  Predicted Slots: [76]\n",
      "Example 5:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [76]\n",
      "  Predicted Slots: [76]\n",
      "Unique true intents: {40}\n",
      "Unique predicted intents: {40}\n",
      "Unique true slots: {76}\n",
      "Unique predicted slots: {76}\n",
      "Cleared GPU memory. Current allocated: 58.68 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:14<00:00, 123.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.01it/s]\n",
      "Epoch 1/2 Evaluation results: {'translation_score': 0.11673181392920659, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 10.706798553466797}\n",
      "Starting epoch 2/2\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 58.69 GB\n",
      "GPU memory cached: 58.84 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n",
      "Updating projection layer: 5120 -> 4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 223\n",
      "Score statistics: Min: 0.0054, Max: 0.2625, Median: 0.1112, Average: 0.1167\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n",
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.66GB (Max: 70.33GB), Reserved: 54.80GB (Max: 71.27GB)\n",
      "Total training steps: 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/1050\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/1050\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/1050\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/1050\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/1050\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 1, Step 300/1050\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 1, Step 350/1050\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 1, Step 400/1050\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 1, Step 450/1050\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 1, Step 500/1050\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Starting train_step for batch 526\n",
      "Starting train_step for batch 527\n",
      "Starting train_step for batch 528\n",
      "Starting train_step for batch 529\n",
      "Starting train_step for batch 530\n",
      "Starting train_step for batch 531\n",
      "Starting train_step for batch 532\n",
      "Starting train_step for batch 533\n",
      "Starting train_step for batch 534\n",
      "Starting train_step for batch 535\n",
      "Starting train_step for batch 536\n",
      "Starting train_step for batch 537\n",
      "Starting train_step for batch 538\n",
      "Starting train_step for batch 539\n",
      "Starting train_step for batch 540\n",
      "Starting train_step for batch 541\n",
      "Starting train_step for batch 542\n",
      "Starting train_step for batch 543\n",
      "Starting train_step for batch 544\n",
      "Starting train_step for batch 545\n",
      "Starting train_step for batch 546\n",
      "Starting train_step for batch 547\n",
      "Starting train_step for batch 548\n",
      "Starting train_step for batch 549\n",
      "Starting train_step for batch 550\n",
      "Epoch 1, Step 550/1050\n",
      "Starting train_step for batch 551\n",
      "Starting train_step for batch 552\n",
      "Starting train_step for batch 553\n",
      "Starting train_step for batch 554\n",
      "Starting train_step for batch 555\n",
      "Starting train_step for batch 556\n",
      "Starting train_step for batch 557\n",
      "Starting train_step for batch 558\n",
      "Starting train_step for batch 559\n",
      "Starting train_step for batch 560\n",
      "Starting train_step for batch 561\n",
      "Starting train_step for batch 562\n",
      "Starting train_step for batch 563\n",
      "Starting train_step for batch 564\n",
      "Starting train_step for batch 565\n",
      "Starting train_step for batch 566\n",
      "Starting train_step for batch 567\n",
      "Starting train_step for batch 568\n",
      "Starting train_step for batch 569\n",
      "Starting train_step for batch 570\n",
      "Starting train_step for batch 571\n",
      "Starting train_step for batch 572\n",
      "Starting train_step for batch 573\n",
      "Starting train_step for batch 574\n",
      "Starting train_step for batch 575\n",
      "Starting train_step for batch 576\n",
      "Starting train_step for batch 577\n",
      "Starting train_step for batch 578\n",
      "Starting train_step for batch 579\n",
      "Starting train_step for batch 580\n",
      "Starting train_step for batch 581\n",
      "Starting train_step for batch 582\n",
      "Starting train_step for batch 583\n",
      "Starting train_step for batch 584\n",
      "Starting train_step for batch 585\n",
      "Starting train_step for batch 586\n",
      "Starting train_step for batch 587\n",
      "Starting train_step for batch 588\n",
      "Starting train_step for batch 589\n",
      "Starting train_step for batch 590\n",
      "Starting train_step for batch 591\n",
      "Starting train_step for batch 592\n",
      "Starting train_step for batch 593\n",
      "Starting train_step for batch 594\n",
      "Starting train_step for batch 595\n",
      "Starting train_step for batch 596\n",
      "Starting train_step for batch 597\n",
      "Starting train_step for batch 598\n",
      "Starting train_step for batch 599\n",
      "Starting train_step for batch 600\n",
      "Epoch 1, Step 600/1050\n",
      "Starting train_step for batch 601\n",
      "Starting train_step for batch 602\n",
      "Starting train_step for batch 603\n",
      "Starting train_step for batch 604\n",
      "Starting train_step for batch 605\n",
      "Starting train_step for batch 606\n",
      "Starting train_step for batch 607\n",
      "Starting train_step for batch 608\n",
      "Starting train_step for batch 609\n",
      "Starting train_step for batch 610\n",
      "Starting train_step for batch 611\n",
      "Starting train_step for batch 612\n",
      "Starting train_step for batch 613\n",
      "Starting train_step for batch 614\n",
      "Starting train_step for batch 615\n",
      "Starting train_step for batch 616\n",
      "Starting train_step for batch 617\n",
      "Starting train_step for batch 618\n",
      "Starting train_step for batch 619\n",
      "Starting train_step for batch 620\n",
      "Starting train_step for batch 621\n",
      "Starting train_step for batch 622\n",
      "Starting train_step for batch 623\n",
      "Starting train_step for batch 624\n",
      "Starting train_step for batch 625\n",
      "Starting train_step for batch 626\n",
      "Starting train_step for batch 627\n",
      "Starting train_step for batch 628\n",
      "Starting train_step for batch 629\n",
      "Starting train_step for batch 630\n",
      "Starting train_step for batch 631\n",
      "Starting train_step for batch 632\n",
      "Starting train_step for batch 633\n",
      "Starting train_step for batch 634\n",
      "Starting train_step for batch 635\n",
      "Starting train_step for batch 636\n",
      "Starting train_step for batch 637\n",
      "Starting train_step for batch 638\n",
      "Starting train_step for batch 639\n",
      "Starting train_step for batch 640\n",
      "Starting train_step for batch 641\n",
      "Starting train_step for batch 642\n",
      "Starting train_step for batch 643\n",
      "Starting train_step for batch 644\n",
      "Starting train_step for batch 645\n",
      "Starting train_step for batch 646\n",
      "Starting train_step for batch 647\n",
      "Starting train_step for batch 648\n",
      "Starting train_step for batch 649\n",
      "Starting train_step for batch 650\n",
      "Epoch 1, Step 650/1050\n",
      "Starting train_step for batch 651\n",
      "Starting train_step for batch 652\n",
      "Starting train_step for batch 653\n",
      "Starting train_step for batch 654\n",
      "Starting train_step for batch 655\n",
      "Starting train_step for batch 656\n",
      "Starting train_step for batch 657\n",
      "Starting train_step for batch 658\n",
      "Starting train_step for batch 659\n",
      "Starting train_step for batch 660\n",
      "Starting train_step for batch 661\n",
      "Starting train_step for batch 662\n",
      "Starting train_step for batch 663\n",
      "Starting train_step for batch 664\n",
      "Starting train_step for batch 665\n",
      "Starting train_step for batch 666\n",
      "Starting train_step for batch 667\n",
      "Starting train_step for batch 668\n",
      "Starting train_step for batch 669\n",
      "Starting train_step for batch 670\n",
      "Starting train_step for batch 671\n",
      "Starting train_step for batch 672\n",
      "Starting train_step for batch 673\n",
      "Starting train_step for batch 674\n",
      "Starting train_step for batch 675\n",
      "Starting train_step for batch 676\n",
      "Starting train_step for batch 677\n",
      "Starting train_step for batch 678\n",
      "Starting train_step for batch 679\n",
      "Starting train_step for batch 680\n",
      "Starting train_step for batch 681\n",
      "Starting train_step for batch 682\n",
      "Starting train_step for batch 683\n",
      "Starting train_step for batch 684\n",
      "Starting train_step for batch 685\n",
      "Starting train_step for batch 686\n",
      "Starting train_step for batch 687\n",
      "Starting train_step for batch 688\n",
      "Starting train_step for batch 689\n",
      "Starting train_step for batch 690\n",
      "Starting train_step for batch 691\n",
      "Starting train_step for batch 692\n",
      "Starting train_step for batch 693\n",
      "Starting train_step for batch 694\n",
      "Starting train_step for batch 695\n",
      "Starting train_step for batch 696\n",
      "Starting train_step for batch 697\n",
      "Starting train_step for batch 698\n",
      "Starting train_step for batch 699\n",
      "Starting train_step for batch 700\n",
      "Epoch 1, Step 700/1050\n",
      "Starting train_step for batch 701\n",
      "Starting train_step for batch 702\n",
      "Starting train_step for batch 703\n",
      "Starting train_step for batch 704\n",
      "Starting train_step for batch 705\n",
      "Starting train_step for batch 706\n",
      "Starting train_step for batch 707\n",
      "Starting train_step for batch 708\n",
      "Starting train_step for batch 709\n",
      "Starting train_step for batch 710\n",
      "Starting train_step for batch 711\n",
      "Starting train_step for batch 712\n",
      "Starting train_step for batch 713\n",
      "Starting train_step for batch 714\n",
      "Starting train_step for batch 715\n",
      "Starting train_step for batch 716\n",
      "Starting train_step for batch 717\n",
      "Starting train_step for batch 718\n",
      "Starting train_step for batch 719\n",
      "Starting train_step for batch 720\n",
      "Starting train_step for batch 721\n",
      "Starting train_step for batch 722\n",
      "Starting train_step for batch 723\n",
      "Starting train_step for batch 724\n",
      "Starting train_step for batch 725\n",
      "Starting train_step for batch 726\n",
      "Starting train_step for batch 727\n",
      "Starting train_step for batch 728\n",
      "Starting train_step for batch 729\n",
      "Starting train_step for batch 730\n",
      "Starting train_step for batch 731\n",
      "Starting train_step for batch 732\n",
      "Starting train_step for batch 733\n",
      "Starting train_step for batch 734\n",
      "Starting train_step for batch 735\n",
      "Starting train_step for batch 736\n",
      "Starting train_step for batch 737\n",
      "Starting train_step for batch 738\n",
      "Starting train_step for batch 739\n",
      "Starting train_step for batch 740\n",
      "Starting train_step for batch 741\n",
      "Starting train_step for batch 742\n",
      "Starting train_step for batch 743\n",
      "Starting train_step for batch 744\n",
      "Starting train_step for batch 745\n",
      "Starting train_step for batch 746\n",
      "Starting train_step for batch 747\n",
      "Starting train_step for batch 748\n",
      "Starting train_step for batch 749\n",
      "Starting train_step for batch 750\n",
      "Epoch 1, Step 750/1050\n",
      "Starting train_step for batch 751\n",
      "Starting train_step for batch 752\n",
      "Starting train_step for batch 753\n",
      "Starting train_step for batch 754\n",
      "Starting train_step for batch 755\n",
      "Starting train_step for batch 756\n",
      "Starting train_step for batch 757\n",
      "Starting train_step for batch 758\n",
      "Starting train_step for batch 759\n",
      "Starting train_step for batch 760\n",
      "Starting train_step for batch 761\n",
      "Starting train_step for batch 762\n",
      "Starting train_step for batch 763\n",
      "Starting train_step for batch 764\n",
      "Starting train_step for batch 765\n",
      "Starting train_step for batch 766\n",
      "Starting train_step for batch 767\n",
      "Starting train_step for batch 768\n",
      "Starting train_step for batch 769\n",
      "Starting train_step for batch 770\n",
      "Starting train_step for batch 771\n",
      "Starting train_step for batch 772\n",
      "Starting train_step for batch 773\n",
      "Starting train_step for batch 774\n",
      "Starting train_step for batch 775\n",
      "Starting train_step for batch 776\n",
      "Starting train_step for batch 777\n",
      "Starting train_step for batch 778\n",
      "Starting train_step for batch 779\n",
      "Starting train_step for batch 780\n",
      "Starting train_step for batch 781\n",
      "Starting train_step for batch 782\n",
      "Starting train_step for batch 783\n",
      "Starting train_step for batch 784\n",
      "Starting train_step for batch 785\n",
      "Starting train_step for batch 786\n",
      "Starting train_step for batch 787\n",
      "Starting train_step for batch 788\n",
      "Starting train_step for batch 789\n",
      "Starting train_step for batch 790\n",
      "Starting train_step for batch 791\n",
      "Starting train_step for batch 792\n",
      "Starting train_step for batch 793\n",
      "Starting train_step for batch 794\n",
      "Starting train_step for batch 795\n",
      "Starting train_step for batch 796\n",
      "Starting train_step for batch 797\n",
      "Starting train_step for batch 798\n",
      "Starting train_step for batch 799\n",
      "Starting train_step for batch 800\n",
      "Epoch 1, Step 800/1050\n",
      "Starting train_step for batch 801\n",
      "Starting train_step for batch 802\n",
      "Starting train_step for batch 803\n",
      "Starting train_step for batch 804\n",
      "Starting train_step for batch 805\n",
      "Starting train_step for batch 806\n",
      "Starting train_step for batch 807\n",
      "Starting train_step for batch 808\n",
      "Starting train_step for batch 809\n",
      "Starting train_step for batch 810\n",
      "Starting train_step for batch 811\n",
      "Starting train_step for batch 812\n",
      "Starting train_step for batch 813\n",
      "Starting train_step for batch 814\n",
      "Starting train_step for batch 815\n",
      "Starting train_step for batch 816\n",
      "Starting train_step for batch 817\n",
      "Starting train_step for batch 818\n",
      "Starting train_step for batch 819\n",
      "Starting train_step for batch 820\n",
      "Starting train_step for batch 821\n",
      "Starting train_step for batch 822\n",
      "Starting train_step for batch 823\n",
      "Starting train_step for batch 824\n",
      "Starting train_step for batch 825\n",
      "Starting train_step for batch 826\n",
      "Starting train_step for batch 827\n",
      "Starting train_step for batch 828\n",
      "Starting train_step for batch 829\n",
      "Starting train_step for batch 830\n",
      "Starting train_step for batch 831\n",
      "Starting train_step for batch 832\n",
      "Starting train_step for batch 833\n",
      "Starting train_step for batch 834\n",
      "Starting train_step for batch 835\n",
      "Starting train_step for batch 836\n",
      "Starting train_step for batch 837\n",
      "Starting train_step for batch 838\n",
      "Starting train_step for batch 839\n",
      "Starting train_step for batch 840\n",
      "Starting train_step for batch 841\n",
      "Starting train_step for batch 842\n",
      "Starting train_step for batch 843\n",
      "Starting train_step for batch 844\n",
      "Starting train_step for batch 845\n",
      "Starting train_step for batch 846\n",
      "Starting train_step for batch 847\n",
      "Starting train_step for batch 848\n",
      "Starting train_step for batch 849\n",
      "Starting train_step for batch 850\n",
      "Epoch 1, Step 850/1050\n",
      "Starting train_step for batch 851\n",
      "Starting train_step for batch 852\n",
      "Starting train_step for batch 853\n",
      "Starting train_step for batch 854\n",
      "Starting train_step for batch 855\n",
      "Starting train_step for batch 856\n",
      "Starting train_step for batch 857\n",
      "Starting train_step for batch 858\n",
      "Starting train_step for batch 859\n",
      "Starting train_step for batch 860\n",
      "Starting train_step for batch 861\n",
      "Starting train_step for batch 862\n",
      "Starting train_step for batch 863\n",
      "Starting train_step for batch 864\n",
      "Starting train_step for batch 865\n",
      "Starting train_step for batch 866\n",
      "Starting train_step for batch 867\n",
      "Starting train_step for batch 868\n",
      "Starting train_step for batch 869\n",
      "Starting train_step for batch 870\n",
      "Starting train_step for batch 871\n",
      "Starting train_step for batch 872\n",
      "Starting train_step for batch 873\n",
      "Starting train_step for batch 874\n",
      "Starting train_step for batch 875\n",
      "Starting train_step for batch 876\n",
      "Starting train_step for batch 877\n",
      "Starting train_step for batch 878\n",
      "Starting train_step for batch 879\n",
      "Starting train_step for batch 880\n",
      "Starting train_step for batch 881\n",
      "Starting train_step for batch 882\n",
      "Starting train_step for batch 883\n",
      "Starting train_step for batch 884\n",
      "Starting train_step for batch 885\n",
      "Starting train_step for batch 886\n",
      "Starting train_step for batch 887\n",
      "Starting train_step for batch 888\n",
      "Starting train_step for batch 889\n",
      "Starting train_step for batch 890\n",
      "Starting train_step for batch 891\n",
      "Starting train_step for batch 892\n",
      "Starting train_step for batch 893\n",
      "Starting train_step for batch 894\n",
      "Starting train_step for batch 895\n",
      "Starting train_step for batch 896\n",
      "Starting train_step for batch 897\n",
      "Starting train_step for batch 898\n",
      "Starting train_step for batch 899\n",
      "Starting train_step for batch 900\n",
      "Epoch 1, Step 900/1050\n",
      "Starting train_step for batch 901\n",
      "Starting train_step for batch 902\n",
      "Starting train_step for batch 903\n",
      "Starting train_step for batch 904\n",
      "Starting train_step for batch 905\n",
      "Starting train_step for batch 906\n",
      "Starting train_step for batch 907\n",
      "Starting train_step for batch 908\n",
      "Starting train_step for batch 909\n",
      "Starting train_step for batch 910\n",
      "Starting train_step for batch 911\n",
      "Starting train_step for batch 912\n",
      "Starting train_step for batch 913\n",
      "Starting train_step for batch 914\n",
      "Starting train_step for batch 915\n",
      "Starting train_step for batch 916\n",
      "Starting train_step for batch 917\n",
      "Starting train_step for batch 918\n",
      "Starting train_step for batch 919\n",
      "Starting train_step for batch 920\n",
      "Starting train_step for batch 921\n",
      "Starting train_step for batch 922\n",
      "Starting train_step for batch 923\n",
      "Starting train_step for batch 924\n",
      "Starting train_step for batch 925\n",
      "Starting train_step for batch 926\n",
      "Starting train_step for batch 927\n",
      "Starting train_step for batch 928\n",
      "Starting train_step for batch 929\n",
      "Starting train_step for batch 930\n",
      "Starting train_step for batch 931\n",
      "Starting train_step for batch 932\n",
      "Starting train_step for batch 933\n",
      "Starting train_step for batch 934\n",
      "Starting train_step for batch 935\n",
      "Starting train_step for batch 936\n",
      "Starting train_step for batch 937\n",
      "Starting train_step for batch 938\n",
      "Starting train_step for batch 939\n",
      "Starting train_step for batch 940\n",
      "Starting train_step for batch 941\n",
      "Starting train_step for batch 942\n",
      "Starting train_step for batch 943\n",
      "Starting train_step for batch 944\n",
      "Starting train_step for batch 945\n",
      "Starting train_step for batch 946\n",
      "Starting train_step for batch 947\n",
      "Starting train_step for batch 948\n",
      "Starting train_step for batch 949\n",
      "Starting train_step for batch 950\n",
      "Epoch 1, Step 950/1050\n",
      "Starting train_step for batch 951\n",
      "Starting train_step for batch 952\n",
      "Starting train_step for batch 953\n",
      "Starting train_step for batch 954\n",
      "Starting train_step for batch 955\n",
      "Starting train_step for batch 956\n",
      "Starting train_step for batch 957\n",
      "Starting train_step for batch 958\n",
      "Starting train_step for batch 959\n",
      "Starting train_step for batch 960\n",
      "Starting train_step for batch 961\n",
      "Starting train_step for batch 962\n",
      "Starting train_step for batch 963\n",
      "Starting train_step for batch 964\n",
      "Starting train_step for batch 965\n",
      "Starting train_step for batch 966\n",
      "Starting train_step for batch 967\n",
      "Starting train_step for batch 968\n",
      "Starting train_step for batch 969\n",
      "Starting train_step for batch 970\n",
      "Starting train_step for batch 971\n",
      "Starting train_step for batch 972\n",
      "Starting train_step for batch 973\n",
      "Starting train_step for batch 974\n",
      "Starting train_step for batch 975\n",
      "Starting train_step for batch 976\n",
      "Starting train_step for batch 977\n",
      "Starting train_step for batch 978\n",
      "Starting train_step for batch 979\n",
      "Starting train_step for batch 980\n",
      "Starting train_step for batch 981\n",
      "Starting train_step for batch 982\n",
      "Starting train_step for batch 983\n",
      "Starting train_step for batch 984\n",
      "Starting train_step for batch 985\n",
      "Starting train_step for batch 986\n",
      "Starting train_step for batch 987\n",
      "Starting train_step for batch 988\n",
      "Starting train_step for batch 989\n",
      "Starting train_step for batch 990\n",
      "Starting train_step for batch 991\n",
      "Starting train_step for batch 992\n",
      "Starting train_step for batch 993\n",
      "Starting train_step for batch 994\n",
      "Starting train_step for batch 995\n",
      "Starting train_step for batch 996\n",
      "Starting train_step for batch 997\n",
      "Starting train_step for batch 998\n",
      "Starting train_step for batch 999\n",
      "Starting train_step for batch 1000\n",
      "Epoch 1, Step 1000/1050\n",
      "Starting train_step for batch 1001\n",
      "Starting train_step for batch 1002\n",
      "Starting train_step for batch 1003\n",
      "Starting train_step for batch 1004\n",
      "Starting train_step for batch 1005\n",
      "Starting train_step for batch 1006\n",
      "Starting train_step for batch 1007\n",
      "Starting train_step for batch 1008\n",
      "Starting train_step for batch 1009\n",
      "Starting train_step for batch 1010\n",
      "Starting train_step for batch 1011\n",
      "Starting train_step for batch 1012\n",
      "Starting train_step for batch 1013\n",
      "Starting train_step for batch 1014\n",
      "Starting train_step for batch 1015\n",
      "Starting train_step for batch 1016\n",
      "Starting train_step for batch 1017\n",
      "Starting train_step for batch 1018\n",
      "Starting train_step for batch 1019\n",
      "Starting train_step for batch 1020\n",
      "Starting train_step for batch 1021\n",
      "Starting train_step for batch 1022\n",
      "Starting train_step for batch 1023\n",
      "Starting train_step for batch 1024\n",
      "Starting train_step for batch 1025\n",
      "Starting train_step for batch 1026\n",
      "Starting train_step for batch 1027\n",
      "Starting train_step for batch 1028\n",
      "Starting train_step for batch 1029\n",
      "Starting train_step for batch 1030\n",
      "Starting train_step for batch 1031\n",
      "Starting train_step for batch 1032\n",
      "Starting train_step for batch 1033\n",
      "Starting train_step for batch 1034\n",
      "Starting train_step for batch 1035\n",
      "Starting train_step for batch 1036\n",
      "Starting train_step for batch 1037\n",
      "Starting train_step for batch 1038\n",
      "Starting train_step for batch 1039\n",
      "Starting train_step for batch 1040\n",
      "Starting train_step for batch 1041\n",
      "Starting train_step for batch 1042\n",
      "Starting train_step for batch 1043\n",
      "Starting train_step for batch 1044\n",
      "Starting train_step for batch 1045\n",
      "Starting train_step for batch 1046\n",
      "Starting train_step for batch 1047\n",
      "Starting train_step for batch 1048\n",
      "Starting train_step for batch 1049\n",
      "Starting train_step for batch 1050\n",
      "Epoch 1, Step 1050/1050\n",
      "Epoch 1 - Average train loss: 0.0001\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/1050\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/1050\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/1050\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/1050\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/1050\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 2, Step 300/1050\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 2, Step 350/1050\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 2, Step 400/1050\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 2, Step 450/1050\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 2, Step 500/1050\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Starting train_step for batch 526\n",
      "Starting train_step for batch 527\n",
      "Starting train_step for batch 528\n",
      "Starting train_step for batch 529\n",
      "Starting train_step for batch 530\n",
      "Starting train_step for batch 531\n",
      "Starting train_step for batch 532\n",
      "Starting train_step for batch 533\n",
      "Starting train_step for batch 534\n",
      "Starting train_step for batch 535\n",
      "Starting train_step for batch 536\n",
      "Starting train_step for batch 537\n",
      "Starting train_step for batch 538\n",
      "Starting train_step for batch 539\n",
      "Starting train_step for batch 540\n",
      "Starting train_step for batch 541\n",
      "Starting train_step for batch 542\n",
      "Starting train_step for batch 543\n",
      "Starting train_step for batch 544\n",
      "Starting train_step for batch 545\n",
      "Starting train_step for batch 546\n",
      "Starting train_step for batch 547\n",
      "Starting train_step for batch 548\n",
      "Starting train_step for batch 549\n",
      "Starting train_step for batch 550\n",
      "Epoch 2, Step 550/1050\n",
      "Starting train_step for batch 551\n",
      "Starting train_step for batch 552\n",
      "Starting train_step for batch 553\n",
      "Starting train_step for batch 554\n",
      "Starting train_step for batch 555\n",
      "Starting train_step for batch 556\n",
      "Starting train_step for batch 557\n",
      "Starting train_step for batch 558\n",
      "Starting train_step for batch 559\n",
      "Starting train_step for batch 560\n",
      "Starting train_step for batch 561\n",
      "Starting train_step for batch 562\n",
      "Starting train_step for batch 563\n",
      "Starting train_step for batch 564\n",
      "Starting train_step for batch 565\n",
      "Starting train_step for batch 566\n",
      "Starting train_step for batch 567\n",
      "Starting train_step for batch 568\n",
      "Starting train_step for batch 569\n",
      "Starting train_step for batch 570\n",
      "Starting train_step for batch 571\n",
      "Starting train_step for batch 572\n",
      "Starting train_step for batch 573\n",
      "Starting train_step for batch 574\n",
      "Starting train_step for batch 575\n",
      "Starting train_step for batch 576\n",
      "Starting train_step for batch 577\n",
      "Starting train_step for batch 578\n",
      "Starting train_step for batch 579\n",
      "Starting train_step for batch 580\n",
      "Starting train_step for batch 581\n",
      "Starting train_step for batch 582\n",
      "Starting train_step for batch 583\n",
      "Starting train_step for batch 584\n",
      "Starting train_step for batch 585\n",
      "Starting train_step for batch 586\n",
      "Starting train_step for batch 587\n",
      "Starting train_step for batch 588\n",
      "Starting train_step for batch 589\n",
      "Starting train_step for batch 590\n",
      "Starting train_step for batch 591\n",
      "Starting train_step for batch 592\n",
      "Starting train_step for batch 593\n",
      "Starting train_step for batch 594\n",
      "Starting train_step for batch 595\n",
      "Starting train_step for batch 596\n",
      "Starting train_step for batch 597\n",
      "Starting train_step for batch 598\n",
      "Starting train_step for batch 599\n",
      "Starting train_step for batch 600\n",
      "Epoch 2, Step 600/1050\n",
      "Starting train_step for batch 601\n",
      "Starting train_step for batch 602\n",
      "Starting train_step for batch 603\n",
      "Starting train_step for batch 604\n",
      "Starting train_step for batch 605\n",
      "Starting train_step for batch 606\n",
      "Starting train_step for batch 607\n",
      "Starting train_step for batch 608\n",
      "Starting train_step for batch 609\n",
      "Starting train_step for batch 610\n",
      "Starting train_step for batch 611\n",
      "Starting train_step for batch 612\n",
      "Starting train_step for batch 613\n",
      "Starting train_step for batch 614\n",
      "Starting train_step for batch 615\n",
      "Starting train_step for batch 616\n",
      "Starting train_step for batch 617\n",
      "Starting train_step for batch 618\n",
      "Starting train_step for batch 619\n",
      "Starting train_step for batch 620\n",
      "Starting train_step for batch 621\n",
      "Starting train_step for batch 622\n",
      "Starting train_step for batch 623\n",
      "Starting train_step for batch 624\n",
      "Starting train_step for batch 625\n",
      "Starting train_step for batch 626\n",
      "Starting train_step for batch 627\n",
      "Starting train_step for batch 628\n",
      "Starting train_step for batch 629\n",
      "Starting train_step for batch 630\n",
      "Starting train_step for batch 631\n",
      "Starting train_step for batch 632\n",
      "Starting train_step for batch 633\n",
      "Starting train_step for batch 634\n",
      "Starting train_step for batch 635\n",
      "Starting train_step for batch 636\n",
      "Starting train_step for batch 637\n",
      "Starting train_step for batch 638\n",
      "Starting train_step for batch 639\n",
      "Starting train_step for batch 640\n",
      "Starting train_step for batch 641\n",
      "Starting train_step for batch 642\n",
      "Starting train_step for batch 643\n",
      "Starting train_step for batch 644\n",
      "Starting train_step for batch 645\n",
      "Starting train_step for batch 646\n",
      "Starting train_step for batch 647\n",
      "Starting train_step for batch 648\n",
      "Starting train_step for batch 649\n",
      "Starting train_step for batch 650\n",
      "Epoch 2, Step 650/1050\n",
      "Starting train_step for batch 651\n",
      "Starting train_step for batch 652\n",
      "Starting train_step for batch 653\n",
      "Starting train_step for batch 654\n",
      "Starting train_step for batch 655\n",
      "Starting train_step for batch 656\n",
      "Starting train_step for batch 657\n",
      "Starting train_step for batch 658\n",
      "Starting train_step for batch 659\n",
      "Starting train_step for batch 660\n",
      "Starting train_step for batch 661\n",
      "Starting train_step for batch 662\n",
      "Starting train_step for batch 663\n",
      "Starting train_step for batch 664\n",
      "Starting train_step for batch 665\n",
      "Starting train_step for batch 666\n",
      "Starting train_step for batch 667\n",
      "Starting train_step for batch 668\n",
      "Starting train_step for batch 669\n",
      "Starting train_step for batch 670\n",
      "Starting train_step for batch 671\n",
      "Starting train_step for batch 672\n",
      "Starting train_step for batch 673\n",
      "Starting train_step for batch 674\n",
      "Starting train_step for batch 675\n",
      "Starting train_step for batch 676\n",
      "Starting train_step for batch 677\n",
      "Starting train_step for batch 678\n",
      "Starting train_step for batch 679\n",
      "Starting train_step for batch 680\n",
      "Starting train_step for batch 681\n",
      "Starting train_step for batch 682\n",
      "Starting train_step for batch 683\n",
      "Starting train_step for batch 684\n",
      "Starting train_step for batch 685\n",
      "Starting train_step for batch 686\n",
      "Starting train_step for batch 687\n",
      "Starting train_step for batch 688\n",
      "Starting train_step for batch 689\n",
      "Starting train_step for batch 690\n",
      "Starting train_step for batch 691\n",
      "Starting train_step for batch 692\n",
      "Starting train_step for batch 693\n",
      "Starting train_step for batch 694\n",
      "Starting train_step for batch 695\n",
      "Starting train_step for batch 696\n",
      "Starting train_step for batch 697\n",
      "Starting train_step for batch 698\n",
      "Starting train_step for batch 699\n",
      "Starting train_step for batch 700\n",
      "Epoch 2, Step 700/1050\n",
      "Starting train_step for batch 701\n",
      "Starting train_step for batch 702\n",
      "Starting train_step for batch 703\n",
      "Starting train_step for batch 704\n",
      "Starting train_step for batch 705\n",
      "Starting train_step for batch 706\n",
      "Starting train_step for batch 707\n",
      "Starting train_step for batch 708\n",
      "Starting train_step for batch 709\n",
      "Starting train_step for batch 710\n",
      "Starting train_step for batch 711\n",
      "Starting train_step for batch 712\n",
      "Starting train_step for batch 713\n",
      "Starting train_step for batch 714\n",
      "Starting train_step for batch 715\n",
      "Starting train_step for batch 716\n",
      "Starting train_step for batch 717\n",
      "Starting train_step for batch 718\n",
      "Starting train_step for batch 719\n",
      "Starting train_step for batch 720\n",
      "Starting train_step for batch 721\n",
      "Starting train_step for batch 722\n",
      "Starting train_step for batch 723\n",
      "Starting train_step for batch 724\n",
      "Starting train_step for batch 725\n",
      "Starting train_step for batch 726\n",
      "Starting train_step for batch 727\n",
      "Starting train_step for batch 728\n",
      "Starting train_step for batch 729\n",
      "Starting train_step for batch 730\n",
      "Starting train_step for batch 731\n",
      "Starting train_step for batch 732\n",
      "Starting train_step for batch 733\n",
      "Starting train_step for batch 734\n",
      "Starting train_step for batch 735\n",
      "Starting train_step for batch 736\n",
      "Starting train_step for batch 737\n",
      "Starting train_step for batch 738\n",
      "Starting train_step for batch 739\n",
      "Starting train_step for batch 740\n",
      "Starting train_step for batch 741\n",
      "Starting train_step for batch 742\n",
      "Starting train_step for batch 743\n",
      "Starting train_step for batch 744\n",
      "Starting train_step for batch 745\n",
      "Starting train_step for batch 746\n",
      "Starting train_step for batch 747\n",
      "Starting train_step for batch 748\n",
      "Starting train_step for batch 749\n",
      "Starting train_step for batch 750\n",
      "Epoch 2, Step 750/1050\n",
      "Starting train_step for batch 751\n",
      "Starting train_step for batch 752\n",
      "Starting train_step for batch 753\n",
      "Starting train_step for batch 754\n",
      "Starting train_step for batch 755\n",
      "Starting train_step for batch 756\n",
      "Starting train_step for batch 757\n",
      "Starting train_step for batch 758\n",
      "Starting train_step for batch 759\n",
      "Starting train_step for batch 760\n",
      "Starting train_step for batch 761\n",
      "Starting train_step for batch 762\n",
      "Starting train_step for batch 763\n",
      "Starting train_step for batch 764\n",
      "Starting train_step for batch 765\n",
      "Starting train_step for batch 766\n",
      "Starting train_step for batch 767\n",
      "Starting train_step for batch 768\n",
      "Starting train_step for batch 769\n",
      "Starting train_step for batch 770\n",
      "Starting train_step for batch 771\n",
      "Starting train_step for batch 772\n",
      "Starting train_step for batch 773\n",
      "Starting train_step for batch 774\n",
      "Starting train_step for batch 775\n",
      "Starting train_step for batch 776\n",
      "Starting train_step for batch 777\n",
      "Starting train_step for batch 778\n",
      "Starting train_step for batch 779\n",
      "Starting train_step for batch 780\n",
      "Starting train_step for batch 781\n",
      "Starting train_step for batch 782\n",
      "Starting train_step for batch 783\n",
      "Starting train_step for batch 784\n",
      "Starting train_step for batch 785\n",
      "Starting train_step for batch 786\n",
      "Starting train_step for batch 787\n",
      "Starting train_step for batch 788\n",
      "Starting train_step for batch 789\n",
      "Starting train_step for batch 790\n",
      "Starting train_step for batch 791\n",
      "Starting train_step for batch 792\n",
      "Starting train_step for batch 793\n",
      "Starting train_step for batch 794\n",
      "Starting train_step for batch 795\n",
      "Starting train_step for batch 796\n",
      "Starting train_step for batch 797\n",
      "Starting train_step for batch 798\n",
      "Starting train_step for batch 799\n",
      "Starting train_step for batch 800\n",
      "Epoch 2, Step 800/1050\n",
      "Starting train_step for batch 801\n",
      "Starting train_step for batch 802\n",
      "Starting train_step for batch 803\n",
      "Starting train_step for batch 804\n",
      "Starting train_step for batch 805\n",
      "Starting train_step for batch 806\n",
      "Starting train_step for batch 807\n",
      "Starting train_step for batch 808\n",
      "Starting train_step for batch 809\n",
      "Starting train_step for batch 810\n",
      "Starting train_step for batch 811\n",
      "Starting train_step for batch 812\n",
      "Starting train_step for batch 813\n",
      "Starting train_step for batch 814\n",
      "Starting train_step for batch 815\n",
      "Starting train_step for batch 816\n",
      "Starting train_step for batch 817\n",
      "Starting train_step for batch 818\n",
      "Starting train_step for batch 819\n",
      "Starting train_step for batch 820\n",
      "Starting train_step for batch 821\n",
      "Starting train_step for batch 822\n",
      "Starting train_step for batch 823\n",
      "Starting train_step for batch 824\n",
      "Starting train_step for batch 825\n",
      "Starting train_step for batch 826\n",
      "Starting train_step for batch 827\n",
      "Starting train_step for batch 828\n",
      "Starting train_step for batch 829\n",
      "Starting train_step for batch 830\n",
      "Starting train_step for batch 831\n",
      "Starting train_step for batch 832\n",
      "Starting train_step for batch 833\n",
      "Starting train_step for batch 834\n",
      "Starting train_step for batch 835\n",
      "Starting train_step for batch 836\n",
      "Starting train_step for batch 837\n",
      "Starting train_step for batch 838\n",
      "Starting train_step for batch 839\n",
      "Starting train_step for batch 840\n",
      "Starting train_step for batch 841\n",
      "Starting train_step for batch 842\n",
      "Starting train_step for batch 843\n",
      "Starting train_step for batch 844\n",
      "Starting train_step for batch 845\n",
      "Starting train_step for batch 846\n",
      "Starting train_step for batch 847\n",
      "Starting train_step for batch 848\n",
      "Starting train_step for batch 849\n",
      "Starting train_step for batch 850\n",
      "Epoch 2, Step 850/1050\n",
      "Starting train_step for batch 851\n",
      "Starting train_step for batch 852\n",
      "Starting train_step for batch 853\n",
      "Starting train_step for batch 854\n",
      "Starting train_step for batch 855\n",
      "Starting train_step for batch 856\n",
      "Starting train_step for batch 857\n",
      "Starting train_step for batch 858\n",
      "Starting train_step for batch 859\n",
      "Starting train_step for batch 860\n",
      "Starting train_step for batch 861\n",
      "Starting train_step for batch 862\n",
      "Starting train_step for batch 863\n",
      "Starting train_step for batch 864\n",
      "Starting train_step for batch 865\n",
      "Starting train_step for batch 866\n",
      "Starting train_step for batch 867\n",
      "Starting train_step for batch 868\n",
      "Starting train_step for batch 869\n",
      "Starting train_step for batch 870\n",
      "Starting train_step for batch 871\n",
      "Starting train_step for batch 872\n",
      "Starting train_step for batch 873\n",
      "Starting train_step for batch 874\n",
      "Starting train_step for batch 875\n",
      "Starting train_step for batch 876\n",
      "Starting train_step for batch 877\n",
      "Starting train_step for batch 878\n",
      "Starting train_step for batch 879\n",
      "Starting train_step for batch 880\n",
      "Starting train_step for batch 881\n",
      "Starting train_step for batch 882\n",
      "Starting train_step for batch 883\n",
      "Starting train_step for batch 884\n",
      "Starting train_step for batch 885\n",
      "Starting train_step for batch 886\n",
      "Starting train_step for batch 887\n",
      "Starting train_step for batch 888\n",
      "Starting train_step for batch 889\n",
      "Starting train_step for batch 890\n",
      "Starting train_step for batch 891\n",
      "Starting train_step for batch 892\n",
      "Starting train_step for batch 893\n",
      "Starting train_step for batch 894\n",
      "Starting train_step for batch 895\n",
      "Starting train_step for batch 896\n",
      "Starting train_step for batch 897\n",
      "Starting train_step for batch 898\n",
      "Starting train_step for batch 899\n",
      "Starting train_step for batch 900\n",
      "Epoch 2, Step 900/1050\n",
      "Starting train_step for batch 901\n",
      "Starting train_step for batch 902\n",
      "Starting train_step for batch 903\n",
      "Starting train_step for batch 904\n",
      "Starting train_step for batch 905\n",
      "Starting train_step for batch 906\n",
      "Starting train_step for batch 907\n",
      "Starting train_step for batch 908\n",
      "Starting train_step for batch 909\n",
      "Starting train_step for batch 910\n",
      "Starting train_step for batch 911\n",
      "Starting train_step for batch 912\n",
      "Starting train_step for batch 913\n",
      "Starting train_step for batch 914\n",
      "Starting train_step for batch 915\n",
      "Starting train_step for batch 916\n",
      "Starting train_step for batch 917\n",
      "Starting train_step for batch 918\n",
      "Starting train_step for batch 919\n",
      "Starting train_step for batch 920\n",
      "Starting train_step for batch 921\n",
      "Starting train_step for batch 922\n",
      "Starting train_step for batch 923\n",
      "Starting train_step for batch 924\n",
      "Starting train_step for batch 925\n",
      "Starting train_step for batch 926\n",
      "Starting train_step for batch 927\n",
      "Starting train_step for batch 928\n",
      "Starting train_step for batch 929\n",
      "Starting train_step for batch 930\n",
      "Starting train_step for batch 931\n",
      "Starting train_step for batch 932\n",
      "Starting train_step for batch 933\n",
      "Starting train_step for batch 934\n",
      "Starting train_step for batch 935\n",
      "Starting train_step for batch 936\n",
      "Starting train_step for batch 937\n",
      "Starting train_step for batch 938\n",
      "Starting train_step for batch 939\n",
      "Starting train_step for batch 940\n",
      "Starting train_step for batch 941\n",
      "Starting train_step for batch 942\n",
      "Starting train_step for batch 943\n",
      "Starting train_step for batch 944\n",
      "Starting train_step for batch 945\n",
      "Starting train_step for batch 946\n",
      "Starting train_step for batch 947\n",
      "Starting train_step for batch 948\n",
      "Starting train_step for batch 949\n",
      "Starting train_step for batch 950\n",
      "Epoch 2, Step 950/1050\n",
      "Starting train_step for batch 951\n",
      "Starting train_step for batch 952\n",
      "Starting train_step for batch 953\n",
      "Starting train_step for batch 954\n",
      "Starting train_step for batch 955\n",
      "Starting train_step for batch 956\n",
      "Starting train_step for batch 957\n",
      "Starting train_step for batch 958\n",
      "Starting train_step for batch 959\n",
      "Starting train_step for batch 960\n",
      "Starting train_step for batch 961\n",
      "Starting train_step for batch 962\n",
      "Starting train_step for batch 963\n",
      "Starting train_step for batch 964\n",
      "Starting train_step for batch 965\n",
      "Starting train_step for batch 966\n",
      "Starting train_step for batch 967\n",
      "Starting train_step for batch 968\n",
      "Starting train_step for batch 969\n",
      "Starting train_step for batch 970\n",
      "Starting train_step for batch 971\n",
      "Starting train_step for batch 972\n",
      "Starting train_step for batch 973\n",
      "Starting train_step for batch 974\n",
      "Starting train_step for batch 975\n",
      "Starting train_step for batch 976\n",
      "Starting train_step for batch 977\n",
      "Starting train_step for batch 978\n",
      "Starting train_step for batch 979\n",
      "Starting train_step for batch 980\n",
      "Starting train_step for batch 981\n",
      "Starting train_step for batch 982\n",
      "Starting train_step for batch 983\n",
      "Starting train_step for batch 984\n",
      "Starting train_step for batch 985\n",
      "Starting train_step for batch 986\n",
      "Starting train_step for batch 987\n",
      "Starting train_step for batch 988\n",
      "Starting train_step for batch 989\n",
      "Starting train_step for batch 990\n",
      "Starting train_step for batch 991\n",
      "Starting train_step for batch 992\n",
      "Starting train_step for batch 993\n",
      "Starting train_step for batch 994\n",
      "Starting train_step for batch 995\n",
      "Starting train_step for batch 996\n",
      "Starting train_step for batch 997\n",
      "Starting train_step for batch 998\n",
      "Starting train_step for batch 999\n",
      "Starting train_step for batch 1000\n",
      "Epoch 2, Step 1000/1050\n",
      "Starting train_step for batch 1001\n",
      "Starting train_step for batch 1002\n",
      "Starting train_step for batch 1003\n",
      "Starting train_step for batch 1004\n",
      "Starting train_step for batch 1005\n",
      "Starting train_step for batch 1006\n",
      "Starting train_step for batch 1007\n",
      "Starting train_step for batch 1008\n",
      "Starting train_step for batch 1009\n",
      "Starting train_step for batch 1010\n",
      "Starting train_step for batch 1011\n",
      "Starting train_step for batch 1012\n",
      "Starting train_step for batch 1013\n",
      "Starting train_step for batch 1014\n",
      "Starting train_step for batch 1015\n",
      "Starting train_step for batch 1016\n",
      "Starting train_step for batch 1017\n",
      "Starting train_step for batch 1018\n",
      "Starting train_step for batch 1019\n",
      "Starting train_step for batch 1020\n",
      "Starting train_step for batch 1021\n",
      "Starting train_step for batch 1022\n",
      "Starting train_step for batch 1023\n",
      "Starting train_step for batch 1024\n",
      "Starting train_step for batch 1025\n",
      "Starting train_step for batch 1026\n",
      "Starting train_step for batch 1027\n",
      "Starting train_step for batch 1028\n",
      "Starting train_step for batch 1029\n",
      "Starting train_step for batch 1030\n",
      "Starting train_step for batch 1031\n",
      "Starting train_step for batch 1032\n",
      "Starting train_step for batch 1033\n",
      "Starting train_step for batch 1034\n",
      "Starting train_step for batch 1035\n",
      "Starting train_step for batch 1036\n",
      "Starting train_step for batch 1037\n",
      "Starting train_step for batch 1038\n",
      "Starting train_step for batch 1039\n",
      "Starting train_step for batch 1040\n",
      "Starting train_step for batch 1041\n",
      "Starting train_step for batch 1042\n",
      "Starting train_step for batch 1043\n",
      "Starting train_step for batch 1044\n",
      "Starting train_step for batch 1045\n",
      "Starting train_step for batch 1046\n",
      "Starting train_step for batch 1047\n",
      "Starting train_step for batch 1048\n",
      "Starting train_step for batch 1049\n",
      "Starting train_step for batch 1050\n",
      "Epoch 2, Step 1050/1050\n",
      "Epoch 2 - Average train loss: 0.0001\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.87GB (Max: 70.33GB), Reserved: 55.21GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 58.92 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 9.797112\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [90]\n",
      "  Predicted Slots: [90]\n",
      "Example 2:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [90 90 90]\n",
      "  Predicted Slots: [90 90 90]\n",
      "Example 3:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [90 90 90 90]\n",
      "  Predicted Slots: [90 90 90 90]\n",
      "Example 4:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [90]\n",
      "  Predicted Slots: [90]\n",
      "Example 5:\n",
      "  True Intent: 40, Predicted Intent: 40\n",
      "  True Slots: [90]\n",
      "  Predicted Slots: [90]\n",
      "Unique true intents: {40}\n",
      "Unique predicted intents: {40}\n",
      "Unique true slots: {90}\n",
      "Unique predicted slots: {90}\n",
      "Cleared GPU memory. Current allocated: 58.93 GB\n",
      "Generating batches: 100%|██████████| 4/4 [08:18<00:00, 124.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating translation results with Africomet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET model moved to GPU.\n",
      "Number of texts to evaluate: 225\n",
      "Starting AfriCOMET prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:03<00:00,  8.03it/s]\n",
      "Epoch 2/2 Evaluation results: {'translation_score': 0.10894583156399215, 'intent_accuracy': 1.0, 'slot_f1': 1.0, 'eval_loss': 9.797112464904785}\n",
      "Loaded best model state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfriCOMET prediction completed\n",
      "Type of model_output: <class 'comet.models.utils.Prediction'>\n",
      "Number of valid scores: 209\n",
      "Score statistics: Min: 0.0007, Max: 0.3218, Median: 0.1043, Average: 0.1089\n",
      "CUDA memory cleared and AfriCOMET model moved to CPU.\n",
      "Complete evaluating translation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory. Current allocated: 29.20 GB\n",
      "[I 2024-09-23 07:44:20,252] Trial 11 finished with value: -0.8910541684360078 and parameters: {'encoder_lr': 2.994471116556308e-06, 'decoder_lr': 3.3477388089161037e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 1, 'weight_decay': 0.09304895986444242, 'warmup_steps': 487, 'gradient_accumulation_steps': 128, 'intent_loss_weight': 0.9761991671313265, 'slot_loss_weight': 0.43081143618503664}. Best is trial 11 with value: -0.8910541684360078.\n",
      "Trial 5 completed in 4954.80 seconds\n",
      "After trial 5 - GPU Memory (GB): Total: 85.06, Allocated: 29.20, Reserved: 32.14, Available: 23.72\n",
      "Trial 5 results:\n",
      "  Translation score: -0.8911\n",
      "  Intent accuracy: 1.0000\n",
      "  Slot F1 score: 1.0000\n",
      "Best trial results:\n",
      "  Translation score: -0.8911\n",
      "  Intent accuracy: 1.0000\n",
      "  Slot F1 score: 1.0000\n",
      "Average delay between trials: 0.11 seconds\n",
      "Cleared GPU memory. Current allocated: 29.20 GB\n"
     ]
    }
   ],
   "source": [
    "# Run optimization for combined Afro-XLMR and LLaMA\n",
    "combined_study = run_combined_optimization(\n",
    "    models['afro-xlmr-large'],  # Encoder\n",
    "    models['meta-llama/Llama-2-7b-hf'],  # Decoder\n",
    "    tokenizers['afro-xlmr-large'], # encoder_tokenizer\n",
    "    tokenizers['meta-llama/Llama-2-7b-hf'], # decoder_tokenizer\n",
    "    datasets['combined_afro_xlmr_llama'],  \n",
    "    config,\n",
    "    evaluators['combined_afro_xlmr_llama'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('combined_study', <optuna.study.study.Study object at 0x7f8f262249d0>)])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Studies: {studies.items()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for combined_study: {'encoder_lr': 2.994471116556308e-06, 'decoder_lr': 3.3477388089161037e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 1, 'weight_decay': 0.09304895986444242, 'warmup_steps': 487, 'gradient_accumulation_steps': 128, 'intent_loss_weight': 0.9761991671313265, 'slot_loss_weight': 0.43081143618503664}\n"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter optimization\n",
    "try:\n",
    "    # Consolidate the results into the studies variable\n",
    "    studies = {\n",
    "        'combined_study': combined_study,\n",
    "    }\n",
    "    # Extract best parameters\n",
    "    best_params = {model_name: study.best_params for model_name, study in studies.items()}\n",
    "    print(best_params)\n",
    "\n",
    "    # Log best parameters\n",
    "    for model_name, params in best_params.items():\n",
    "        logger.info(f\"Best hyperparameters for {model_name}: {params}\")\n",
    "        config['training'][model_name] = params\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during or after hyperparameter optimization: {str(e)}\")\n",
    "    logger.exception(\"Exception details:\")\n",
    "    studies = {}  # Initialize an empty dictionary if optimization failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing hyperparameter analysis...\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter analysis\n",
    "logger.info(\"Performing hyperparameter analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Sensitivity Analysis for combined_study:\n",
      "gradient_accumulation_steps: 0.6329\n",
      "weight_decay: 0.5228\n",
      "intent_loss_weight: 0.5163\n",
      "warmup_steps: 0.3905\n",
      "encoder_lr: 0.2217\n",
      "decoder_lr: 0.1756\n",
      "num_train_epochs: 0.1456\n",
      "per_device_train_batch_size: 0.0577\n",
      "slot_loss_weight: 0.0524\n"
     ]
    }
   ],
   "source": [
    "for model_name, study in studies.items():\n",
    "    # Plot hyperparameter importance\n",
    "    importance_fig = plot_hyperparameter_importance(study)\n",
    "    importance_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
    "\n",
    "    # Plot optimization history\n",
    "    history_fig = optuna.visualization.plot_optimization_history(study)\n",
    "    history_fig.update_layout(title=\"Optimization History\")\n",
    "    history_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
    "\n",
    "    # Plot parallel coordinate\n",
    "    dimensions = []\n",
    "    for param in study.best_trials[0].params:\n",
    "        values = [trial.params[param] for trial in study.trials if param in trial.params]\n",
    "        if isinstance(values[0], (int, float)):\n",
    "            dimensions.append(\n",
    "                dict(range = [min(values), max(values)],\n",
    "                     label = param,\n",
    "                     values = values)\n",
    "            )\n",
    "        elif isinstance(values[0], str):\n",
    "            # For categorical parameters, we need a different approach\n",
    "            unique_values = list(set(values))\n",
    "            dimensions.append(\n",
    "                dict(range = [0, len(unique_values) - 1],\n",
    "                     tickvals = list(range(len(unique_values))),\n",
    "                     ticktext = unique_values,\n",
    "                     label = param,\n",
    "                     values = [unique_values.index(v) for v in values])\n",
    "            )\n",
    "\n",
    "    parallel_fig = go.Figure(data=\n",
    "        go.Parcoords(\n",
    "            line = dict(color = [trial.value for trial in study.trials],\n",
    "                        colorscale = 'Viridis',\n",
    "                        showscale = True),\n",
    "            dimensions = dimensions\n",
    "        )\n",
    "    )\n",
    "    parallel_fig.update_layout(title=\"Parallel Coordinate Plot of Hyperparameters\")\n",
    "    parallel_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
    "\n",
    "    # Analyze and plot sensitivity\n",
    "    sensitivity = analyze_hyperparameter_sensitivity(study)\n",
    "    sensitivity_fig = plot_sensitivity_analysis(sensitivity)\n",
    "    sensitivity_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")\n",
    "\n",
    "    # Print sensitivity analysis results\n",
    "    print(f\"\\nHyperparameter Sensitivity Analysis for {model_name}:\")\n",
    "    for param, sens in sensitivity:\n",
    "        print(f\"{param}: {sens:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter analysis complete. Plots saved in output directory.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Hyperparameter analysis complete. Plots saved in output directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Accelerator\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(mixed_precision='no', kwargs_handlers=[ddp_kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized CombinedEncoderDecoderTrainer with config: {'model': {'names': ['afro-xlmr-large', 'meta-llama/Llama-2-7b-hf'], 'output_dir': './model_outputs'}, 'data': {'masakhane_dir': '../Datasets/Masakhane', 'ontonotes_dir': '../Datasets/OntoNotes_5.0', 'flores_dir': '../Datasets/FLORES-200', 'experiments_dir': '../Datasets/Experiments'}, 'training': {'batch_size': 8, 'learning_rate': '2e-5', 'num_epochs': 3, 'warmup_steps': 500, 'combined_study': {'encoder_lr': 2.994471116556308e-06, 'decoder_lr': 3.3477388089161037e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 1, 'weight_decay': 0.09304895986444242, 'warmup_steps': 487, 'gradient_accumulation_steps': 128, 'intent_loss_weight': 0.9761991671313265, 'slot_loss_weight': 0.43081143618503664}}, 'evaluation': {'metrics': ['accuracy', 'precision', 'recall', 'f1']}, 'dataset_split': {'train_ratio': 0.7, 'val_ratio': 0.15, 'test_ratio': 0.15}, 'hyperparameters': {'learning_rate_min': '1e-6', 'learning_rate_max': '1e-4', 'num_train_epochs_min': 1, 'num_train_epochs_max': 3, 'batch_sizes': [1, 2, 4, 8, 16, 32], 'weight_decay_min': 0.01, 'weight_decay_max': 0.1, 'warmup_steps_min': 50, 'warmup_steps_max': 500, 'gradient_accumulation_steps': [1, 2, 4, 8, 16, 32, 64, 128], 'n_trials': 5}, 'experiments': {'zero_shot': {'enabled': True}, 'code_switch': {'enabled': True}}, 'benchmarking': {'use_flores_200': True}, 'seed': 42, 'logging': {'log_file': './logs/evaluation.log', 'log_level': 'INFO'}, 'device': 'cuda', 'auth_token': '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', 'cache': {'dir': './model_cache'}, 'encoder_lr': 2.994471116556308e-06, 'decoder_lr': 3.3477388089161037e-06, 'num_train_epochs': 2, 'per_device_train_batch_size': 1, 'weight_decay': 0.09304895986444242, 'warmup_steps': 487, 'gradient_accumulation_steps': 128, 'intent_loss_weight': 0.9761991671313265, 'slot_loss_weight': 0.43081143618503664, 'per_device_eval_batch_size': 1, 'max_grad_norm': 1, 'num_intent_classes': 50, 'num_slot_classes': 100}\n",
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainers with the best hyperparameters from the studies\n",
    "trainers = {}\n",
    "\n",
    "if 'combined_study' in studies and studies['combined_study'] is not None:\n",
    "    # Start with the base config\n",
    "    combined_config = config.copy()\n",
    "    # Add the best parameters from the study\n",
    "    combined_config.update(studies['combined_study'].best_params)\n",
    "    combined_config['per_device_eval_batch_size'] = 1\n",
    "    combined_config['max_grad_norm'] = 1\n",
    "\n",
    "    # Add required parameters if they're not already present\n",
    "    if 'num_intent_classes' not in combined_config:\n",
    "        combined_config['num_intent_classes'] = 50  # Replace with the actual number of intent classes\n",
    "    \n",
    "    if 'num_slot_classes' not in combined_config:\n",
    "        combined_config['num_slot_classes'] = 100  # Replace with the actual number of slot classes\n",
    "\n",
    "    trainers['combined_afro_xlmr_llama'] = CombinedEncoderDecoderTrainer(\n",
    "        encoder=models['afro-xlmr-large'],  # Afro-XLMR as encoder\n",
    "        decoder=models['meta-llama/Llama-2-7b-hf'],  # LLaMA as decoder\n",
    "        encoder_tokenizer=tokenizers['afro-xlmr-large'],\n",
    "        decoder_tokenizer=tokenizers['meta-llama/Llama-2-7b-hf'],\n",
    "        config=combined_config,\n",
    "        accelerator=accelerator,\n",
    "        batch_size=64  # or whatever batch size you prefer\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"Study results for 'combined_study' not found or optimization failed. \"\n",
    "                   \"CombinedEncoderDecoderTrainer will not be initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluations\n",
    "results = {\n",
    "    'classification': {},\n",
    "    'translation': {},\n",
    "    'generation': {},\n",
    "    'zero_shot': {},\n",
    "    'code_switch': {},\n",
    "    'intent_recognition': {}, \n",
    "    'slot_filling': {}, \n",
    "    'hyperparameter_studies': studies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for model: combined_afro_xlmr_llama\n",
      "Train dataset size: 1050, Eval dataset size: 225\n",
      "Encoder parameters: 560,711,457\n",
      "Decoder parameters: 6,738,415,616\n",
      "Total parameters: 7,299,127,073\n",
      "Encoder dtype: torch.bfloat16\n",
      "Decoder dtype: torch.bfloat16\n",
      "Training dtype: torch.bfloat16\n",
      "GPU memory allocated: 59.00 GB\n",
      "GPU memory cached: 59.18 GB\n",
      "Validating Training dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Training dataset\n",
      "Validating Evaluation dataset:\n",
      "Batch 0 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 1 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 2 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 3 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Batch 4 shapes:\n",
      "  input_ids: torch.Size([1, 128])\n",
      "  attention_mask: torch.Size([1, 128])\n",
      "  labels: torch.Size([1, 128])\n",
      "Finished validating Evaluation dataset\n",
      "Starting train_step for batch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process\n",
      "CUDA Memory (Start of training) - Allocated: 54.95GB (Max: 70.33GB), Reserved: 55.12GB (Max: 71.27GB)\n",
      "Total training steps: 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 1, Step 50/1050\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 1, Step 100/1050\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 1, Step 150/1050\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 1, Step 200/1050\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 1, Step 250/1050\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 1, Step 300/1050\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 1, Step 350/1050\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 1, Step 400/1050\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 1, Step 450/1050\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 1, Step 500/1050\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Starting train_step for batch 526\n",
      "Starting train_step for batch 527\n",
      "Starting train_step for batch 528\n",
      "Starting train_step for batch 529\n",
      "Starting train_step for batch 530\n",
      "Starting train_step for batch 531\n",
      "Starting train_step for batch 532\n",
      "Starting train_step for batch 533\n",
      "Starting train_step for batch 534\n",
      "Starting train_step for batch 535\n",
      "Starting train_step for batch 536\n",
      "Starting train_step for batch 537\n",
      "Starting train_step for batch 538\n",
      "Starting train_step for batch 539\n",
      "Starting train_step for batch 540\n",
      "Starting train_step for batch 541\n",
      "Starting train_step for batch 542\n",
      "Starting train_step for batch 543\n",
      "Starting train_step for batch 544\n",
      "Starting train_step for batch 545\n",
      "Starting train_step for batch 546\n",
      "Starting train_step for batch 547\n",
      "Starting train_step for batch 548\n",
      "Starting train_step for batch 549\n",
      "Starting train_step for batch 550\n",
      "Epoch 1, Step 550/1050\n",
      "Starting train_step for batch 551\n",
      "Starting train_step for batch 552\n",
      "Starting train_step for batch 553\n",
      "Starting train_step for batch 554\n",
      "Starting train_step for batch 555\n",
      "Starting train_step for batch 556\n",
      "Starting train_step for batch 557\n",
      "Starting train_step for batch 558\n",
      "Starting train_step for batch 559\n",
      "Starting train_step for batch 560\n",
      "Starting train_step for batch 561\n",
      "Starting train_step for batch 562\n",
      "Starting train_step for batch 563\n",
      "Starting train_step for batch 564\n",
      "Starting train_step for batch 565\n",
      "Starting train_step for batch 566\n",
      "Starting train_step for batch 567\n",
      "Starting train_step for batch 568\n",
      "Starting train_step for batch 569\n",
      "Starting train_step for batch 570\n",
      "Starting train_step for batch 571\n",
      "Starting train_step for batch 572\n",
      "Starting train_step for batch 573\n",
      "Starting train_step for batch 574\n",
      "Starting train_step for batch 575\n",
      "Starting train_step for batch 576\n",
      "Starting train_step for batch 577\n",
      "Starting train_step for batch 578\n",
      "Starting train_step for batch 579\n",
      "Starting train_step for batch 580\n",
      "Starting train_step for batch 581\n",
      "Starting train_step for batch 582\n",
      "Starting train_step for batch 583\n",
      "Starting train_step for batch 584\n",
      "Starting train_step for batch 585\n",
      "Starting train_step for batch 586\n",
      "Starting train_step for batch 587\n",
      "Starting train_step for batch 588\n",
      "Starting train_step for batch 589\n",
      "Starting train_step for batch 590\n",
      "Starting train_step for batch 591\n",
      "Starting train_step for batch 592\n",
      "Starting train_step for batch 593\n",
      "Starting train_step for batch 594\n",
      "Starting train_step for batch 595\n",
      "Starting train_step for batch 596\n",
      "Starting train_step for batch 597\n",
      "Starting train_step for batch 598\n",
      "Starting train_step for batch 599\n",
      "Starting train_step for batch 600\n",
      "Epoch 1, Step 600/1050\n",
      "Starting train_step for batch 601\n",
      "Starting train_step for batch 602\n",
      "Starting train_step for batch 603\n",
      "Starting train_step for batch 604\n",
      "Starting train_step for batch 605\n",
      "Starting train_step for batch 606\n",
      "Starting train_step for batch 607\n",
      "Starting train_step for batch 608\n",
      "Starting train_step for batch 609\n",
      "Starting train_step for batch 610\n",
      "Starting train_step for batch 611\n",
      "Starting train_step for batch 612\n",
      "Starting train_step for batch 613\n",
      "Starting train_step for batch 614\n",
      "Starting train_step for batch 615\n",
      "Starting train_step for batch 616\n",
      "Starting train_step for batch 617\n",
      "Starting train_step for batch 618\n",
      "Starting train_step for batch 619\n",
      "Starting train_step for batch 620\n",
      "Starting train_step for batch 621\n",
      "Starting train_step for batch 622\n",
      "Starting train_step for batch 623\n",
      "Starting train_step for batch 624\n",
      "Starting train_step for batch 625\n",
      "Starting train_step for batch 626\n",
      "Starting train_step for batch 627\n",
      "Starting train_step for batch 628\n",
      "Starting train_step for batch 629\n",
      "Starting train_step for batch 630\n",
      "Starting train_step for batch 631\n",
      "Starting train_step for batch 632\n",
      "Starting train_step for batch 633\n",
      "Starting train_step for batch 634\n",
      "Starting train_step for batch 635\n",
      "Starting train_step for batch 636\n",
      "Starting train_step for batch 637\n",
      "Starting train_step for batch 638\n",
      "Starting train_step for batch 639\n",
      "Starting train_step for batch 640\n",
      "Starting train_step for batch 641\n",
      "Starting train_step for batch 642\n",
      "Starting train_step for batch 643\n",
      "Starting train_step for batch 644\n",
      "Starting train_step for batch 645\n",
      "Starting train_step for batch 646\n",
      "Starting train_step for batch 647\n",
      "Starting train_step for batch 648\n",
      "Starting train_step for batch 649\n",
      "Starting train_step for batch 650\n",
      "Epoch 1, Step 650/1050\n",
      "Starting train_step for batch 651\n",
      "Starting train_step for batch 652\n",
      "Starting train_step for batch 653\n",
      "Starting train_step for batch 654\n",
      "Starting train_step for batch 655\n",
      "Starting train_step for batch 656\n",
      "Starting train_step for batch 657\n",
      "Starting train_step for batch 658\n",
      "Starting train_step for batch 659\n",
      "Starting train_step for batch 660\n",
      "Starting train_step for batch 661\n",
      "Starting train_step for batch 662\n",
      "Starting train_step for batch 663\n",
      "Starting train_step for batch 664\n",
      "Starting train_step for batch 665\n",
      "Starting train_step for batch 666\n",
      "Starting train_step for batch 667\n",
      "Starting train_step for batch 668\n",
      "Starting train_step for batch 669\n",
      "Starting train_step for batch 670\n",
      "Starting train_step for batch 671\n",
      "Starting train_step for batch 672\n",
      "Starting train_step for batch 673\n",
      "Starting train_step for batch 674\n",
      "Starting train_step for batch 675\n",
      "Starting train_step for batch 676\n",
      "Starting train_step for batch 677\n",
      "Starting train_step for batch 678\n",
      "Starting train_step for batch 679\n",
      "Starting train_step for batch 680\n",
      "Starting train_step for batch 681\n",
      "Starting train_step for batch 682\n",
      "Starting train_step for batch 683\n",
      "Starting train_step for batch 684\n",
      "Starting train_step for batch 685\n",
      "Starting train_step for batch 686\n",
      "Starting train_step for batch 687\n",
      "Starting train_step for batch 688\n",
      "Starting train_step for batch 689\n",
      "Starting train_step for batch 690\n",
      "Starting train_step for batch 691\n",
      "Starting train_step for batch 692\n",
      "Starting train_step for batch 693\n",
      "Starting train_step for batch 694\n",
      "Starting train_step for batch 695\n",
      "Starting train_step for batch 696\n",
      "Starting train_step for batch 697\n",
      "Starting train_step for batch 698\n",
      "Starting train_step for batch 699\n",
      "Starting train_step for batch 700\n",
      "Epoch 1, Step 700/1050\n",
      "Starting train_step for batch 701\n",
      "Starting train_step for batch 702\n",
      "Starting train_step for batch 703\n",
      "Starting train_step for batch 704\n",
      "Starting train_step for batch 705\n",
      "Starting train_step for batch 706\n",
      "Starting train_step for batch 707\n",
      "Starting train_step for batch 708\n",
      "Starting train_step for batch 709\n",
      "Starting train_step for batch 710\n",
      "Starting train_step for batch 711\n",
      "Starting train_step for batch 712\n",
      "Starting train_step for batch 713\n",
      "Starting train_step for batch 714\n",
      "Starting train_step for batch 715\n",
      "Starting train_step for batch 716\n",
      "Starting train_step for batch 717\n",
      "Starting train_step for batch 718\n",
      "Starting train_step for batch 719\n",
      "Starting train_step for batch 720\n",
      "Starting train_step for batch 721\n",
      "Starting train_step for batch 722\n",
      "Starting train_step for batch 723\n",
      "Starting train_step for batch 724\n",
      "Starting train_step for batch 725\n",
      "Starting train_step for batch 726\n",
      "Starting train_step for batch 727\n",
      "Starting train_step for batch 728\n",
      "Starting train_step for batch 729\n",
      "Starting train_step for batch 730\n",
      "Starting train_step for batch 731\n",
      "Starting train_step for batch 732\n",
      "Starting train_step for batch 733\n",
      "Starting train_step for batch 734\n",
      "Starting train_step for batch 735\n",
      "Starting train_step for batch 736\n",
      "Starting train_step for batch 737\n",
      "Starting train_step for batch 738\n",
      "Starting train_step for batch 739\n",
      "Starting train_step for batch 740\n",
      "Starting train_step for batch 741\n",
      "Starting train_step for batch 742\n",
      "Starting train_step for batch 743\n",
      "Starting train_step for batch 744\n",
      "Starting train_step for batch 745\n",
      "Starting train_step for batch 746\n",
      "Starting train_step for batch 747\n",
      "Starting train_step for batch 748\n",
      "Starting train_step for batch 749\n",
      "Starting train_step for batch 750\n",
      "Epoch 1, Step 750/1050\n",
      "Starting train_step for batch 751\n",
      "Starting train_step for batch 752\n",
      "Starting train_step for batch 753\n",
      "Starting train_step for batch 754\n",
      "Starting train_step for batch 755\n",
      "Starting train_step for batch 756\n",
      "Starting train_step for batch 757\n",
      "Starting train_step for batch 758\n",
      "Starting train_step for batch 759\n",
      "Starting train_step for batch 760\n",
      "Starting train_step for batch 761\n",
      "Starting train_step for batch 762\n",
      "Starting train_step for batch 763\n",
      "Starting train_step for batch 764\n",
      "Starting train_step for batch 765\n",
      "Starting train_step for batch 766\n",
      "Starting train_step for batch 767\n",
      "Starting train_step for batch 768\n",
      "Starting train_step for batch 769\n",
      "Starting train_step for batch 770\n",
      "Starting train_step for batch 771\n",
      "Starting train_step for batch 772\n",
      "Starting train_step for batch 773\n",
      "Starting train_step for batch 774\n",
      "Starting train_step for batch 775\n",
      "Starting train_step for batch 776\n",
      "Starting train_step for batch 777\n",
      "Starting train_step for batch 778\n",
      "Starting train_step for batch 779\n",
      "Starting train_step for batch 780\n",
      "Starting train_step for batch 781\n",
      "Starting train_step for batch 782\n",
      "Starting train_step for batch 783\n",
      "Starting train_step for batch 784\n",
      "Starting train_step for batch 785\n",
      "Starting train_step for batch 786\n",
      "Starting train_step for batch 787\n",
      "Starting train_step for batch 788\n",
      "Starting train_step for batch 789\n",
      "Starting train_step for batch 790\n",
      "Starting train_step for batch 791\n",
      "Starting train_step for batch 792\n",
      "Starting train_step for batch 793\n",
      "Starting train_step for batch 794\n",
      "Starting train_step for batch 795\n",
      "Starting train_step for batch 796\n",
      "Starting train_step for batch 797\n",
      "Starting train_step for batch 798\n",
      "Starting train_step for batch 799\n",
      "Starting train_step for batch 800\n",
      "Epoch 1, Step 800/1050\n",
      "Starting train_step for batch 801\n",
      "Starting train_step for batch 802\n",
      "Starting train_step for batch 803\n",
      "Starting train_step for batch 804\n",
      "Starting train_step for batch 805\n",
      "Starting train_step for batch 806\n",
      "Starting train_step for batch 807\n",
      "Starting train_step for batch 808\n",
      "Starting train_step for batch 809\n",
      "Starting train_step for batch 810\n",
      "Starting train_step for batch 811\n",
      "Starting train_step for batch 812\n",
      "Starting train_step for batch 813\n",
      "Starting train_step for batch 814\n",
      "Starting train_step for batch 815\n",
      "Starting train_step for batch 816\n",
      "Starting train_step for batch 817\n",
      "Starting train_step for batch 818\n",
      "Starting train_step for batch 819\n",
      "Starting train_step for batch 820\n",
      "Starting train_step for batch 821\n",
      "Starting train_step for batch 822\n",
      "Starting train_step for batch 823\n",
      "Starting train_step for batch 824\n",
      "Starting train_step for batch 825\n",
      "Starting train_step for batch 826\n",
      "Starting train_step for batch 827\n",
      "Starting train_step for batch 828\n",
      "Starting train_step for batch 829\n",
      "Starting train_step for batch 830\n",
      "Starting train_step for batch 831\n",
      "Starting train_step for batch 832\n",
      "Starting train_step for batch 833\n",
      "Starting train_step for batch 834\n",
      "Starting train_step for batch 835\n",
      "Starting train_step for batch 836\n",
      "Starting train_step for batch 837\n",
      "Starting train_step for batch 838\n",
      "Starting train_step for batch 839\n",
      "Starting train_step for batch 840\n",
      "Starting train_step for batch 841\n",
      "Starting train_step for batch 842\n",
      "Starting train_step for batch 843\n",
      "Starting train_step for batch 844\n",
      "Starting train_step for batch 845\n",
      "Starting train_step for batch 846\n",
      "Starting train_step for batch 847\n",
      "Starting train_step for batch 848\n",
      "Starting train_step for batch 849\n",
      "Starting train_step for batch 850\n",
      "Epoch 1, Step 850/1050\n",
      "Starting train_step for batch 851\n",
      "Starting train_step for batch 852\n",
      "Starting train_step for batch 853\n",
      "Starting train_step for batch 854\n",
      "Starting train_step for batch 855\n",
      "Starting train_step for batch 856\n",
      "Starting train_step for batch 857\n",
      "Starting train_step for batch 858\n",
      "Starting train_step for batch 859\n",
      "Starting train_step for batch 860\n",
      "Starting train_step for batch 861\n",
      "Starting train_step for batch 862\n",
      "Starting train_step for batch 863\n",
      "Starting train_step for batch 864\n",
      "Starting train_step for batch 865\n",
      "Starting train_step for batch 866\n",
      "Starting train_step for batch 867\n",
      "Starting train_step for batch 868\n",
      "Starting train_step for batch 869\n",
      "Starting train_step for batch 870\n",
      "Starting train_step for batch 871\n",
      "Starting train_step for batch 872\n",
      "Starting train_step for batch 873\n",
      "Starting train_step for batch 874\n",
      "Starting train_step for batch 875\n",
      "Starting train_step for batch 876\n",
      "Starting train_step for batch 877\n",
      "Starting train_step for batch 878\n",
      "Starting train_step for batch 879\n",
      "Starting train_step for batch 880\n",
      "Starting train_step for batch 881\n",
      "Starting train_step for batch 882\n",
      "Starting train_step for batch 883\n",
      "Starting train_step for batch 884\n",
      "Starting train_step for batch 885\n",
      "Starting train_step for batch 886\n",
      "Starting train_step for batch 887\n",
      "Starting train_step for batch 888\n",
      "Starting train_step for batch 889\n",
      "Starting train_step for batch 890\n",
      "Starting train_step for batch 891\n",
      "Starting train_step for batch 892\n",
      "Starting train_step for batch 893\n",
      "Starting train_step for batch 894\n",
      "Starting train_step for batch 895\n",
      "Starting train_step for batch 896\n",
      "Starting train_step for batch 897\n",
      "Starting train_step for batch 898\n",
      "Starting train_step for batch 899\n",
      "Starting train_step for batch 900\n",
      "Epoch 1, Step 900/1050\n",
      "Starting train_step for batch 901\n",
      "Starting train_step for batch 902\n",
      "Starting train_step for batch 903\n",
      "Starting train_step for batch 904\n",
      "Starting train_step for batch 905\n",
      "Starting train_step for batch 906\n",
      "Starting train_step for batch 907\n",
      "Starting train_step for batch 908\n",
      "Starting train_step for batch 909\n",
      "Starting train_step for batch 910\n",
      "Starting train_step for batch 911\n",
      "Starting train_step for batch 912\n",
      "Starting train_step for batch 913\n",
      "Starting train_step for batch 914\n",
      "Starting train_step for batch 915\n",
      "Starting train_step for batch 916\n",
      "Starting train_step for batch 917\n",
      "Starting train_step for batch 918\n",
      "Starting train_step for batch 919\n",
      "Starting train_step for batch 920\n",
      "Starting train_step for batch 921\n",
      "Starting train_step for batch 922\n",
      "Starting train_step for batch 923\n",
      "Starting train_step for batch 924\n",
      "Starting train_step for batch 925\n",
      "Starting train_step for batch 926\n",
      "Starting train_step for batch 927\n",
      "Starting train_step for batch 928\n",
      "Starting train_step for batch 929\n",
      "Starting train_step for batch 930\n",
      "Starting train_step for batch 931\n",
      "Starting train_step for batch 932\n",
      "Starting train_step for batch 933\n",
      "Starting train_step for batch 934\n",
      "Starting train_step for batch 935\n",
      "Starting train_step for batch 936\n",
      "Starting train_step for batch 937\n",
      "Starting train_step for batch 938\n",
      "Starting train_step for batch 939\n",
      "Starting train_step for batch 940\n",
      "Starting train_step for batch 941\n",
      "Starting train_step for batch 942\n",
      "Starting train_step for batch 943\n",
      "Starting train_step for batch 944\n",
      "Starting train_step for batch 945\n",
      "Starting train_step for batch 946\n",
      "Starting train_step for batch 947\n",
      "Starting train_step for batch 948\n",
      "Starting train_step for batch 949\n",
      "Starting train_step for batch 950\n",
      "Epoch 1, Step 950/1050\n",
      "Starting train_step for batch 951\n",
      "Starting train_step for batch 952\n",
      "Starting train_step for batch 953\n",
      "Starting train_step for batch 954\n",
      "Starting train_step for batch 955\n",
      "Starting train_step for batch 956\n",
      "Starting train_step for batch 957\n",
      "Starting train_step for batch 958\n",
      "Starting train_step for batch 959\n",
      "Starting train_step for batch 960\n",
      "Starting train_step for batch 961\n",
      "Starting train_step for batch 962\n",
      "Starting train_step for batch 963\n",
      "Starting train_step for batch 964\n",
      "Starting train_step for batch 965\n",
      "Starting train_step for batch 966\n",
      "Starting train_step for batch 967\n",
      "Starting train_step for batch 968\n",
      "Starting train_step for batch 969\n",
      "Starting train_step for batch 970\n",
      "Starting train_step for batch 971\n",
      "Starting train_step for batch 972\n",
      "Starting train_step for batch 973\n",
      "Starting train_step for batch 974\n",
      "Starting train_step for batch 975\n",
      "Starting train_step for batch 976\n",
      "Starting train_step for batch 977\n",
      "Starting train_step for batch 978\n",
      "Starting train_step for batch 979\n",
      "Starting train_step for batch 980\n",
      "Starting train_step for batch 981\n",
      "Starting train_step for batch 982\n",
      "Starting train_step for batch 983\n",
      "Starting train_step for batch 984\n",
      "Starting train_step for batch 985\n",
      "Starting train_step for batch 986\n",
      "Starting train_step for batch 987\n",
      "Starting train_step for batch 988\n",
      "Starting train_step for batch 989\n",
      "Starting train_step for batch 990\n",
      "Starting train_step for batch 991\n",
      "Starting train_step for batch 992\n",
      "Starting train_step for batch 993\n",
      "Starting train_step for batch 994\n",
      "Starting train_step for batch 995\n",
      "Starting train_step for batch 996\n",
      "Starting train_step for batch 997\n",
      "Starting train_step for batch 998\n",
      "Starting train_step for batch 999\n",
      "Starting train_step for batch 1000\n",
      "Epoch 1, Step 1000/1050\n",
      "Starting train_step for batch 1001\n",
      "Starting train_step for batch 1002\n",
      "Starting train_step for batch 1003\n",
      "Starting train_step for batch 1004\n",
      "Starting train_step for batch 1005\n",
      "Starting train_step for batch 1006\n",
      "Starting train_step for batch 1007\n",
      "Starting train_step for batch 1008\n",
      "Starting train_step for batch 1009\n",
      "Starting train_step for batch 1010\n",
      "Starting train_step for batch 1011\n",
      "Starting train_step for batch 1012\n",
      "Starting train_step for batch 1013\n",
      "Starting train_step for batch 1014\n",
      "Starting train_step for batch 1015\n",
      "Starting train_step for batch 1016\n",
      "Starting train_step for batch 1017\n",
      "Starting train_step for batch 1018\n",
      "Starting train_step for batch 1019\n",
      "Starting train_step for batch 1020\n",
      "Starting train_step for batch 1021\n",
      "Starting train_step for batch 1022\n",
      "Starting train_step for batch 1023\n",
      "Starting train_step for batch 1024\n",
      "Starting train_step for batch 1025\n",
      "Starting train_step for batch 1026\n",
      "Starting train_step for batch 1027\n",
      "Starting train_step for batch 1028\n",
      "Starting train_step for batch 1029\n",
      "Starting train_step for batch 1030\n",
      "Starting train_step for batch 1031\n",
      "Starting train_step for batch 1032\n",
      "Starting train_step for batch 1033\n",
      "Starting train_step for batch 1034\n",
      "Starting train_step for batch 1035\n",
      "Starting train_step for batch 1036\n",
      "Starting train_step for batch 1037\n",
      "Starting train_step for batch 1038\n",
      "Starting train_step for batch 1039\n",
      "Starting train_step for batch 1040\n",
      "Starting train_step for batch 1041\n",
      "Starting train_step for batch 1042\n",
      "Starting train_step for batch 1043\n",
      "Starting train_step for batch 1044\n",
      "Starting train_step for batch 1045\n",
      "Starting train_step for batch 1046\n",
      "Starting train_step for batch 1047\n",
      "Starting train_step for batch 1048\n",
      "Starting train_step for batch 1049\n",
      "Starting train_step for batch 1050\n",
      "Epoch 1, Step 1050/1050\n",
      "Epoch 1 - Average train loss: 0.0002\n",
      "Starting train_step for batch 1\n",
      "Starting train_step for batch 2\n",
      "Starting train_step for batch 3\n",
      "Starting train_step for batch 4\n",
      "Starting train_step for batch 5\n",
      "Starting train_step for batch 6\n",
      "Starting train_step for batch 7\n",
      "Starting train_step for batch 8\n",
      "Starting train_step for batch 9\n",
      "Starting train_step for batch 10\n",
      "Starting train_step for batch 11\n",
      "Starting train_step for batch 12\n",
      "Starting train_step for batch 13\n",
      "Starting train_step for batch 14\n",
      "Starting train_step for batch 15\n",
      "Starting train_step for batch 16\n",
      "Starting train_step for batch 17\n",
      "Starting train_step for batch 18\n",
      "Starting train_step for batch 19\n",
      "Starting train_step for batch 20\n",
      "Starting train_step for batch 21\n",
      "Starting train_step for batch 22\n",
      "Starting train_step for batch 23\n",
      "Starting train_step for batch 24\n",
      "Starting train_step for batch 25\n",
      "Starting train_step for batch 26\n",
      "Starting train_step for batch 27\n",
      "Starting train_step for batch 28\n",
      "Starting train_step for batch 29\n",
      "Starting train_step for batch 30\n",
      "Starting train_step for batch 31\n",
      "Starting train_step for batch 32\n",
      "Starting train_step for batch 33\n",
      "Starting train_step for batch 34\n",
      "Starting train_step for batch 35\n",
      "Starting train_step for batch 36\n",
      "Starting train_step for batch 37\n",
      "Starting train_step for batch 38\n",
      "Starting train_step for batch 39\n",
      "Starting train_step for batch 40\n",
      "Starting train_step for batch 41\n",
      "Starting train_step for batch 42\n",
      "Starting train_step for batch 43\n",
      "Starting train_step for batch 44\n",
      "Starting train_step for batch 45\n",
      "Starting train_step for batch 46\n",
      "Starting train_step for batch 47\n",
      "Starting train_step for batch 48\n",
      "Starting train_step for batch 49\n",
      "Starting train_step for batch 50\n",
      "Epoch 2, Step 50/1050\n",
      "Starting train_step for batch 51\n",
      "Starting train_step for batch 52\n",
      "Starting train_step for batch 53\n",
      "Starting train_step for batch 54\n",
      "Starting train_step for batch 55\n",
      "Starting train_step for batch 56\n",
      "Starting train_step for batch 57\n",
      "Starting train_step for batch 58\n",
      "Starting train_step for batch 59\n",
      "Starting train_step for batch 60\n",
      "Starting train_step for batch 61\n",
      "Starting train_step for batch 62\n",
      "Starting train_step for batch 63\n",
      "Starting train_step for batch 64\n",
      "Starting train_step for batch 65\n",
      "Starting train_step for batch 66\n",
      "Starting train_step for batch 67\n",
      "Starting train_step for batch 68\n",
      "Starting train_step for batch 69\n",
      "Starting train_step for batch 70\n",
      "Starting train_step for batch 71\n",
      "Starting train_step for batch 72\n",
      "Starting train_step for batch 73\n",
      "Starting train_step for batch 74\n",
      "Starting train_step for batch 75\n",
      "Starting train_step for batch 76\n",
      "Starting train_step for batch 77\n",
      "Starting train_step for batch 78\n",
      "Starting train_step for batch 79\n",
      "Starting train_step for batch 80\n",
      "Starting train_step for batch 81\n",
      "Starting train_step for batch 82\n",
      "Starting train_step for batch 83\n",
      "Starting train_step for batch 84\n",
      "Starting train_step for batch 85\n",
      "Starting train_step for batch 86\n",
      "Starting train_step for batch 87\n",
      "Starting train_step for batch 88\n",
      "Starting train_step for batch 89\n",
      "Starting train_step for batch 90\n",
      "Starting train_step for batch 91\n",
      "Starting train_step for batch 92\n",
      "Starting train_step for batch 93\n",
      "Starting train_step for batch 94\n",
      "Starting train_step for batch 95\n",
      "Starting train_step for batch 96\n",
      "Starting train_step for batch 97\n",
      "Starting train_step for batch 98\n",
      "Starting train_step for batch 99\n",
      "Starting train_step for batch 100\n",
      "Epoch 2, Step 100/1050\n",
      "Starting train_step for batch 101\n",
      "Starting train_step for batch 102\n",
      "Starting train_step for batch 103\n",
      "Starting train_step for batch 104\n",
      "Starting train_step for batch 105\n",
      "Starting train_step for batch 106\n",
      "Starting train_step for batch 107\n",
      "Starting train_step for batch 108\n",
      "Starting train_step for batch 109\n",
      "Starting train_step for batch 110\n",
      "Starting train_step for batch 111\n",
      "Starting train_step for batch 112\n",
      "Starting train_step for batch 113\n",
      "Starting train_step for batch 114\n",
      "Starting train_step for batch 115\n",
      "Starting train_step for batch 116\n",
      "Starting train_step for batch 117\n",
      "Starting train_step for batch 118\n",
      "Starting train_step for batch 119\n",
      "Starting train_step for batch 120\n",
      "Starting train_step for batch 121\n",
      "Starting train_step for batch 122\n",
      "Starting train_step for batch 123\n",
      "Starting train_step for batch 124\n",
      "Starting train_step for batch 125\n",
      "Starting train_step for batch 126\n",
      "Starting train_step for batch 127\n",
      "Starting train_step for batch 128\n",
      "Starting train_step for batch 129\n",
      "Starting train_step for batch 130\n",
      "Starting train_step for batch 131\n",
      "Starting train_step for batch 132\n",
      "Starting train_step for batch 133\n",
      "Starting train_step for batch 134\n",
      "Starting train_step for batch 135\n",
      "Starting train_step for batch 136\n",
      "Starting train_step for batch 137\n",
      "Starting train_step for batch 138\n",
      "Starting train_step for batch 139\n",
      "Starting train_step for batch 140\n",
      "Starting train_step for batch 141\n",
      "Starting train_step for batch 142\n",
      "Starting train_step for batch 143\n",
      "Starting train_step for batch 144\n",
      "Starting train_step for batch 145\n",
      "Starting train_step for batch 146\n",
      "Starting train_step for batch 147\n",
      "Starting train_step for batch 148\n",
      "Starting train_step for batch 149\n",
      "Starting train_step for batch 150\n",
      "Epoch 2, Step 150/1050\n",
      "Starting train_step for batch 151\n",
      "Starting train_step for batch 152\n",
      "Starting train_step for batch 153\n",
      "Starting train_step for batch 154\n",
      "Starting train_step for batch 155\n",
      "Starting train_step for batch 156\n",
      "Starting train_step for batch 157\n",
      "Starting train_step for batch 158\n",
      "Starting train_step for batch 159\n",
      "Starting train_step for batch 160\n",
      "Starting train_step for batch 161\n",
      "Starting train_step for batch 162\n",
      "Starting train_step for batch 163\n",
      "Starting train_step for batch 164\n",
      "Starting train_step for batch 165\n",
      "Starting train_step for batch 166\n",
      "Starting train_step for batch 167\n",
      "Starting train_step for batch 168\n",
      "Starting train_step for batch 169\n",
      "Starting train_step for batch 170\n",
      "Starting train_step for batch 171\n",
      "Starting train_step for batch 172\n",
      "Starting train_step for batch 173\n",
      "Starting train_step for batch 174\n",
      "Starting train_step for batch 175\n",
      "Starting train_step for batch 176\n",
      "Starting train_step for batch 177\n",
      "Starting train_step for batch 178\n",
      "Starting train_step for batch 179\n",
      "Starting train_step for batch 180\n",
      "Starting train_step for batch 181\n",
      "Starting train_step for batch 182\n",
      "Starting train_step for batch 183\n",
      "Starting train_step for batch 184\n",
      "Starting train_step for batch 185\n",
      "Starting train_step for batch 186\n",
      "Starting train_step for batch 187\n",
      "Starting train_step for batch 188\n",
      "Starting train_step for batch 189\n",
      "Starting train_step for batch 190\n",
      "Starting train_step for batch 191\n",
      "Starting train_step for batch 192\n",
      "Starting train_step for batch 193\n",
      "Starting train_step for batch 194\n",
      "Starting train_step for batch 195\n",
      "Starting train_step for batch 196\n",
      "Starting train_step for batch 197\n",
      "Starting train_step for batch 198\n",
      "Starting train_step for batch 199\n",
      "Starting train_step for batch 200\n",
      "Epoch 2, Step 200/1050\n",
      "Starting train_step for batch 201\n",
      "Starting train_step for batch 202\n",
      "Starting train_step for batch 203\n",
      "Starting train_step for batch 204\n",
      "Starting train_step for batch 205\n",
      "Starting train_step for batch 206\n",
      "Starting train_step for batch 207\n",
      "Starting train_step for batch 208\n",
      "Starting train_step for batch 209\n",
      "Starting train_step for batch 210\n",
      "Starting train_step for batch 211\n",
      "Starting train_step for batch 212\n",
      "Starting train_step for batch 213\n",
      "Starting train_step for batch 214\n",
      "Starting train_step for batch 215\n",
      "Starting train_step for batch 216\n",
      "Starting train_step for batch 217\n",
      "Starting train_step for batch 218\n",
      "Starting train_step for batch 219\n",
      "Starting train_step for batch 220\n",
      "Starting train_step for batch 221\n",
      "Starting train_step for batch 222\n",
      "Starting train_step for batch 223\n",
      "Starting train_step for batch 224\n",
      "Starting train_step for batch 225\n",
      "Starting train_step for batch 226\n",
      "Starting train_step for batch 227\n",
      "Starting train_step for batch 228\n",
      "Starting train_step for batch 229\n",
      "Starting train_step for batch 230\n",
      "Starting train_step for batch 231\n",
      "Starting train_step for batch 232\n",
      "Starting train_step for batch 233\n",
      "Starting train_step for batch 234\n",
      "Starting train_step for batch 235\n",
      "Starting train_step for batch 236\n",
      "Starting train_step for batch 237\n",
      "Starting train_step for batch 238\n",
      "Starting train_step for batch 239\n",
      "Starting train_step for batch 240\n",
      "Starting train_step for batch 241\n",
      "Starting train_step for batch 242\n",
      "Starting train_step for batch 243\n",
      "Starting train_step for batch 244\n",
      "Starting train_step for batch 245\n",
      "Starting train_step for batch 246\n",
      "Starting train_step for batch 247\n",
      "Starting train_step for batch 248\n",
      "Starting train_step for batch 249\n",
      "Starting train_step for batch 250\n",
      "Epoch 2, Step 250/1050\n",
      "Starting train_step for batch 251\n",
      "Starting train_step for batch 252\n",
      "Starting train_step for batch 253\n",
      "Starting train_step for batch 254\n",
      "Starting train_step for batch 255\n",
      "Starting train_step for batch 256\n",
      "Starting train_step for batch 257\n",
      "Starting train_step for batch 258\n",
      "Starting train_step for batch 259\n",
      "Starting train_step for batch 260\n",
      "Starting train_step for batch 261\n",
      "Starting train_step for batch 262\n",
      "Starting train_step for batch 263\n",
      "Starting train_step for batch 264\n",
      "Starting train_step for batch 265\n",
      "Starting train_step for batch 266\n",
      "Starting train_step for batch 267\n",
      "Starting train_step for batch 268\n",
      "Starting train_step for batch 269\n",
      "Starting train_step for batch 270\n",
      "Starting train_step for batch 271\n",
      "Starting train_step for batch 272\n",
      "Starting train_step for batch 273\n",
      "Starting train_step for batch 274\n",
      "Starting train_step for batch 275\n",
      "Starting train_step for batch 276\n",
      "Starting train_step for batch 277\n",
      "Starting train_step for batch 278\n",
      "Starting train_step for batch 279\n",
      "Starting train_step for batch 280\n",
      "Starting train_step for batch 281\n",
      "Starting train_step for batch 282\n",
      "Starting train_step for batch 283\n",
      "Starting train_step for batch 284\n",
      "Starting train_step for batch 285\n",
      "Starting train_step for batch 286\n",
      "Starting train_step for batch 287\n",
      "Starting train_step for batch 288\n",
      "Starting train_step for batch 289\n",
      "Starting train_step for batch 290\n",
      "Starting train_step for batch 291\n",
      "Starting train_step for batch 292\n",
      "Starting train_step for batch 293\n",
      "Starting train_step for batch 294\n",
      "Starting train_step for batch 295\n",
      "Starting train_step for batch 296\n",
      "Starting train_step for batch 297\n",
      "Starting train_step for batch 298\n",
      "Starting train_step for batch 299\n",
      "Starting train_step for batch 300\n",
      "Epoch 2, Step 300/1050\n",
      "Starting train_step for batch 301\n",
      "Starting train_step for batch 302\n",
      "Starting train_step for batch 303\n",
      "Starting train_step for batch 304\n",
      "Starting train_step for batch 305\n",
      "Starting train_step for batch 306\n",
      "Starting train_step for batch 307\n",
      "Starting train_step for batch 308\n",
      "Starting train_step for batch 309\n",
      "Starting train_step for batch 310\n",
      "Starting train_step for batch 311\n",
      "Starting train_step for batch 312\n",
      "Starting train_step for batch 313\n",
      "Starting train_step for batch 314\n",
      "Starting train_step for batch 315\n",
      "Starting train_step for batch 316\n",
      "Starting train_step for batch 317\n",
      "Starting train_step for batch 318\n",
      "Starting train_step for batch 319\n",
      "Starting train_step for batch 320\n",
      "Starting train_step for batch 321\n",
      "Starting train_step for batch 322\n",
      "Starting train_step for batch 323\n",
      "Starting train_step for batch 324\n",
      "Starting train_step for batch 325\n",
      "Starting train_step for batch 326\n",
      "Starting train_step for batch 327\n",
      "Starting train_step for batch 328\n",
      "Starting train_step for batch 329\n",
      "Starting train_step for batch 330\n",
      "Starting train_step for batch 331\n",
      "Starting train_step for batch 332\n",
      "Starting train_step for batch 333\n",
      "Starting train_step for batch 334\n",
      "Starting train_step for batch 335\n",
      "Starting train_step for batch 336\n",
      "Starting train_step for batch 337\n",
      "Starting train_step for batch 338\n",
      "Starting train_step for batch 339\n",
      "Starting train_step for batch 340\n",
      "Starting train_step for batch 341\n",
      "Starting train_step for batch 342\n",
      "Starting train_step for batch 343\n",
      "Starting train_step for batch 344\n",
      "Starting train_step for batch 345\n",
      "Starting train_step for batch 346\n",
      "Starting train_step for batch 347\n",
      "Starting train_step for batch 348\n",
      "Starting train_step for batch 349\n",
      "Starting train_step for batch 350\n",
      "Epoch 2, Step 350/1050\n",
      "Starting train_step for batch 351\n",
      "Starting train_step for batch 352\n",
      "Starting train_step for batch 353\n",
      "Starting train_step for batch 354\n",
      "Starting train_step for batch 355\n",
      "Starting train_step for batch 356\n",
      "Starting train_step for batch 357\n",
      "Starting train_step for batch 358\n",
      "Starting train_step for batch 359\n",
      "Starting train_step for batch 360\n",
      "Starting train_step for batch 361\n",
      "Starting train_step for batch 362\n",
      "Starting train_step for batch 363\n",
      "Starting train_step for batch 364\n",
      "Starting train_step for batch 365\n",
      "Starting train_step for batch 366\n",
      "Starting train_step for batch 367\n",
      "Starting train_step for batch 368\n",
      "Starting train_step for batch 369\n",
      "Starting train_step for batch 370\n",
      "Starting train_step for batch 371\n",
      "Starting train_step for batch 372\n",
      "Starting train_step for batch 373\n",
      "Starting train_step for batch 374\n",
      "Starting train_step for batch 375\n",
      "Starting train_step for batch 376\n",
      "Starting train_step for batch 377\n",
      "Starting train_step for batch 378\n",
      "Starting train_step for batch 379\n",
      "Starting train_step for batch 380\n",
      "Starting train_step for batch 381\n",
      "Starting train_step for batch 382\n",
      "Starting train_step for batch 383\n",
      "Starting train_step for batch 384\n",
      "Starting train_step for batch 385\n",
      "Starting train_step for batch 386\n",
      "Starting train_step for batch 387\n",
      "Starting train_step for batch 388\n",
      "Starting train_step for batch 389\n",
      "Starting train_step for batch 390\n",
      "Starting train_step for batch 391\n",
      "Starting train_step for batch 392\n",
      "Starting train_step for batch 393\n",
      "Starting train_step for batch 394\n",
      "Starting train_step for batch 395\n",
      "Starting train_step for batch 396\n",
      "Starting train_step for batch 397\n",
      "Starting train_step for batch 398\n",
      "Starting train_step for batch 399\n",
      "Starting train_step for batch 400\n",
      "Epoch 2, Step 400/1050\n",
      "Starting train_step for batch 401\n",
      "Starting train_step for batch 402\n",
      "Starting train_step for batch 403\n",
      "Starting train_step for batch 404\n",
      "Starting train_step for batch 405\n",
      "Starting train_step for batch 406\n",
      "Starting train_step for batch 407\n",
      "Starting train_step for batch 408\n",
      "Starting train_step for batch 409\n",
      "Starting train_step for batch 410\n",
      "Starting train_step for batch 411\n",
      "Starting train_step for batch 412\n",
      "Starting train_step for batch 413\n",
      "Starting train_step for batch 414\n",
      "Starting train_step for batch 415\n",
      "Starting train_step for batch 416\n",
      "Starting train_step for batch 417\n",
      "Starting train_step for batch 418\n",
      "Starting train_step for batch 419\n",
      "Starting train_step for batch 420\n",
      "Starting train_step for batch 421\n",
      "Starting train_step for batch 422\n",
      "Starting train_step for batch 423\n",
      "Starting train_step for batch 424\n",
      "Starting train_step for batch 425\n",
      "Starting train_step for batch 426\n",
      "Starting train_step for batch 427\n",
      "Starting train_step for batch 428\n",
      "Starting train_step for batch 429\n",
      "Starting train_step for batch 430\n",
      "Starting train_step for batch 431\n",
      "Starting train_step for batch 432\n",
      "Starting train_step for batch 433\n",
      "Starting train_step for batch 434\n",
      "Starting train_step for batch 435\n",
      "Starting train_step for batch 436\n",
      "Starting train_step for batch 437\n",
      "Starting train_step for batch 438\n",
      "Starting train_step for batch 439\n",
      "Starting train_step for batch 440\n",
      "Starting train_step for batch 441\n",
      "Starting train_step for batch 442\n",
      "Starting train_step for batch 443\n",
      "Starting train_step for batch 444\n",
      "Starting train_step for batch 445\n",
      "Starting train_step for batch 446\n",
      "Starting train_step for batch 447\n",
      "Starting train_step for batch 448\n",
      "Starting train_step for batch 449\n",
      "Starting train_step for batch 450\n",
      "Epoch 2, Step 450/1050\n",
      "Starting train_step for batch 451\n",
      "Starting train_step for batch 452\n",
      "Starting train_step for batch 453\n",
      "Starting train_step for batch 454\n",
      "Starting train_step for batch 455\n",
      "Starting train_step for batch 456\n",
      "Starting train_step for batch 457\n",
      "Starting train_step for batch 458\n",
      "Starting train_step for batch 459\n",
      "Starting train_step for batch 460\n",
      "Starting train_step for batch 461\n",
      "Starting train_step for batch 462\n",
      "Starting train_step for batch 463\n",
      "Starting train_step for batch 464\n",
      "Starting train_step for batch 465\n",
      "Starting train_step for batch 466\n",
      "Starting train_step for batch 467\n",
      "Starting train_step for batch 468\n",
      "Starting train_step for batch 469\n",
      "Starting train_step for batch 470\n",
      "Starting train_step for batch 471\n",
      "Starting train_step for batch 472\n",
      "Starting train_step for batch 473\n",
      "Starting train_step for batch 474\n",
      "Starting train_step for batch 475\n",
      "Starting train_step for batch 476\n",
      "Starting train_step for batch 477\n",
      "Starting train_step for batch 478\n",
      "Starting train_step for batch 479\n",
      "Starting train_step for batch 480\n",
      "Starting train_step for batch 481\n",
      "Starting train_step for batch 482\n",
      "Starting train_step for batch 483\n",
      "Starting train_step for batch 484\n",
      "Starting train_step for batch 485\n",
      "Starting train_step for batch 486\n",
      "Starting train_step for batch 487\n",
      "Starting train_step for batch 488\n",
      "Starting train_step for batch 489\n",
      "Starting train_step for batch 490\n",
      "Starting train_step for batch 491\n",
      "Starting train_step for batch 492\n",
      "Starting train_step for batch 493\n",
      "Starting train_step for batch 494\n",
      "Starting train_step for batch 495\n",
      "Starting train_step for batch 496\n",
      "Starting train_step for batch 497\n",
      "Starting train_step for batch 498\n",
      "Starting train_step for batch 499\n",
      "Starting train_step for batch 500\n",
      "Epoch 2, Step 500/1050\n",
      "Starting train_step for batch 501\n",
      "Starting train_step for batch 502\n",
      "Starting train_step for batch 503\n",
      "Starting train_step for batch 504\n",
      "Starting train_step for batch 505\n",
      "Starting train_step for batch 506\n",
      "Starting train_step for batch 507\n",
      "Starting train_step for batch 508\n",
      "Starting train_step for batch 509\n",
      "Starting train_step for batch 510\n",
      "Starting train_step for batch 511\n",
      "Starting train_step for batch 512\n",
      "Starting train_step for batch 513\n",
      "Starting train_step for batch 514\n",
      "Starting train_step for batch 515\n",
      "Starting train_step for batch 516\n",
      "Starting train_step for batch 517\n",
      "Starting train_step for batch 518\n",
      "Starting train_step for batch 519\n",
      "Starting train_step for batch 520\n",
      "Starting train_step for batch 521\n",
      "Starting train_step for batch 522\n",
      "Starting train_step for batch 523\n",
      "Starting train_step for batch 524\n",
      "Starting train_step for batch 525\n",
      "Starting train_step for batch 526\n",
      "Starting train_step for batch 527\n",
      "Starting train_step for batch 528\n",
      "Starting train_step for batch 529\n",
      "Starting train_step for batch 530\n",
      "Starting train_step for batch 531\n",
      "Starting train_step for batch 532\n",
      "Starting train_step for batch 533\n",
      "Starting train_step for batch 534\n",
      "Starting train_step for batch 535\n",
      "Starting train_step for batch 536\n",
      "Starting train_step for batch 537\n",
      "Starting train_step for batch 538\n",
      "Starting train_step for batch 539\n",
      "Starting train_step for batch 540\n",
      "Starting train_step for batch 541\n",
      "Starting train_step for batch 542\n",
      "Starting train_step for batch 543\n",
      "Starting train_step for batch 544\n",
      "Starting train_step for batch 545\n",
      "Starting train_step for batch 546\n",
      "Starting train_step for batch 547\n",
      "Starting train_step for batch 548\n",
      "Starting train_step for batch 549\n",
      "Starting train_step for batch 550\n",
      "Epoch 2, Step 550/1050\n",
      "Starting train_step for batch 551\n",
      "Starting train_step for batch 552\n",
      "Starting train_step for batch 553\n",
      "Starting train_step for batch 554\n",
      "Starting train_step for batch 555\n",
      "Starting train_step for batch 556\n",
      "Starting train_step for batch 557\n",
      "Starting train_step for batch 558\n",
      "Starting train_step for batch 559\n",
      "Starting train_step for batch 560\n",
      "Starting train_step for batch 561\n",
      "Starting train_step for batch 562\n",
      "Starting train_step for batch 563\n",
      "Starting train_step for batch 564\n",
      "Starting train_step for batch 565\n",
      "Starting train_step for batch 566\n",
      "Starting train_step for batch 567\n",
      "Starting train_step for batch 568\n",
      "Starting train_step for batch 569\n",
      "Starting train_step for batch 570\n",
      "Starting train_step for batch 571\n",
      "Starting train_step for batch 572\n",
      "Starting train_step for batch 573\n",
      "Starting train_step for batch 574\n",
      "Starting train_step for batch 575\n",
      "Starting train_step for batch 576\n",
      "Starting train_step for batch 577\n",
      "Starting train_step for batch 578\n",
      "Starting train_step for batch 579\n",
      "Starting train_step for batch 580\n",
      "Starting train_step for batch 581\n",
      "Starting train_step for batch 582\n",
      "Starting train_step for batch 583\n",
      "Starting train_step for batch 584\n",
      "Starting train_step for batch 585\n",
      "Starting train_step for batch 586\n",
      "Starting train_step for batch 587\n",
      "Starting train_step for batch 588\n",
      "Starting train_step for batch 589\n",
      "Starting train_step for batch 590\n",
      "Starting train_step for batch 591\n",
      "Starting train_step for batch 592\n",
      "Starting train_step for batch 593\n",
      "Starting train_step for batch 594\n",
      "Starting train_step for batch 595\n",
      "Starting train_step for batch 596\n",
      "Starting train_step for batch 597\n",
      "Starting train_step for batch 598\n",
      "Starting train_step for batch 599\n",
      "Starting train_step for batch 600\n",
      "Epoch 2, Step 600/1050\n",
      "Starting train_step for batch 601\n",
      "Starting train_step for batch 602\n",
      "Starting train_step for batch 603\n",
      "Starting train_step for batch 604\n",
      "Starting train_step for batch 605\n",
      "Starting train_step for batch 606\n",
      "Starting train_step for batch 607\n",
      "Starting train_step for batch 608\n",
      "Starting train_step for batch 609\n",
      "Starting train_step for batch 610\n",
      "Starting train_step for batch 611\n",
      "Starting train_step for batch 612\n",
      "Starting train_step for batch 613\n",
      "Starting train_step for batch 614\n",
      "Starting train_step for batch 615\n",
      "Starting train_step for batch 616\n",
      "Starting train_step for batch 617\n",
      "Starting train_step for batch 618\n",
      "Starting train_step for batch 619\n",
      "Starting train_step for batch 620\n",
      "Starting train_step for batch 621\n",
      "Starting train_step for batch 622\n",
      "Starting train_step for batch 623\n",
      "Starting train_step for batch 624\n",
      "Starting train_step for batch 625\n",
      "Starting train_step for batch 626\n",
      "Starting train_step for batch 627\n",
      "Starting train_step for batch 628\n",
      "Starting train_step for batch 629\n",
      "Starting train_step for batch 630\n",
      "Starting train_step for batch 631\n",
      "Starting train_step for batch 632\n",
      "Starting train_step for batch 633\n",
      "Starting train_step for batch 634\n",
      "Starting train_step for batch 635\n",
      "Starting train_step for batch 636\n",
      "Starting train_step for batch 637\n",
      "Starting train_step for batch 638\n",
      "Starting train_step for batch 639\n",
      "Starting train_step for batch 640\n",
      "Starting train_step for batch 641\n",
      "Starting train_step for batch 642\n",
      "Starting train_step for batch 643\n",
      "Starting train_step for batch 644\n",
      "Starting train_step for batch 645\n",
      "Starting train_step for batch 646\n",
      "Starting train_step for batch 647\n",
      "Starting train_step for batch 648\n",
      "Starting train_step for batch 649\n",
      "Starting train_step for batch 650\n",
      "Epoch 2, Step 650/1050\n",
      "Starting train_step for batch 651\n",
      "Starting train_step for batch 652\n",
      "Starting train_step for batch 653\n",
      "Starting train_step for batch 654\n",
      "Starting train_step for batch 655\n",
      "Starting train_step for batch 656\n",
      "Starting train_step for batch 657\n",
      "Starting train_step for batch 658\n",
      "Starting train_step for batch 659\n",
      "Starting train_step for batch 660\n",
      "Starting train_step for batch 661\n",
      "Starting train_step for batch 662\n",
      "Starting train_step for batch 663\n",
      "Starting train_step for batch 664\n",
      "Starting train_step for batch 665\n",
      "Starting train_step for batch 666\n",
      "Starting train_step for batch 667\n",
      "Starting train_step for batch 668\n",
      "Starting train_step for batch 669\n",
      "Starting train_step for batch 670\n",
      "Starting train_step for batch 671\n",
      "Starting train_step for batch 672\n",
      "Starting train_step for batch 673\n",
      "Starting train_step for batch 674\n",
      "Starting train_step for batch 675\n",
      "Starting train_step for batch 676\n",
      "Starting train_step for batch 677\n",
      "Starting train_step for batch 678\n",
      "Starting train_step for batch 679\n",
      "Starting train_step for batch 680\n",
      "Starting train_step for batch 681\n",
      "Starting train_step for batch 682\n",
      "Starting train_step for batch 683\n",
      "Starting train_step for batch 684\n",
      "Starting train_step for batch 685\n",
      "Starting train_step for batch 686\n",
      "Starting train_step for batch 687\n",
      "Starting train_step for batch 688\n",
      "Starting train_step for batch 689\n",
      "Starting train_step for batch 690\n",
      "Starting train_step for batch 691\n",
      "Starting train_step for batch 692\n",
      "Starting train_step for batch 693\n",
      "Starting train_step for batch 694\n",
      "Starting train_step for batch 695\n",
      "Starting train_step for batch 696\n",
      "Starting train_step for batch 697\n",
      "Starting train_step for batch 698\n",
      "Starting train_step for batch 699\n",
      "Starting train_step for batch 700\n",
      "Epoch 2, Step 700/1050\n",
      "Starting train_step for batch 701\n",
      "Starting train_step for batch 702\n",
      "Starting train_step for batch 703\n",
      "Starting train_step for batch 704\n",
      "Starting train_step for batch 705\n",
      "Starting train_step for batch 706\n",
      "Starting train_step for batch 707\n",
      "Starting train_step for batch 708\n",
      "Starting train_step for batch 709\n",
      "Starting train_step for batch 710\n",
      "Starting train_step for batch 711\n",
      "Starting train_step for batch 712\n",
      "Starting train_step for batch 713\n",
      "Starting train_step for batch 714\n",
      "Starting train_step for batch 715\n",
      "Starting train_step for batch 716\n",
      "Starting train_step for batch 717\n",
      "Starting train_step for batch 718\n",
      "Starting train_step for batch 719\n",
      "Starting train_step for batch 720\n",
      "Starting train_step for batch 721\n",
      "Starting train_step for batch 722\n",
      "Starting train_step for batch 723\n",
      "Starting train_step for batch 724\n",
      "Starting train_step for batch 725\n",
      "Starting train_step for batch 726\n",
      "Starting train_step for batch 727\n",
      "Starting train_step for batch 728\n",
      "Starting train_step for batch 729\n",
      "Starting train_step for batch 730\n",
      "Starting train_step for batch 731\n",
      "Starting train_step for batch 732\n",
      "Starting train_step for batch 733\n",
      "Starting train_step for batch 734\n",
      "Starting train_step for batch 735\n",
      "Starting train_step for batch 736\n",
      "Starting train_step for batch 737\n",
      "Starting train_step for batch 738\n",
      "Starting train_step for batch 739\n",
      "Starting train_step for batch 740\n",
      "Starting train_step for batch 741\n",
      "Starting train_step for batch 742\n",
      "Starting train_step for batch 743\n",
      "Starting train_step for batch 744\n",
      "Starting train_step for batch 745\n",
      "Starting train_step for batch 746\n",
      "Starting train_step for batch 747\n",
      "Starting train_step for batch 748\n",
      "Starting train_step for batch 749\n",
      "Starting train_step for batch 750\n",
      "Epoch 2, Step 750/1050\n",
      "Starting train_step for batch 751\n",
      "Starting train_step for batch 752\n",
      "Starting train_step for batch 753\n",
      "Starting train_step for batch 754\n",
      "Starting train_step for batch 755\n",
      "Starting train_step for batch 756\n",
      "Starting train_step for batch 757\n",
      "Starting train_step for batch 758\n",
      "Starting train_step for batch 759\n",
      "Starting train_step for batch 760\n",
      "Starting train_step for batch 761\n",
      "Starting train_step for batch 762\n",
      "Starting train_step for batch 763\n",
      "Starting train_step for batch 764\n",
      "Starting train_step for batch 765\n",
      "Starting train_step for batch 766\n",
      "Starting train_step for batch 767\n",
      "Starting train_step for batch 768\n",
      "Starting train_step for batch 769\n",
      "Starting train_step for batch 770\n",
      "Starting train_step for batch 771\n",
      "Starting train_step for batch 772\n",
      "Starting train_step for batch 773\n",
      "Starting train_step for batch 774\n",
      "Starting train_step for batch 775\n",
      "Starting train_step for batch 776\n",
      "Starting train_step for batch 777\n",
      "Starting train_step for batch 778\n",
      "Starting train_step for batch 779\n",
      "Starting train_step for batch 780\n",
      "Starting train_step for batch 781\n",
      "Starting train_step for batch 782\n",
      "Starting train_step for batch 783\n",
      "Starting train_step for batch 784\n",
      "Starting train_step for batch 785\n",
      "Starting train_step for batch 786\n",
      "Starting train_step for batch 787\n",
      "Starting train_step for batch 788\n",
      "Starting train_step for batch 789\n",
      "Starting train_step for batch 790\n",
      "Starting train_step for batch 791\n",
      "Starting train_step for batch 792\n",
      "Starting train_step for batch 793\n",
      "Starting train_step for batch 794\n",
      "Starting train_step for batch 795\n",
      "Starting train_step for batch 796\n",
      "Starting train_step for batch 797\n",
      "Starting train_step for batch 798\n",
      "Starting train_step for batch 799\n",
      "Starting train_step for batch 800\n",
      "Epoch 2, Step 800/1050\n",
      "Starting train_step for batch 801\n",
      "Starting train_step for batch 802\n",
      "Starting train_step for batch 803\n",
      "Starting train_step for batch 804\n",
      "Starting train_step for batch 805\n",
      "Starting train_step for batch 806\n",
      "Starting train_step for batch 807\n",
      "Starting train_step for batch 808\n",
      "Starting train_step for batch 809\n",
      "Starting train_step for batch 810\n",
      "Starting train_step for batch 811\n",
      "Starting train_step for batch 812\n",
      "Starting train_step for batch 813\n",
      "Starting train_step for batch 814\n",
      "Starting train_step for batch 815\n",
      "Starting train_step for batch 816\n",
      "Starting train_step for batch 817\n",
      "Starting train_step for batch 818\n",
      "Starting train_step for batch 819\n",
      "Starting train_step for batch 820\n",
      "Starting train_step for batch 821\n",
      "Starting train_step for batch 822\n",
      "Starting train_step for batch 823\n",
      "Starting train_step for batch 824\n",
      "Starting train_step for batch 825\n",
      "Starting train_step for batch 826\n",
      "Starting train_step for batch 827\n",
      "Starting train_step for batch 828\n",
      "Starting train_step for batch 829\n",
      "Starting train_step for batch 830\n",
      "Starting train_step for batch 831\n",
      "Starting train_step for batch 832\n",
      "Starting train_step for batch 833\n",
      "Starting train_step for batch 834\n",
      "Starting train_step for batch 835\n",
      "Starting train_step for batch 836\n",
      "Starting train_step for batch 837\n",
      "Starting train_step for batch 838\n",
      "Starting train_step for batch 839\n",
      "Starting train_step for batch 840\n",
      "Starting train_step for batch 841\n",
      "Starting train_step for batch 842\n",
      "Starting train_step for batch 843\n",
      "Starting train_step for batch 844\n",
      "Starting train_step for batch 845\n",
      "Starting train_step for batch 846\n",
      "Starting train_step for batch 847\n",
      "Starting train_step for batch 848\n",
      "Starting train_step for batch 849\n",
      "Starting train_step for batch 850\n",
      "Epoch 2, Step 850/1050\n",
      "Starting train_step for batch 851\n",
      "Starting train_step for batch 852\n",
      "Starting train_step for batch 853\n",
      "Starting train_step for batch 854\n",
      "Starting train_step for batch 855\n",
      "Starting train_step for batch 856\n",
      "Starting train_step for batch 857\n",
      "Starting train_step for batch 858\n",
      "Starting train_step for batch 859\n",
      "Starting train_step for batch 860\n",
      "Starting train_step for batch 861\n",
      "Starting train_step for batch 862\n",
      "Starting train_step for batch 863\n",
      "Starting train_step for batch 864\n",
      "Starting train_step for batch 865\n",
      "Starting train_step for batch 866\n",
      "Starting train_step for batch 867\n",
      "Starting train_step for batch 868\n",
      "Starting train_step for batch 869\n",
      "Starting train_step for batch 870\n",
      "Starting train_step for batch 871\n",
      "Starting train_step for batch 872\n",
      "Starting train_step for batch 873\n",
      "Starting train_step for batch 874\n",
      "Starting train_step for batch 875\n",
      "Starting train_step for batch 876\n",
      "Starting train_step for batch 877\n",
      "Starting train_step for batch 878\n",
      "Starting train_step for batch 879\n",
      "Starting train_step for batch 880\n",
      "Starting train_step for batch 881\n",
      "Starting train_step for batch 882\n",
      "Starting train_step for batch 883\n",
      "Starting train_step for batch 884\n",
      "Starting train_step for batch 885\n",
      "Starting train_step for batch 886\n",
      "Starting train_step for batch 887\n",
      "Starting train_step for batch 888\n",
      "Starting train_step for batch 889\n",
      "Starting train_step for batch 890\n",
      "Starting train_step for batch 891\n",
      "Starting train_step for batch 892\n",
      "Starting train_step for batch 893\n",
      "Starting train_step for batch 894\n",
      "Starting train_step for batch 895\n",
      "Starting train_step for batch 896\n",
      "Starting train_step for batch 897\n",
      "Starting train_step for batch 898\n",
      "Starting train_step for batch 899\n",
      "Starting train_step for batch 900\n",
      "Epoch 2, Step 900/1050\n",
      "Starting train_step for batch 901\n",
      "Starting train_step for batch 902\n",
      "Starting train_step for batch 903\n",
      "Starting train_step for batch 904\n",
      "Starting train_step for batch 905\n",
      "Starting train_step for batch 906\n",
      "Starting train_step for batch 907\n",
      "Starting train_step for batch 908\n",
      "Starting train_step for batch 909\n",
      "Starting train_step for batch 910\n",
      "Starting train_step for batch 911\n",
      "Starting train_step for batch 912\n",
      "Starting train_step for batch 913\n",
      "Starting train_step for batch 914\n",
      "Starting train_step for batch 915\n",
      "Starting train_step for batch 916\n",
      "Starting train_step for batch 917\n",
      "Starting train_step for batch 918\n",
      "Starting train_step for batch 919\n",
      "Starting train_step for batch 920\n",
      "Starting train_step for batch 921\n",
      "Starting train_step for batch 922\n",
      "Starting train_step for batch 923\n",
      "Starting train_step for batch 924\n",
      "Starting train_step for batch 925\n",
      "Starting train_step for batch 926\n",
      "Starting train_step for batch 927\n",
      "Starting train_step for batch 928\n",
      "Starting train_step for batch 929\n",
      "Starting train_step for batch 930\n",
      "Starting train_step for batch 931\n",
      "Starting train_step for batch 932\n",
      "Starting train_step for batch 933\n",
      "Starting train_step for batch 934\n",
      "Starting train_step for batch 935\n",
      "Starting train_step for batch 936\n",
      "Starting train_step for batch 937\n",
      "Starting train_step for batch 938\n",
      "Starting train_step for batch 939\n",
      "Starting train_step for batch 940\n",
      "Starting train_step for batch 941\n",
      "Starting train_step for batch 942\n",
      "Starting train_step for batch 943\n",
      "Starting train_step for batch 944\n",
      "Starting train_step for batch 945\n",
      "Starting train_step for batch 946\n",
      "Starting train_step for batch 947\n",
      "Starting train_step for batch 948\n",
      "Starting train_step for batch 949\n",
      "Starting train_step for batch 950\n",
      "Epoch 2, Step 950/1050\n",
      "Starting train_step for batch 951\n",
      "Starting train_step for batch 952\n",
      "Starting train_step for batch 953\n",
      "Starting train_step for batch 954\n",
      "Starting train_step for batch 955\n",
      "Starting train_step for batch 956\n",
      "Starting train_step for batch 957\n",
      "Starting train_step for batch 958\n",
      "Starting train_step for batch 959\n",
      "Starting train_step for batch 960\n",
      "Starting train_step for batch 961\n",
      "Starting train_step for batch 962\n",
      "Starting train_step for batch 963\n",
      "Starting train_step for batch 964\n",
      "Starting train_step for batch 965\n",
      "Starting train_step for batch 966\n",
      "Starting train_step for batch 967\n",
      "Starting train_step for batch 968\n",
      "Starting train_step for batch 969\n",
      "Starting train_step for batch 970\n",
      "Starting train_step for batch 971\n",
      "Starting train_step for batch 972\n",
      "Starting train_step for batch 973\n",
      "Starting train_step for batch 974\n",
      "Starting train_step for batch 975\n",
      "Starting train_step for batch 976\n",
      "Starting train_step for batch 977\n",
      "Starting train_step for batch 978\n",
      "Starting train_step for batch 979\n",
      "Starting train_step for batch 980\n",
      "Starting train_step for batch 981\n",
      "Starting train_step for batch 982\n",
      "Starting train_step for batch 983\n",
      "Starting train_step for batch 984\n",
      "Starting train_step for batch 985\n",
      "Starting train_step for batch 986\n",
      "Starting train_step for batch 987\n",
      "Starting train_step for batch 988\n",
      "Starting train_step for batch 989\n",
      "Starting train_step for batch 990\n",
      "Starting train_step for batch 991\n",
      "Starting train_step for batch 992\n",
      "Starting train_step for batch 993\n",
      "Starting train_step for batch 994\n",
      "Starting train_step for batch 995\n",
      "Starting train_step for batch 996\n",
      "Starting train_step for batch 997\n",
      "Starting train_step for batch 998\n",
      "Starting train_step for batch 999\n",
      "Starting train_step for batch 1000\n",
      "Epoch 2, Step 1000/1050\n",
      "Starting train_step for batch 1001\n",
      "Starting train_step for batch 1002\n",
      "Starting train_step for batch 1003\n",
      "Starting train_step for batch 1004\n",
      "Starting train_step for batch 1005\n",
      "Starting train_step for batch 1006\n",
      "Starting train_step for batch 1007\n",
      "Starting train_step for batch 1008\n",
      "Starting train_step for batch 1009\n",
      "Starting train_step for batch 1010\n",
      "Starting train_step for batch 1011\n",
      "Starting train_step for batch 1012\n",
      "Starting train_step for batch 1013\n",
      "Starting train_step for batch 1014\n",
      "Starting train_step for batch 1015\n",
      "Starting train_step for batch 1016\n",
      "Starting train_step for batch 1017\n",
      "Starting train_step for batch 1018\n",
      "Starting train_step for batch 1019\n",
      "Starting train_step for batch 1020\n",
      "Starting train_step for batch 1021\n",
      "Starting train_step for batch 1022\n",
      "Starting train_step for batch 1023\n",
      "Starting train_step for batch 1024\n",
      "Starting train_step for batch 1025\n",
      "Starting train_step for batch 1026\n",
      "Starting train_step for batch 1027\n",
      "Starting train_step for batch 1028\n",
      "Starting train_step for batch 1029\n",
      "Starting train_step for batch 1030\n",
      "Starting train_step for batch 1031\n",
      "Starting train_step for batch 1032\n",
      "Starting train_step for batch 1033\n",
      "Starting train_step for batch 1034\n",
      "Starting train_step for batch 1035\n",
      "Starting train_step for batch 1036\n",
      "Starting train_step for batch 1037\n",
      "Starting train_step for batch 1038\n",
      "Starting train_step for batch 1039\n",
      "Starting train_step for batch 1040\n",
      "Starting train_step for batch 1041\n",
      "Starting train_step for batch 1042\n",
      "Starting train_step for batch 1043\n",
      "Starting train_step for batch 1044\n",
      "Starting train_step for batch 1045\n",
      "Starting train_step for batch 1046\n",
      "Starting train_step for batch 1047\n",
      "Starting train_step for batch 1048\n",
      "Starting train_step for batch 1049\n",
      "Starting train_step for batch 1050\n",
      "Epoch 2, Step 1050/1050\n",
      "Epoch 2 - Average train loss: 0.0002\n",
      "Training completed for combined_afro_xlmr_llama\n",
      "Training results: {'train_loss': 0.00017770464076463777, 'epochs_trained': 2}\n",
      "Model saved to ./model_outputs/combined_afro_xlmr_llama\n",
      "Evaluating: Batch 0/225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory (End of training) - Allocated: 54.95GB (Max: 70.33GB), Reserved: 55.12GB (Max: 71.27GB)\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 10/225\n",
      "Evaluating: Batch 20/225\n",
      "Evaluating: Batch 30/225\n",
      "Evaluating: Batch 40/225\n",
      "Evaluating: Batch 50/225\n",
      "Cleared GPU memory. Current allocated: 59.00 GB\n",
      "Evaluating: Batch 60/225\n",
      "Evaluating: Batch 70/225\n",
      "Evaluating: Batch 80/225\n",
      "Evaluating: Batch 90/225\n",
      "Evaluating: Batch 100/225\n",
      "Cleared GPU memory. Current allocated: 59.00 GB\n",
      "Evaluating: Batch 110/225\n",
      "Evaluating: Batch 120/225\n",
      "Evaluating: Batch 130/225\n",
      "Evaluating: Batch 140/225\n",
      "Evaluating: Batch 150/225\n",
      "Cleared GPU memory. Current allocated: 59.00 GB\n",
      "Evaluating: Batch 160/225\n",
      "Evaluating: Batch 170/225\n",
      "Evaluating: Batch 180/225\n",
      "Evaluating: Batch 190/225\n",
      "Evaluating: Batch 200/225\n",
      "Cleared GPU memory. Current allocated: 59.00 GB\n",
      "Evaluating: Batch 210/225\n",
      "Evaluating: Batch 220/225\n",
      "Average evaluation loss: 10.053349\n",
      "Intent recognition accuracy: 1.0000\n",
      "Slot filling F1 score: 1.0000\n",
      "Example 1:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [30]\n",
      "  Predicted Slots: [30]\n",
      "Example 2:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [30 30 30]\n",
      "  Predicted Slots: [30 30 30]\n",
      "Example 3:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [ 7 30 30 30]\n",
      "  Predicted Slots: [ 7 30 30 30]\n",
      "Example 4:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [30]\n",
      "  Predicted Slots: [30]\n",
      "Example 5:\n",
      "  True Intent: 27, Predicted Intent: 27\n",
      "  True Slots: [30]\n",
      "  Predicted Slots: [30]\n",
      "Unique true intents: {27, 38}\n",
      "Unique predicted intents: {27, 38}\n",
      "Unique true slots: {30, 7}\n",
      "Unique predicted slots: {30, 7}\n",
      "Cleared GPU memory. Current allocated: 59.01 GB\n",
      "Evaluation results for combined_afro_xlmr_llama: {'eval_loss': 10.053348541259766, 'intent_accuracy': 1.0, 'slot_f1': 1.0}\n",
      "Training process completed for all models.\n"
     ]
    }
   ],
   "source": [
    "# Train models with the best hyperparameters\n",
    "for model_name, trainer in trainers.items():\n",
    "    logger.info(f\"Starting training for model: {model_name}\")\n",
    "    \n",
    "    # Determine the correct dataset keys\n",
    "    train_dataset_key = 'train'\n",
    "    eval_dataset_key = 'eval'\n",
    "    \n",
    "    # Ensure the required datasets exist\n",
    "    if train_dataset_key not in datasets['combined_afro_xlmr_llama'] or eval_dataset_key not in datasets['combined_afro_xlmr_llama']:\n",
    "        logger.error(f\"Required datasets not found for {model_name}. Skipping this model.\")\n",
    "        continue\n",
    "\n",
    "    train_dataset = datasets['combined_afro_xlmr_llama'][train_dataset_key]\n",
    "    eval_dataset = datasets['combined_afro_xlmr_llama'][eval_dataset_key]\n",
    "\n",
    "    logger.info(f\"Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        train_results = trainer.train(train_dataset, eval_dataset)\n",
    "        \n",
    "        # Log training results\n",
    "        logger.info(f\"Training completed for {model_name}\")\n",
    "        logger.info(f\"Training results: {train_results}\")\n",
    "        \n",
    "        # Save the trained model\n",
    "        save_dir = os.path.join(config['model']['output_dir'], model_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # trainer.save_model(save_dir)\n",
    "        logger.info(f\"Model saved to {save_dir}\")\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_results = trainer.evaluate(eval_dataset)\n",
    "        logger.info(f\"Evaluation results for {model_name}: {eval_results}\")\n",
    "        \n",
    "        # You might want to save these results to a file or database\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training or evaluation of {model_name}: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        continue  # Move to the next model if there's an error\n",
    "\n",
    "logger.info(\"Training process completed for all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([     0,   3664, 111868,   1704,  14770,    208,  60657,  15623,  33416,\n",
      "           156,    156,   7978,    842,   2866,  36155,   2387,     80,     75,\n",
      "         38269,    151,  59085,  19915,   2527,    429,    708,  14770,    177,\n",
      "         44190,   3613,  28821,   3584,     36,   8789,   6990,    156,   7978,\n",
      "           842,   2866,  36155,   2387,     80,  16670,      2,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "print(datasets['combined_afro_xlmr_llama']['benchmark'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0    I want to fly from Nairobi to Nueva York mañana   \n",
      "1  Reserva un vuelo from Nairobi to London por favor   \n",
      "2      Book a flight desde Nairobi a París next week   \n",
      "3             Quiero un hotel en Tokyo para mi viaje   \n",
      "4              Find me a restaurante in Berlin bitte   \n",
      "\n",
      "                                  label language      split  \n",
      "0  O O O O O B-LOC O B-LOC I-LOC B-DATE      eng  zero_shot  \n",
      "1             O O O O B-LOC O B-LOC O O      eng  zero_shot  \n",
      "2   O O O O B-LOC O B-LOC B-DATE I-DATE      eng  zero_shot  \n",
      "3                   O O O O B-LOC O O O      eng  zero_shot  \n",
      "4                     O O O O O B-LOC O      eng  zero_shot  \n"
     ]
    }
   ],
   "source": [
    "print(stratified_datasets['experimental']['code_switch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FLORES-200 'text' dataset shape: (225,)\n",
      "Error during evaluation of afro-xlmr-large: 'src_lang'\n",
      "FLORES-200 'text' dataset shape: (225,)\n",
      "Error during evaluation of meta-llama/Llama-2-7b-hf: 'src_lang'\n"
     ]
    }
   ],
   "source": [
    "# Evaluation for all models\n",
    "for model_name, model in models.items():\n",
    "    tokenizer = tokenizers['afro-xlmr-large']\n",
    "    evaluator = evaluators['combined_afro_xlmr_llama']  # Assuming all evaluators are AfriCOMETEvaluator\n",
    "\n",
    "    try:\n",
    "         # Use the FLORES data that's already loaded\n",
    "        if 'benchmark' in stratified_datasets:\n",
    "            results['translation'][model_name] = evaluate_translation(model, tokenizer, stratified_datasets['benchmark'], evaluator)\n",
    "        else:\n",
    "            logger.warning(f\"FLORES data not available for model: {model_name}\")\n",
    "        \n",
    "        # Zero-shot evaluation\n",
    "        if 'experimental' in stratified_datasets and 'zero_shot' in stratified_datasets['experimental']:\n",
    "            zero_shot_data = stratified_datasets['experimental']['zero_shot']\n",
    "            results['zero_shot'][model_name] = evaluate_code_switch(zero_shot_data, code_switch_classifier)\n",
    "        else:                                    \n",
    "            logger.info(f\"No zero-shot data for model: {model_name}\")\n",
    "        \n",
    "        # Code-switch evaluation\n",
    "        if 'experimental' in stratified_datasets and 'code_switch' in stratified_datasets['experimental']:\n",
    "            code_switch_data = stratified_datasets['experimental']['code_switch']\n",
    "            results['code_switch'][model_name] = evaluate_zero_shot(code_switch_data, zero_shot_classifier)\n",
    "        else:\n",
    "            logger.info(f\"No code-switch data for model: {model_name}\")\n",
    "        \n",
    "        logger.info(f\"Completed evaluation for {model_name}\")\n",
    "        \n",
    "        logger.info(f\"Completed evaluation for {model_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during evaluation of {model_name}: {str(e)}\")\n",
    "        continue  # Move to the next model if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification': {}, 'translation': {'afro-xlmr-large': {'translations': {}}, 'meta-llama/Llama-2-7b-hf': {'translations': {}}}, 'generation': {}, 'zero_shot': {}, 'code_switch': {}, 'intent_recognition': {}, 'slot_filling': {}, 'hyperparameter_studies': {'combined_study': <optuna.study.study.Study object at 0x7f8f262249d0>}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing results...\n",
      "Evaluation Results Summary:\n",
      "                   afro-xlmr-large meta-llama/Llama-2-7b-hf\n",
      "translation                   None                     None\n",
      "intent_recognition            None                     None\n",
      "slot_filling                  None                     None\n",
      "zero_shot                     None                     None\n",
      "code_switch                   None                     None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'summary_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m results_summary \u001b[38;5;241m=\u001b[39m summarize_results(results, config)\n\u001b[1;32m      3\u001b[0m plot_results(results, config)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mlog_results_to_mlflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[78], line 23\u001b[0m, in \u001b[0;36mlog_results_to_mlflow\u001b[0;34m(results, config, best_params)\u001b[0m\n\u001b[1;32m     20\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43msummary_df\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m summary_df[model_name]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize and log results\n",
    "results_summary = summarize_results(results, config)\n",
    "plot_results(results, config)\n",
    "log_results_to_mlflow(results, config, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Evaluation complete!\")\n",
    "print(\"Evaluation completed successfully. Results and visualizations have been saved and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results and visualizations\n",
    "display(Image(filename=f\"{config['model']['output_dir']}/overall_performance_heatmap.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameter optimization results\n",
    "for model_name in config['model']['names']:\n",
    "    print(f\"\\nHyperparameter Optimization Results for {model_name}:\")\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_summary(results_summary, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation notebook execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
