{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow --ignore-installed embedchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna-integration lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers datasets peft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os\n",
    "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'  # Suppress Git warnings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Enable CUDA launch blocking for debugging\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"  # Enable CUDA device-side assertions\n",
    "os.environ['MLFLOW_FLATTEN_PARAMS'] = 'true' # Flatten parameters parameters for logging\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress info messages\n",
    "\n",
    "import sys\n",
    "sys.path.append('../py')  # Add the parent directory to the Python path\n",
    "import gc\n",
    "import torch\n",
    "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import mlflow\n",
    "import optuna\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from IPython.display import Image, display\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from packaging import version\n",
    "\n",
    "from dataset_loader import DatasetLoader\n",
    "from utils import set_seed, load_config, get_device, CustomDataset\n",
    "from models.llama2_decoder import Llama2Decoder  # Import Llama2Decoder model\n",
    "from models.afro_xlmr_large import AfroXLMRLarge  # Import AfroXLMRLarge model\n",
    "from models.gemini import GeminiModel # Import Gemini model\n",
    "from evaluators.africomet_evaluator import AfriCOMETEvaluator\n",
    "from classifiers.zeroshot_classifier import ZeroShotClassifier\n",
    "from classifiers.codeswitch_classifier import CodeSwitchClassifier\n",
    "from classifiers.zeroshot_classifier_for_gemini import ZeroShotClassifierForGemini\n",
    "from classifiers.codeswitch_classifier_for_gemini import CodeSwitchClassifierForGemini\n",
    "from trainers.combined_encoder_decoder_trainer import CombinedEncoderDecoderTrainer\n",
    "from trainers.gemini_trainer import GeminiTrainer\n",
    "from hyperparameter_analysis import (plot_hyperparameter_importance, plot_study_optimization_history,\n",
    "                                     plot_parallel_coordinate, analyze_hyperparameter_sensitivity,\n",
    "                                     plot_sensitivity_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(config):\n",
    "    \"\"\"\n",
    "    Set up logging configuration based on the provided config.\n",
    "    \n",
    "    This function initializes the logging system with the specified log level,\n",
    "    format, and output file from the configuration.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing logging configuration.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger object.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, config['logging']['log_level']),\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        filename=config['logging']['log_file']\n",
    "    )\n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clear unused memory to prevent out-of-memory errors.\n",
    "    \n",
    "    This function uses Python's garbage collector and PyTorch's CUDA memory \n",
    "    cache clearing (if available) to free up memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    logging.info(f\"Cleared GPU memory. Current allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_loader):\n",
    "    \"\"\"\n",
    "    Load datasets using the provided DatasetLoader object.\n",
    "    \n",
    "    This function attempts to load all datasets specified in the configuration\n",
    "    using the DatasetLoader. It includes error handling for common issues.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DatasetLoader): An instance of the DatasetLoader class.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary of loaded datasets, or None if loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Loading and preparing datasets...\")\n",
    "        return data_loader.load_datasets()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading datasets: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_combined(trial, encoder, decoder, encoder_tokenizer, decoder_tokenizer, train_dataset, eval_dataset, evaluator):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization of the combined Afro-XLMR and LLaMA model, including intent recognition and slot filling tasks.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The Optuna trial object used for hyperparameter suggestions.\n",
    "        encoder (torch.nn.Module): The Afro-XLMR encoder model to be trained and evaluated.\n",
    "        decoder (torch.nn.Module): The LLaMA decoder model to be trained and evaluated.\n",
    "        encoder_tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the encoder.\n",
    "        decoder_tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the decoder.\n",
    "        train_dataset (Dataset): The dataset used for training the model.\n",
    "        eval_dataset (Dataset): The dataset used for evaluating the model.\n",
    "        evaluator (Evaluator): The evaluator object used to compute evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation metric to be minimized (lower is better).\n",
    "    \"\"\"\n",
    "    # Initialize Accelerator\n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(mixed_precision='no', kwargs_handlers=[ddp_kwargs])\n",
    "    \n",
    "    # Hyperparameter suggestions based on config\n",
    "    try:\n",
    "        hyperparams = config.get('hyperparameters', {})\n",
    "        \n",
    "        # Learning rates\n",
    "        lr_min = float(hyperparams.get('learning_rate_min', 1e-6))\n",
    "        lr_max = float(hyperparams.get('learning_rate_max', 1e-4))\n",
    "        \n",
    "        if lr_min >= lr_max:\n",
    "            logging.error(f\"Invalid learning rate range: min ({lr_min}) must be less than max ({lr_max})\")\n",
    "            return float('inf')\n",
    "        \n",
    "        encoder_lr = trial.suggest_float('encoder_lr', lr_min, lr_max, log=True)\n",
    "        decoder_lr = trial.suggest_float('decoder_lr', lr_min, lr_max, log=True)\n",
    "        \n",
    "        # Number of training epochs\n",
    "        num_train_epochs = trial.suggest_int('num_train_epochs', \n",
    "                                             int(hyperparams.get('num_train_epochs_min', 1)), \n",
    "                                             int(hyperparams.get('num_train_epochs_max', 3)))\n",
    "        \n",
    "        per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', \n",
    "                                                                hyperparams.get('batch_sizes', [1, 2, 4, 8]))\n",
    "        \n",
    "        weight_decay = trial.suggest_float('weight_decay', \n",
    "                                           float(hyperparams.get('weight_decay_min', 0.01)), \n",
    "                                           float(hyperparams.get('weight_decay_max', 0.1)), \n",
    "                                           log=True)\n",
    "        \n",
    "        warmup_steps = trial.suggest_int('warmup_steps', \n",
    "                                         int(hyperparams.get('warmup_steps_min', 0)), \n",
    "                                         int(hyperparams.get('warmup_steps_max', 1000)))\n",
    "        \n",
    "        gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', \n",
    "                                                                hyperparams.get('gradient_accumulation_steps', [1, 2, 4, 8]))\n",
    "        \n",
    "        # Add hyperparameters for intent and slot tasks\n",
    "        intent_loss_weight = trial.suggest_float('intent_loss_weight', 0.1, 1.0)\n",
    "        slot_loss_weight = trial.suggest_float('slot_loss_weight', 0.1, 1.0)\n",
    "\n",
    "        # Create trial config\n",
    "        trial_config = {\n",
    "            \"encoder_lr\": encoder_lr,\n",
    "            \"decoder_lr\": decoder_lr,\n",
    "            \"num_train_epochs\": num_train_epochs,\n",
    "            \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "            \"per_device_eval_batch_size\": per_device_train_batch_size,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "            \"fp16\": False,\n",
    "            \"evaluation_strategy\": \"steps\",\n",
    "            \"eval_steps\": 200,\n",
    "            \"save_steps\": 200,\n",
    "            \"logging_steps\": 50,\n",
    "            \"max_grad_norm\": 1.0,\n",
    "            \"output_dir\": config['model']['output_dir'],\n",
    "            \"seed\": config['seed'],\n",
    "            \"device\": config['device'],\n",
    "            \"cache_dir\": config['cache']['dir'],\n",
    "            \"gradient_checkpointing\": True,  \n",
    "            \"intent_loss_weight\": intent_loss_weight, \n",
    "            \"slot_loss_weight\": slot_loss_weight,  \n",
    "            \"num_intent_classes\": 50,  \n",
    "            \"num_slot_classes\": 100  \n",
    "        }\n",
    "    \n",
    "        logging.info(f\"Trial {trial.number}: Starting with hyperparameters: {trial_config}\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error converting config values to numeric types: {str(e)}\")\n",
    "        return float('inf')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in hyperparameter suggestion: {str(e)}\")\n",
    "        return float('inf')\n",
    "\n",
    "    # Enable gradient checkpointing for both encoder and decoder\n",
    "    encoder.gradient_checkpointing_enable()\n",
    "    decoder.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = CombinedEncoderDecoderTrainer(encoder, decoder, encoder_tokenizer, decoder_tokenizer, config=trial_config, accelerator=accelerator, batch_size=64)\n",
    "\n",
    "    # Prepare models and optimizers\n",
    "    trainer.encoder, trainer.decoder, trainer.projection, trainer.encoder_optimizer, trainer.decoder_optimizer = accelerator.prepare(\n",
    "        trainer.encoder, trainer.decoder, trainer.projection, trainer.encoder_optimizer, trainer.decoder_optimizer\n",
    "    )\n",
    "\n",
    "    # Log dataset sizes\n",
    "    logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    logging.info(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "    # Validate datasets\n",
    "    trainer.validate_dataset(train_dataset, \"Training\")\n",
    "    trainer.validate_dataset(eval_dataset, \"Evaluation\")\n",
    "\n",
    "    best_metric = float('inf')  # Initialize for early stopping\n",
    "    patience_counter = 0\n",
    "    patience_threshold = trainer.patience  # Use the patience from the trainer configuration\n",
    "\n",
    "    # Train the combined model\n",
    "    for epoch in range(num_train_epochs):\n",
    "        logging.info(f\"Starting epoch {epoch + 1}/{num_train_epochs}\")\n",
    "        train_result = trainer.train(train_dataset, eval_dataset)\n",
    "\n",
    "        # Clear cache after training\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Evaluate the model after each epoch\n",
    "        try:\n",
    "            # Use the evaluate() method here\n",
    "            eval_metrics = trainer.evaluate(eval_dataset)\n",
    "            \n",
    "            # Generate translations for FLORES evaluation\n",
    "            generated_results = trainer.generate_batch(eval_dataset)\n",
    "            translated_texts, _, _ = zip(*generated_results)  # We only need translated texts here\n",
    "\n",
    "            # Extract source texts and reference texts for FLORES evaluation\n",
    "            source_texts = [encoder_tokenizer.decode(eval_dataset[i]['input_ids'], skip_special_tokens=True) for i in range(len(eval_dataset))]\n",
    "            reference_texts = [encoder_tokenizer.decode(eval_dataset[i]['labels'], skip_special_tokens=True) for i in range(len(eval_dataset))]\n",
    "\n",
    "            # Evaluate translation with FLORES\n",
    "            print(\"Evaluating translation results with Africomet\")\n",
    "            translation_results = evaluator.evaluate(source_texts, translated_texts, reference_texts)\n",
    "            print(\"Complete evaluating translation results\")\n",
    "\n",
    "            # Combine all evaluation results\n",
    "            eval_results = {\n",
    "                'translation_score': translation_results.get('average_score', 0),\n",
    "                'intent_accuracy': eval_metrics['intent_accuracy'],\n",
    "                'slot_f1': eval_metrics['slot_f1'],\n",
    "                'eval_loss': eval_metrics['eval_loss']\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation: {str(e)}\")\n",
    "            logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return float('inf')\n",
    "        \n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_train_epochs} Evaluation results: {eval_results}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        current_metric = eval_results['eval_loss']  # Use eval_loss for early stopping\n",
    "        if current_metric < best_metric:\n",
    "            best_metric = current_metric\n",
    "            patience_counter = 0\n",
    "            # Save the best model state\n",
    "            best_model_state = {\n",
    "                'encoder': trainer.encoder.state_dict(),\n",
    "                'decoder': trainer.decoder.state_dict(),\n",
    "                'projection': trainer.projection.state_dict()\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            logging.info(f\"Early stopping patience: {patience_counter}/{patience_threshold}\")\n",
    "\n",
    "        if patience_counter >= patience_threshold:\n",
    "            logging.info(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state if it exists\n",
    "    if 'best_model_state' in locals():\n",
    "        trainer.encoder.load_state_dict(best_model_state['encoder'])\n",
    "        trainer.decoder.load_state_dict(best_model_state['decoder'])\n",
    "        trainer.projection.load_state_dict(best_model_state['projection'])\n",
    "        logging.info(\"Loaded best model state\")\n",
    "\n",
    "    # Set trial user attributes for logging\n",
    "    trial.set_user_attr('intent_accuracy', eval_results['intent_accuracy'])\n",
    "    trial.set_user_attr('slot_f1', eval_results['slot_f1'])\n",
    "\n",
    "    # Return the combined metric for optimization (lower is better)\n",
    "    return eval_results['translation_score'] - (eval_results['intent_accuracy'] + eval_results['slot_f1']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combined_optimization(encoder, decoder, encoder_tokenizer, decoder_tokenizer, datasets, config, evaluator):\n",
    "    \"\"\"\n",
    "    Run hyperparameter optimization for the combined Afro-XLMR and LLaMA model with improved efficiency, including intent recognition and slot filling tasks.\n",
    "    \"\"\"\n",
    "    mlflow.end_run()  # End any existing runs\n",
    "    \n",
    "    logging.info(\"Starting hyperparameter optimization for combined Afro-XLMR and LLaMA (including intent and slot tasks)\")\n",
    "    log_gpu_memory(\"Before optimization\")\n",
    "\n",
    "    # Enable gradient checkpointing for both models\n",
    "    encoder.gradient_checkpointing_enable()\n",
    "    decoder.gradient_checkpointing_enable()\n",
    "\n",
    "    # Check PyTorch version\n",
    "    pytorch_version = version.parse(torch.__version__)\n",
    "    logger.info(f\"PyTorch version: {pytorch_version}\")\n",
    "    \n",
    "    # Prepare the objective function with fixed arguments and memory management\n",
    "    def memory_managed_objective(trial):\n",
    "        clear_memory()\n",
    "        result = objective_combined(\n",
    "            trial, encoder, decoder, encoder_tokenizer, decoder_tokenizer,\n",
    "            datasets['train'], datasets['eval'], evaluator\n",
    "        )\n",
    "        clear_memory()\n",
    "        return result\n",
    "\n",
    "    total_delay = 0\n",
    "    n_trials = config['hyperparameters']['n_trials']\n",
    "    \n",
    "    try:\n",
    "        # Set up MLflow\n",
    "        experiment_name = \"combined_optimization\"\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        except mlflow.exceptions.MlflowException:\n",
    "            experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "        \n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        with mlflow.start_run(run_name=\"optimization_combined_with_intent_slot\"):\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "\n",
    "            for trial_num in range(n_trials):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                study.optimize(\n",
    "                    memory_managed_objective,\n",
    "                    n_trials=n_trials, \n",
    "                    timeout=3600,\n",
    "                    catch=(Exception,),\n",
    "                    n_jobs=1\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                trial_time = end_time - start_time\n",
    "                \n",
    "                if trial_num < n_trials - 1:  # Don't measure delay after the last trial\n",
    "                    delay_start = time.time()\n",
    "                    clear_memory()\n",
    "                    delay_end = time.time()\n",
    "                    delay = delay_end - delay_start\n",
    "                    total_delay += delay\n",
    "                    logging.info(f\"Delay after trial {trial_num + 1}: {delay:.2f} seconds\")\n",
    "                \n",
    "                logging.info(f\"Trial {trial_num + 1} completed in {trial_time:.2f} seconds\")\n",
    "                log_gpu_memory(f\"After trial {trial_num + 1}\")\n",
    "\n",
    "                # Log intermediate results\n",
    "                if study.trials[-1].state == optuna.trial.TrialState.COMPLETE:\n",
    "                    last_trial = study.trials[-1]\n",
    "                    logging.info(f\"Trial {trial_num + 1} results:\")\n",
    "                    logging.info(f\"  Translation score: {last_trial.value:.4f}\")\n",
    "                    \n",
    "                    intent_accuracy = last_trial.user_attrs.get('intent_accuracy', 'N/A')\n",
    "                    slot_f1 = last_trial.user_attrs.get('slot_f1', 'N/A')\n",
    "                    \n",
    "                    logging.info(f\"  Intent accuracy: {intent_accuracy if isinstance(intent_accuracy, str) else f'{intent_accuracy:.4f}'}\")\n",
    "                    logging.info(f\"  Slot F1 score: {slot_f1 if isinstance(slot_f1, str) else f'{slot_f1:.4f}'}\")\n",
    "        \n",
    "            if study.best_trial:\n",
    "                log_best_params(study)\n",
    "                \n",
    "                # Log best results for all tasks\n",
    "                logging.info(\"Best trial results:\")\n",
    "                logging.info(f\"  Translation score: {study.best_trial.value:.4f}\")\n",
    "                \n",
    "                best_intent_accuracy = study.best_trial.user_attrs.get('intent_accuracy', 'N/A')\n",
    "                best_slot_f1 = study.best_trial.user_attrs.get('slot_f1', 'N/A')\n",
    "                \n",
    "                logging.info(f\"  Intent accuracy: {best_intent_accuracy if isinstance(best_intent_accuracy, str) else f'{best_intent_accuracy:.4f}'}\")\n",
    "                logging.info(f\"  Slot F1 score: {best_slot_f1 if isinstance(best_slot_f1, str) else f'{best_slot_f1:.4f}'}\")\n",
    "            else:\n",
    "                logging.warning(\"No completed trials found.\")\n",
    "                \n",
    "    except optuna.exceptions.OptunaError as e:\n",
    "        logging.error(f\"Optuna error during hyperparameter optimization: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during hyperparameter optimization: {str(e)}\")\n",
    "        logging.exception(\"Exception details:\")\n",
    "        return None\n",
    "    finally:\n",
    "        avg_delay = total_delay / (n_trials - 1) if n_trials > 1 else 0\n",
    "        logging.info(f\"Average delay between trials: {avg_delay:.2f} seconds\")\n",
    "        clear_memory()\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_combined_optimization(encoder, decoder, encoder_tokenizer, decoder_tokenizer, datasets, config, evaluator):\n",
    "#     \"\"\"\n",
    "#     Run a single trial with a small batch for testing purposes.\n",
    "#     \"\"\"\n",
    "#     mlflow.end_run()  # End any existing runs\n",
    "    \n",
    "#     logging.info(\"Starting single trial test for combined Afro-XLMR and LLaMA (including intent and slot tasks)\")\n",
    "#     log_gpu_memory(\"Before optimization\")\n",
    "\n",
    "#     # Enable gradient checkpointing for both models\n",
    "#     encoder.gradient_checkpointing_enable()\n",
    "#     decoder.gradient_checkpointing_enable()\n",
    "\n",
    "#     # Prepare the objective function with fixed arguments and memory management\n",
    "#     def memory_managed_objective(trial):\n",
    "#         clear_memory()\n",
    "#         result = objective_combined(\n",
    "#             trial, encoder, decoder, encoder_tokenizer, decoder_tokenizer,\n",
    "#             datasets['train'], datasets['eval'], evaluator\n",
    "#         )\n",
    "#         clear_memory()\n",
    "#         return result\n",
    "\n",
    "#     try:\n",
    "#         # Set up MLflow\n",
    "#         experiment_name = \"combined_optimization_test\"\n",
    "#         try:\n",
    "#             experiment_id = mlflow.create_experiment(experiment_name)\n",
    "#         except mlflow.exceptions.MlflowException:\n",
    "#             experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "        \n",
    "#         mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "#         with mlflow.start_run(run_name=\"single_trial_test\"):\n",
    "#             study = optuna.create_study(direction='minimize')\n",
    "\n",
    "#             # Run a single trial\n",
    "#             study.optimize(\n",
    "#                 memory_managed_objective,\n",
    "#                 n_trials=1,\n",
    "#                 timeout=3600,\n",
    "#                 catch=(Exception,),\n",
    "#                 n_jobs=1\n",
    "#             )\n",
    "\n",
    "#             if study.trials[-1].state == optuna.trial.TrialState.COMPLETE:\n",
    "#                 last_trial = study.trials[-1]\n",
    "#                 logging.info(\"Trial results:\")\n",
    "#                 logging.info(f\"  Translation score: {last_trial.value:.4f}\")\n",
    "#                 intent_accuracy = last_trial.user_attrs.get('intent_accuracy', 'N/A')\n",
    "#                 slot_f1 = last_trial.user_attrs.get('slot_f1', 'N/A')\n",
    "#                 logging.info(f\"  Intent accuracy: {intent_accuracy if isinstance(intent_accuracy, str) else f'{intent_accuracy:.4f}'}\")\n",
    "#                 logging.info(f\"  Slot F1 score: {slot_f1 if isinstance(slot_f1, str) else f'{slot_f1:.4f}'}\")\n",
    "#             else:\n",
    "#                 logging.warning(\"Trial was not completed successfully.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error during single trial test: {str(e)}\")\n",
    "#         logging.exception(\"Exception details:\")\n",
    "#         return None\n",
    "#     finally:\n",
    "#         clear_memory()\n",
    "    \n",
    "#     return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gpu_memory(stage):\n",
    "    \"\"\"Log GPU memory information.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        current_device = torch.cuda.current_device()\n",
    "        total_memory = torch.cuda.get_device_properties(current_device).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(current_device)\n",
    "        reserved_memory = torch.cuda.memory_reserved(current_device)\n",
    "        available_memory = total_memory - allocated_memory - reserved_memory\n",
    "        logging.info(f\"{stage} - GPU Memory (GB): \"\n",
    "                    f\"Total: {total_memory / 1e9:.2f}, \"\n",
    "                    f\"Allocated: {allocated_memory / 1e9:.2f}, \"\n",
    "                    f\"Reserved: {reserved_memory / 1e9:.2f}, \"\n",
    "                    f\"Available: {available_memory / 1e9:.2f}\")\n",
    "    else:\n",
    "        logging.info(\"CUDA is not available. Cannot log GPU memory.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_best_params(study):\n",
    "    \"\"\"Log best parameters from the study.\"\"\"\n",
    "    best_params = study.best_params\n",
    "    for param, value in best_params.items():\n",
    "        mlflow.log_param(f\"best_{param}\", value)\n",
    "    mlflow.log_metric(\"best_score\", study.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_name, study, config):\n",
    "    \"\"\"Save the optimization results to disk.\"\"\"\n",
    "    output_dir = os.path.join(config['model']['output_dir'], 'optimization_results')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    result = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'best_trial': study.best_trial.number\n",
    "    }\n",
    "    with open(os.path.join(output_dir, f\"{model_name}_optimization_results.json\"), 'w') as f:\n",
    "        json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_trial_info(trial):\n",
    "    \"\"\"Print detailed information about the best trial.\"\"\"\n",
    "    print(\"\\nBest Trial Information:\")\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    print(f\"  Trial number: {trial.number}\")\n",
    "    print(f\"  DateTime start: {trial.datetime_start}\")\n",
    "    print(f\"  DateTime complete: {trial.datetime_complete}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_call(obj, method_name, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Safely calls a method on the given object by its name.\n",
    "\n",
    "    Parameters:\n",
    "    obj (object or tuple): The object on which the method is to be called. \n",
    "                           If a tuple is passed, the function extracts the first element of the tuple.\n",
    "    method_name (str): The name of the method to call on the object.\n",
    "    *args: Additional positional arguments to pass to the method.\n",
    "    **kwargs: Additional keyword arguments to pass to the method.\n",
    "\n",
    "    Returns:\n",
    "    result: The result of the method call.\n",
    "\n",
    "    Raises:\n",
    "    AttributeError: If the method does not exist on the object.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the object is a tuple, extract the first element as the actual target object.\n",
    "    if isinstance(obj, tuple):\n",
    "        obj = obj[0]\n",
    "\n",
    "    # Use getattr to dynamically call the method by its name and pass additional arguments and keyword arguments.\n",
    "    return getattr(obj, method_name)(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_requirements(encoder, decoder, encoder_tokenizer, decoder_tokenizer):\n",
    "    \"\"\"\n",
    "    Analyzes and logs the requirements for the encoder and decoder models, as well as their tokenizers.\n",
    "\n",
    "    Parameters:\n",
    "    encoder (object): The encoder model, usually part of a sequence-to-sequence architecture.\n",
    "    decoder (object or tuple): The decoder model, or a tuple of decoder models. The function extracts the first element if a tuple is passed.\n",
    "    encoder_tokenizer (object or tuple): The tokenizer used with the encoder model. If a tuple is passed, the function extracts the first element.\n",
    "    decoder_tokenizer (object or tuple): The tokenizer used with the decoder model. If a tuple is passed, the function extracts the first element.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing:\n",
    "        - 'encoder_input_shapes': The shapes of the tokenized input for the encoder.\n",
    "        - 'expected_encoder_output_shape': The expected shape of the encoder's output.\n",
    "        - 'decoder_input_shapes': The shapes of the tokenized input for the decoder.\n",
    "        - 'expected_decoder_input_shape': The expected shape of the decoder's input.\n",
    "        - 'decoder_generate_params': The parameter names of the decoder's `generate()` method.\n",
    "\n",
    "    Notes:\n",
    "    The function logs various aspects of the encoder and decoder configuration, including hidden sizes,\n",
    "    maximum position embeddings, and input shapes. It also catches exceptions during tokenization and\n",
    "    when accessing the decoder's `generate()` method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Log the encoder model class name\n",
    "    logging.info(f\"Encoder model: {encoder.__class__.__name__}\")\n",
    "    \n",
    "    # Handle the case where the decoder is passed as a tuple and extract the first element\n",
    "    if isinstance(decoder, tuple):\n",
    "        logging.info(f\"Decoder is a tuple of length {len(decoder)}\")\n",
    "        decoder = decoder[0]\n",
    "    logging.info(f\"Decoder model: {decoder.__class__.__name__}\")\n",
    "\n",
    "    # Handle the case where the encoder tokenizer is passed as a tuple and extract the first element\n",
    "    if isinstance(encoder_tokenizer, tuple):\n",
    "        logging.info(f\"Encoder tokenizer is a tuple of length {len(encoder_tokenizer)}\")\n",
    "        encoder_tokenizer = encoder_tokenizer[0]\n",
    "    logging.info(f\"Encoder tokenizer: {encoder_tokenizer.__class__.__name__}\")\n",
    "\n",
    "    # Handle the case where the decoder tokenizer is passed as a tuple and extract the first element\n",
    "    if isinstance(decoder_tokenizer, tuple):\n",
    "        logging.info(f\"Decoder tokenizer is a tuple of length {len(decoder_tokenizer)}\")\n",
    "        decoder_tokenizer = decoder_tokenizer[0]\n",
    "    logging.info(f\"Decoder tokenizer: {decoder_tokenizer.__class__.__name__}\")\n",
    "\n",
    "    # Analyze and log the encoder's configuration\n",
    "    logging.info(\"Analyzing encoder requirements:\")\n",
    "    encoder_config = encoder.config\n",
    "    logging.info(f\"Encoder hidden size: {encoder_config.hidden_size}\")\n",
    "    logging.info(f\"Encoder max position embeddings: {encoder_config.max_position_embeddings}\")\n",
    "    \n",
    "    # Try to tokenize a sample text using the encoder tokenizer and log input shapes\n",
    "    sample_text = \"This is a sample input text.\"\n",
    "    try:\n",
    "        encoder_inputs = safe_call(encoder_tokenizer, '__call__', sample_text, return_tensors=\"pt\")\n",
    "        logging.info(\"Encoder input shapes:\")\n",
    "        for key, value in encoder_inputs.items():\n",
    "            logging.info(f\"  {key}: {value.shape}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in encoder tokenization: {str(e)}\")\n",
    "\n",
    "    # Analyze and log the decoder's configuration\n",
    "    logging.info(\"\\nAnalyzing decoder requirements:\")\n",
    "    decoder_config = decoder.config\n",
    "    logging.info(f\"Decoder hidden size: {decoder_config.hidden_size}\")\n",
    "    logging.info(f\"Decoder max position embeddings: {decoder_config.max_position_embeddings}\")\n",
    "\n",
    "    # Try to tokenize a sample text using the decoder tokenizer and log input shapes\n",
    "    try:\n",
    "        decoder_inputs = safe_call(decoder_tokenizer, '__call__', sample_text, return_tensors=\"pt\")\n",
    "        logging.info(\"Decoder input shapes:\")\n",
    "        for key, value in decoder_inputs.items():\n",
    "            logging.info(f\"  {key}: {value.shape}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in decoder tokenization: {str(e)}\")\n",
    "\n",
    "    # Attempt to log the parameters of the decoder's generate() method\n",
    "    logging.info(\"\\nDecoder generate() method parameters:\")\n",
    "    try:\n",
    "        generate_params = decoder.generate.__code__.co_varnames\n",
    "        logging.info(f\"Parameters: {generate_params}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error accessing generate method: {str(e)}\")\n",
    "\n",
    "    # Determine and log the expected shape of the encoder's output\n",
    "    expected_encoder_output_shape = (1, encoder_config.max_position_embeddings, encoder_config.hidden_size)\n",
    "    logging.info(f\"\\nExpected encoder output shape: {expected_encoder_output_shape}\")\n",
    "    \n",
    "    # Determine and log the expected shape of the decoder's input\n",
    "    expected_decoder_input_shape = (1, decoder_config.max_position_embeddings)\n",
    "    logging.info(f\"Expected decoder input shape: {expected_decoder_input_shape}\")\n",
    "\n",
    "    # Return relevant shapes and parameters as a dictionary\n",
    "    return {\n",
    "        \"encoder_input_shapes\": {k: v.shape for k, v in encoder_inputs.items()} if 'encoder_inputs' in locals() else None,\n",
    "        \"expected_encoder_output_shape\": expected_encoder_output_shape,\n",
    "        \"decoder_input_shapes\": {k: v.shape for k, v in decoder_inputs.items()} if 'decoder_inputs' in locals() else None,\n",
    "        \"expected_decoder_input_shape\": expected_decoder_input_shape,\n",
    "        \"decoder_generate_params\": generate_params if 'generate_params' in locals() else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results, config):\n",
    "    \"\"\"\n",
    "    Summarize evaluation results.\n",
    "    \n",
    "    This function summarizes the results from various evaluation tasks and saves\n",
    "    them to files.\n",
    "\n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of evaluation results.\n",
    "    \"\"\"\n",
    "    logging.info(\"Summarizing results...\")\n",
    "\n",
    "    summary = {}\n",
    "    for model_name in config['model']['names']:\n",
    "        summary[model_name] = {\n",
    "            'translation': results['translation'].get(model_name),\n",
    "            'intent_recognition': results['intent_recognition'].get(model_name),\n",
    "            'slot_filling': results['slot_filling'].get(model_name),\n",
    "            'zero_shot': results['zero_shot'].get(model_name, {}).get('accuracy') if 'zero_shot' in results else None,\n",
    "            'code_switch': results['code_switch'].get(model_name, {}).get('accuracy') if 'code_switch' in results else None,\n",
    "        }\n",
    "\n",
    "        # Add FLORES results if available\n",
    "        if 'flores' in results['translation'].get(model_name, {}):\n",
    "            summary[model_name]['flores_translation'] = results['translation'][model_name]['flores'].get('average_score')\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    logging.info(\"Evaluation Results Summary:\")\n",
    "    logging.info(summary_df)\n",
    "\n",
    "    # Save results\n",
    "    summary_df.to_csv(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
    "    \n",
    "    with open(f\"{config['model']['output_dir']}/all_results.txt\", 'w') as f:\n",
    "        f.write(str(results))\n",
    "\n",
    "    # Add this line to add summary to results\n",
    "    results['summary'] = summary\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, config):\n",
    "    \"\"\"\n",
    "    Plot evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "    \"\"\"\n",
    "    for model_name in config['model']['names']:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        if model_name in results['translation']:\n",
    "            plt.bar(results['translation'][model_name].keys(), results['translation'][model_name].values())\n",
    "            plt.title(f\"Translation Scores - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_translation_scores.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        if model_name in results['generation']:\n",
    "            plt.hist(results['generation'][model_name]['perplexities'], bins=20)\n",
    "            plt.title(f\"Perplexity Distribution - {model_name}\")\n",
    "            plt.xlabel(\"Perplexity\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_perplexity_distribution.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        # Plot intent recognition confusion matrix\n",
    "        if model_name in results['intent_recognition']:\n",
    "            sns.heatmap(results['intent_recognition'][model_name]['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f\"Intent Recognition Confusion Matrix - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_intent_confusion_matrix.png\")\n",
    "            plt.close()\n",
    "\n",
    "        # Plot slot filling F1 scores\n",
    "        if model_name in results['slot_filling']:\n",
    "            plt.bar(results['slot_filling'][model_name]['f1_scores'].keys(), results['slot_filling'][model_name]['f1_scores'].values())\n",
    "            plt.title(f\"Slot Filling F1 Scores - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{config['model']['output_dir']}/{model_name}_slot_f1_scores.png\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Plot overall performance comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    performance_data = {\n",
    "        model: {\n",
    "            'Classification': results['classification'].get(model, {}).get('accuracy', 0),\n",
    "            'Translation': results['translation'].get(model, {}).get('average_score', 0),\n",
    "            'Generation': 1 / results['generation'].get(model, {}).get('average_perplexity', 1),  # Inverse of perplexity\n",
    "            'Zero-shot': results['zero_shot'].get(model, {}).get('accuracy', 0),\n",
    "            'Code-switch': results['code_switch'].get(model, {}).get('accuracy', 0),\n",
    "            'Intent Recognition': results['intent_recognition'].get(model, {}).get('accuracy', 0),  # New\n",
    "            'Slot Filling': results['slot_filling'].get(model, {}).get('f1_score', 0)  # New\n",
    "        } for model in config['model']['names']\n",
    "    }\n",
    "    df = pd.DataFrame(performance_data).T\n",
    "    sns.heatmap(df, annot=True, cmap='YlGnBu')\n",
    "    plt.title(\"Model Performance Across Tasks\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_mlflow(results, config, best_params):\n",
    "    \"\"\"\n",
    "    Log results to MLflow.\n",
    "    \n",
    "    This function logs the evaluation results, model parameters, and artifacts to MLflow.\n",
    "\n",
    "    Args:\n",
    "        results (dict): A dictionary containing results from all evaluation tasks.\n",
    "        config (dict): A dictionary containing configuration information.\n",
    "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters for each model\n",
    "        for model_name, params in best_params.items():\n",
    "            for param, value in params.items():\n",
    "                mlflow.log_param(f\"{model_name}_{param}\", value)\n",
    "        \n",
    "        # Log model information\n",
    "        for model_name in config['model']['names']:\n",
    "            mlflow.log_param(f\"{model_name}_model\", model_name)\n",
    "\n",
    "        # Log metrics\n",
    "        for model_name, metrics in results['summary'].items():\n",
    "            for metric, value in metrics.items():\n",
    "                if value is not None:\n",
    "                    mlflow.log_metric(f\"{model_name}_{metric}\", value)\n",
    "\n",
    "        # Log artifacts\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/evaluation_results_summary.csv\")\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/all_results.txt\")\n",
    "        mlflow.log_artifact(f\"{config['model']['output_dir']}/overall_performance_heatmap.png\")\n",
    "        \n",
    "        # Log hyperparameter optimization plots\n",
    "        for model_name in config['model']['names']:   \n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_intent_confusion_matrix.png\")\n",
    "            mlflow.log_artifact(f\"{config['model']['output_dir']}/{model_name}_slot_f1_scores.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_summary(results_summary, best_params):\n",
    "    \"\"\"\n",
    "    Print a summary of the evaluation results and best hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        results_summary (dict): A dictionary containing summarized results.\n",
    "        best_params (dict): A dictionary of the best hyperparameters found during optimization.\n",
    "    \"\"\"\n",
    "    print(\"\\n===== EVALUATION RESULTS SUMMARY =====\")\n",
    "\n",
    "    if 'classification' in results_summary:\n",
    "        print(\"\\nClassification Results:\")\n",
    "        for dataset, metrics in results_summary['classification'].items():\n",
    "            print(f\"\\n{dataset}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    if 'translation' in results_summary:\n",
    "        print(\"\\nTranslation Results:\")\n",
    "        print(f\"  FLORES-200 Average AfriCOMET Score (A to B): {results_summary['translation']['a_to_b']['average_score']:.4f}\")\n",
    "        print(f\"  FLORES-200 Average AfriCOMET Score (B to A): {results_summary['translation']['b_to_a']['average_score']:.4f}\")\n",
    "\n",
    "    if 'generation' in results_summary:\n",
    "        print(\"\\nGeneration Results:\")\n",
    "        print(f\"  FLORES-200 Average Perplexity: {results_summary['generation']['average_perplexity']:.4f}\")\n",
    "\n",
    "    if 'zero_shot' in results_summary:\n",
    "        print(\"\\nZero-shot Results:\")\n",
    "        print(f\"  Accuracy: {results_summary['zero_shot']['accuracy']:.4f}\")\n",
    "\n",
    "    if 'code_switch' in results_summary:\n",
    "        print(\"\\nCode-switch Results:\")\n",
    "        print(f\"  Accuracy: {results_summary['code_switch']['accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "    print(\"\\n======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../py/config.yaml')\n",
    "# Set the device dynamically based on availability\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'  # Update device setting\n",
    "auth_token = config.get(\"auth_token\")\n",
    "cache_dir = os.path.abspath(config['cache']['dir'])\n",
    "logger = setup_logging(config)\n",
    "set_seed(config['seed'])\n",
    "device = get_device(config['device'])\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the cache directory exists\n",
    "os.makedirs(cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and prepare datasets\n",
    "data_loader = DatasetLoader(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stratified_datasets = data_loader.prepare_stratified_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in stratified_datasets.items():\n",
    "    print(f\"{split} dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify data integrity\n",
    "if not data_loader.verify_data_integrity(stratified_datasets):\n",
    "    logger.error(\"Data integrity check failed. Please review the datasets.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print dataset information\n",
    "data_loader.print_dataset_info(stratified_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the structure of stratified_datasets\n",
    "print(\"Structure of stratified_datasets:\")\n",
    "print(data_loader.inspect_dataset_structure(stratified_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess datasets\n",
    "preprocessed_datasets = {\n",
    "    key: data_loader.preprocess_dataset(dataset)\n",
    "    for key, dataset in stratified_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the structure of preprocessed_datasets\n",
    "print(\"\\nStructure of preprocessed_datasets:\")\n",
    "print(data_loader.inspect_dataset_structure(preprocessed_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the models directory is in the Python path\n",
    "models_dir = os.path.abspath(os.path.join('..', 'py', 'models'))\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.append(models_dir)\n",
    "    print(f\"Added {models_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of the models directory\n",
    "print(f\"Models directory contents: {os.listdir(models_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models, tokenizers = {}, {}\n",
    "num_labels = len(preprocessed_datasets['train']['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize models with the configured parameters and authentication token\n",
    "for model_name in config['model']['names']:\n",
    "    print(f\"Initializing model: {model_name}\")\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    print(f\"Auth token: {auth_token[:5]}...{auth_token[-5:] if auth_token else None}\")\n",
    "    try:\n",
    "        if model_name == \"meta-llama/Llama-2-7b-hf\":\n",
    "            llama_model = Llama2Decoder(model_name, auth_token=auth_token, cache_dir=cache_dir)\n",
    "            models[model_name] = llama_model.get_model()   # The model is already on the appropriate device\n",
    "            tokenizers[model_name] = llama_model.get_tokenizer()\n",
    "        \n",
    "        elif model_name == \"afro-xlmr-large\":\n",
    "            afro_xlmr_large_model = AfroXLMRLarge(model_name, num_labels=num_labels, cache_dir=cache_dir)\n",
    "            models[model_name] = afro_xlmr_large_model.get_model()   # The model is already on the appropriate device\n",
    "            tokenizers[model_name] = afro_xlmr_large_model.get_tokenizer()\n",
    "        \n",
    "        elif model_name == \"gemini\":\n",
    "            # Initialize GeminiModel with the model name\n",
    "            gemini_model = GeminiModel(model_name=model_name)\n",
    "\n",
    "            # Initialize GeminiTrainer with the GeminiModel and config\n",
    "            gemini_trainer = GeminiTrainer(\n",
    "                model=gemini_model, \n",
    "                config=config\n",
    "            )\n",
    "\n",
    "            # Store GeminiModel in models dictionary\n",
    "            models[model_name] = gemini_model\n",
    "\n",
    "        if models[model_name] is not None:\n",
    "            print(f\"Successfully initialized {model_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to initialize {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing {model_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available models:\", list(models.keys()))\n",
    "print(\"Available tokenizers:\", list(tokenizers.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create custom datasets for PyTorch\n",
    "datasets = {}\n",
    "\n",
    "model_type_combined = 'encoder_decoder' \n",
    "combined_tokenizer = tokenizers['afro-xlmr-large']  \n",
    "logging.info(f\"Creating datasets for combined Afro-XLMR and LLaMA model with model type: {model_type_combined}\")\n",
    "\n",
    "model_type_gemini = 'gemini' \n",
    "logging.info(f\"Creating datasets for Gemini model with model type: {model_type_gemini}\")\n",
    "\n",
    "\n",
    "# Use the key 'combined_afro_xlmr_llama'\n",
    "datasets = {\n",
    "    'combined_afro_xlmr_llama':{\n",
    "        'train': CustomDataset(preprocessed_datasets['train'], combined_tokenizer, model_type=model_type_combined),\n",
    "        'eval': CustomDataset(preprocessed_datasets['eval'], combined_tokenizer, model_type=model_type_combined),\n",
    "        'benchmark': CustomDataset(preprocessed_datasets['benchmark'], combined_tokenizer, model_type=model_type_combined)\n",
    "    },\n",
    "    'gemini': {\n",
    "        'train': CustomDataset(stratified_datasets['train'], gemini_model.count_tokens, model_type=model_type_gemini),\n",
    "        'eval': CustomDataset(stratified_datasets['eval'], gemini_model.count_tokens, model_type=model_type_gemini),\n",
    "        'benchmark': CustomDataset(stratified_datasets['benchmark'], gemini_model.count_tokens, model_type=model_type_gemini)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "zero_shot_classifier = ZeroShotClassifier(\n",
    "    encoder=models['afro-xlmr-large'],\n",
    "    decoder=models['meta-llama/Llama-2-7b-hf'],\n",
    "    tokenizer=combined_tokenizer\n",
    ")\n",
    "\n",
    "code_switch_classifier = CodeSwitchClassifier(\n",
    "    encoder=models['afro-xlmr-large'],\n",
    "    decoder=models['meta-llama/Llama-2-7b-hf'],\n",
    "    tokenizer=combined_tokenizer\n",
    ")\n",
    "\n",
    "zero_shot_classifier_gemini = ZeroShotClassifierForGemini(gemini_model)\n",
    "code_switch_classifier_gemini = CodeSwitchClassifierForGemini(gemini_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluators\n",
    "evaluators = {\n",
    "    'combined_afro_xlmr_llama': AfriCOMETEvaluator(\n",
    "        model=None,  # AfriCOMETEvaluator doesn't use the model directly\n",
    "        tokenizer=tokenizers['afro-xlmr-large']\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run optimization for combined Afro-XLMR and LLaMA\n",
    "combined_study = run_combined_optimization(\n",
    "    models['afro-xlmr-large'],  # Encoder\n",
    "    models['meta-llama/Llama-2-7b-hf'],  # Decoder\n",
    "    tokenizers['afro-xlmr-large'], # encoder_tokenizer\n",
    "    tokenizers['meta-llama/Llama-2-7b-hf'], # decoder_tokenizer\n",
    "    datasets['combined_afro_xlmr_llama'],  \n",
    "    config,\n",
    "    evaluators['combined_afro_xlmr_llama'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Gemini model optimization\n",
    "gemini_study = gemini_trainer.run_tuning(training_data=datasets['gemini'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter optimization for combined model\n",
    "try:\n",
    "    # Consolidate the results into the studies variable\n",
    "    studies = {\n",
    "        'combined_study': combined_study,\n",
    "        'gemini_study': gemini_study\n",
    "    }\n",
    "    \n",
    "    # Extract best parameters\n",
    "    best_params = {model_name: study.best_params for model_name, study in studies.items()}\n",
    "    print(best_params)\n",
    "\n",
    "    # Log best parameters\n",
    "    for model_name, params in best_params.items():\n",
    "        logger.info(f\"Best hyperparameters for {model_name}: {params}\")\n",
    "        config['training'][model_name] = params\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during or after hyperparameter optimization: {str(e)}\")\n",
    "    logger.exception(\"Exception details:\")\n",
    "    studies = {}  # Initialize an empty dictionary if optimization failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter analysis\n",
    "logger.info(\"Performing hyperparameter analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, study in studies.items():\n",
    "    print(f\"model name: {model_name}, study: {study}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, study in studies.items():\n",
    "    # Plot hyperparameter importance\n",
    "    importance_fig = plot_hyperparameter_importance(study)\n",
    "    importance_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\")\n",
    "\n",
    "    # Plot optimization history\n",
    "    history_fig = optuna.visualization.plot_optimization_history(study)\n",
    "    history_fig.update_layout(title=\"Optimization History\")\n",
    "    history_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\")\n",
    "\n",
    "    # Plot parallel coordinate\n",
    "    dimensions = []\n",
    "    for param in study.best_trials[0].params:\n",
    "        values = [trial.params[param] for trial in study.trials if param in trial.params]\n",
    "        if isinstance(values[0], (int, float)):\n",
    "            dimensions.append(\n",
    "                dict(range = [min(values), max(values)],\n",
    "                     label = param,\n",
    "                     values = values)\n",
    "            )\n",
    "        elif isinstance(values[0], str):\n",
    "            # For categorical parameters, we need a different approach\n",
    "            unique_values = list(set(values))\n",
    "            dimensions.append(\n",
    "                dict(range = [0, len(unique_values) - 1],\n",
    "                     tickvals = list(range(len(unique_values))),\n",
    "                     ticktext = unique_values,\n",
    "                     label = param,\n",
    "                     values = [unique_values.index(v) for v in values])\n",
    "            )\n",
    "\n",
    "    parallel_fig = go.Figure(data=\n",
    "        go.Parcoords(\n",
    "            line = dict(color = [trial.value for trial in study.trials],\n",
    "                        colorscale = 'Viridis',\n",
    "                        showscale = True),\n",
    "            dimensions = dimensions\n",
    "        )\n",
    "    )\n",
    "    parallel_fig.update_layout(title=\"Parallel Coordinate Plot of Hyperparameters\")\n",
    "    parallel_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\")\n",
    "\n",
    "    # Analyze and plot sensitivity\n",
    "    sensitivity = analyze_hyperparameter_sensitivity(study)\n",
    "    sensitivity_fig = plot_sensitivity_analysis(sensitivity)\n",
    "    sensitivity_fig.write_image(f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\")\n",
    "\n",
    "    # Print sensitivity analysis results\n",
    "    print(f\"\\nHyperparameter Sensitivity Analysis for {model_name}:\")\n",
    "    for param, sens in sensitivity:\n",
    "        print(f\"{param}: {sens:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Hyperparameter analysis complete. Plots saved in output directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Accelerator\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(mixed_precision='no', kwargs_handlers=[ddp_kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainers with the best hyperparameters from the studies\n",
    "trainers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'combined_study' in studies and studies['combined_study'] is not None:\n",
    "    # Start with the base config\n",
    "    combined_config = config.copy()\n",
    "    # Add the best parameters from the study\n",
    "    combined_config.update(studies['combined_study'].best_params)\n",
    "    combined_config['per_device_eval_batch_size'] = 1\n",
    "    combined_config['max_grad_norm'] = 1\n",
    "\n",
    "    # Add required parameters if they're not already present\n",
    "    if 'num_intent_classes' not in combined_config:\n",
    "        combined_config['num_intent_classes'] = 50  # Replace with the actual number of intent classes\n",
    "    \n",
    "    if 'num_slot_classes' not in combined_config:\n",
    "        combined_config['num_slot_classes'] = 100  # Replace with the actual number of slot classes\n",
    "\n",
    "    trainers['combined_afro_xlmr_llama'] = CombinedEncoderDecoderTrainer(\n",
    "        encoder=models['afro-xlmr-large'],  # Afro-XLMR as encoder\n",
    "        decoder=models['meta-llama/Llama-2-7b-hf'],  # LLaMA as decoder\n",
    "        encoder_tokenizer=tokenizers['afro-xlmr-large'],\n",
    "        decoder_tokenizer=tokenizers['meta-llama/Llama-2-7b-hf'],\n",
    "        config=combined_config,\n",
    "        accelerator=accelerator,\n",
    "        batch_size=64  # or whatever batch size you prefer\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"Study results for 'combined_study' not found or optimization failed. \"\n",
    "                   \"CombinedEncoderDecoderTrainer will not be initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'gemini' in models:\n",
    "    gemini_config = config.copy()\n",
    "    gemini_config.update(studies.get('gemini_study', {}).get('best_params', {}))\n",
    "    gemini_config['per_device_eval_batch_size'] = 1\n",
    "    gemini_config['max_grad_norm'] = 1\n",
    "    \n",
    "    # Initialize GeminiTrainer if GeminiModel is available\n",
    "    trainers['gemini'] = GeminiTrainer(\n",
    "        model=models['gemini'],\n",
    "        tokenizer=tokenizers['gemini'],\n",
    "        config=gemini_config,\n",
    "        accelerator=accelerator,\n",
    "        batch_size=64  # or your preferred batch size\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"Gemini model not found. GeminiTrainer will not be initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluations\n",
    "results = {\n",
    "    'classification': {},\n",
    "    'translation': {},\n",
    "    'generation': {},\n",
    "    'zero_shot': {},\n",
    "    'code_switch': {},\n",
    "    'intent_recognition': {}, \n",
    "    'slot_filling': {}, \n",
    "    'hyperparameter_studies': studies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train models with the best hyperparameters\n",
    "for model_name, trainer in trainers.items():\n",
    "    logger.info(f\"Starting training for model: {model_name}\")\n",
    "    \n",
    "    # Determine the correct dataset keys\n",
    "    train_dataset_key = 'train'\n",
    "    eval_dataset_key = 'eval'\n",
    "    \n",
    "    # Ensure the required datasets exist\n",
    "    if train_dataset_key not in datasets['combined_afro_xlmr_llama'] or eval_dataset_key not in datasets['combined_afro_xlmr_llama']:\n",
    "        logger.error(f\"Required datasets not found for {model_name}. Skipping this model.\")\n",
    "        continue\n",
    "\n",
    "    train_dataset = datasets['combined_afro_xlmr_llama'][train_dataset_key]\n",
    "    eval_dataset = datasets['combined_afro_xlmr_llama'][eval_dataset_key]\n",
    "\n",
    "    logger.info(f\"Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        train_results = trainer.train(train_dataset, eval_dataset)\n",
    "        \n",
    "        # Log training results\n",
    "        logger.info(f\"Training completed for {model_name}\")\n",
    "        logger.info(f\"Training results: {train_results}\")\n",
    "        \n",
    "        # Save the trained model\n",
    "        save_dir = os.path.join(config['model']['output_dir'], model_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # trainer.save_model(save_dir)\n",
    "        logger.info(f\"Model saved to {save_dir}\")\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_results = trainer.evaluate(eval_dataset)\n",
    "        logger.info(f\"Evaluation results for {model_name}: {eval_results}\")\n",
    "        \n",
    "        # You might want to save these results to a file or database\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training or evaluation of {model_name}: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        continue  # Move to the next model if there's an error\n",
    "\n",
    "logger.info(\"Training process completed for all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets['combined_afro_xlmr_llama']['benchmark'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flores_200 = data_loader.load_flores_200_benchmark()\n",
    "if not flores_200:\n",
    "    logging.warning(\"FLORES-200 benchmark data is empty or failed to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flores_200[\"devtest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both zero-shot and code-switch datasets\n",
    "experimental_datasets = data_loader.load_experimental_datasets(flores_200)\n",
    "\n",
    "if not experimental_datasets:\n",
    "    logging.warning(\"Experimenta data is empty or failed to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access zero-shot and code-switch datasets\n",
    "zero_shot_df = experimental_datasets['zero_shot']\n",
    "code_switch_df = experimental_datasets['code_switch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zero_shot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_switch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainers.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation for all models\n",
    "for model_name, trainer in trainers.items():\n",
    "    if model_name == 'combined_afro_xlmr_llama':\n",
    "        encoder=models['afro-xlmr-large']  # Afro-XLMR as encoder\n",
    "        decoder=models['meta-llama/Llama-2-7b-hf']  # LLaMA as decoder\n",
    "        encoder_tokenizer=tokenizers['afro-xlmr-large']\n",
    "        decoder_tokenizer=tokenizers['meta-llama/Llama-2-7b-hf']\n",
    "        evaluator = evaluators['combined_afro_xlmr_llama']  # Assuming all evaluators are AfriCOMETEvaluator\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    try:\n",
    "         # Use the FLORES data that's already loaded\n",
    "        # results['translation'][model_name] = trainer.evaluate_translation(\n",
    "        #     encoder, \n",
    "        #     decoder,\n",
    "        #     encoder_tokenizer, \n",
    "        #     decoder_tokenizer,\n",
    "        #     flores_200, \n",
    "        #     evaluator, \n",
    "        #     studies['combined_study'].best_params\n",
    "        # )\n",
    "        # result = analyze_model_requirements(encoder, decoder, encoder_tokenizer, decoder_tokenizer)\n",
    "        # print(result)\n",
    "            \n",
    "        # Zero-shot evaluation\n",
    "        # results['zero_shot'][model_name] = trainer.evaluate_zero_shot(\n",
    "        #     zero_shot_df, \n",
    "        #     zero_shot_classifier, \n",
    "        #     studies['combined_study'].best_params\n",
    "        # )\n",
    "        \n",
    "        # Code-switch evaluation\n",
    "        results['code_switch'][model_name] = trainer.evaluate_code_switch(\n",
    "            zero_shot_df, \n",
    "            code_switch_classifier,\n",
    "            studies['combined_study'].best_params\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Completed evaluation for {model_name}\")\n",
    "        \n",
    "        logger.info(f\"Completed evaluation for {model_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during evaluation of {model_name}: {str(e)}\")\n",
    "        continue  # Move to the next model if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize and log results\n",
    "results_summary = summarize_results(results, config)\n",
    "plot_results(results, config)\n",
    "log_results_to_mlflow(results, config, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Evaluation complete!\")\n",
    "print(\"Evaluation completed successfully. Results and visualizations have been saved and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results and visualizations\n",
    "display(Image(filename=f\"{config['model']['output_dir']}/overall_performance_heatmap.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameter optimization results\n",
    "for model_name in config['model']['names']:\n",
    "    print(f\"\\nHyperparameter Optimization Results for {model_name}:\")\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_hyperparameter_importance.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_optimization_history.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_parallel_coordinate.png\"))\n",
    "    display(Image(filename=f\"{config['model']['output_dir']}/{model_name}_sensitivity_analysis.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_summary(results_summary, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation notebook execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
